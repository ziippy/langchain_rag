{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랭체인(LangChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from langchain) (0.3.51)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from langchain) (0.3.30)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.13.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
      "Requirement already satisfied: sniffio in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 체인\n",
    "\n",
    "체인은 개별 단계를 순서대로 이어 붙여 하나의 흐름을 만들어 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='피타고라스 정리는 직각삼각형에서 성립하는 정리로, 그 공식은 다음과 같습니다:\\n\\n\\\\[ a^2 + b^2 = c^2 \\\\]\\n\\n여기서 \\\\( c \\\\)는 직각삼각형의 빗변의 길이이고, \\\\( a \\\\)와 \\\\( b \\\\)는 빗변이 아닌 두 변의 길이입니다. 이 공식은 빗변의 제곱은 다른 두 변의 제곱의 합과 같다는 것을 나타냅니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 114, 'prompt_tokens': 21, 'total_tokens': 135, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d8864f8b6b', 'id': 'chatcmpl-BQD0ixIStMdSNy3YytxdlQEd214jp', 'finish_reason': 'stop', 'logprobs': None}, id='run-66c4345b-9b93-4396-9af7-d6222dbe2582-0', usage_metadata={'input_tokens': 21, 'output_tokens': 114, 'total_tokens': 135, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain 실행 방법\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "llm.invoke(\"피타고라스 정리의 공식은 무엇인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"피타고라스 정리는 삼각형 중에서 직각삼각형에 대한 거예요. 직각삼각형은 한 각이 90도인 삼각형이에요. 이 삼각형을 이루는 세 변 중에서, 가장 긴 변을 빗변이라고 부르고, 나머지 두 변을 각각 다른 이름 대신 보통 '변'이라고 불러요.\\n\\n피타고라스 정리의 공식은 이렇게 생겼어요:\\n\\n\\\\[\\na^2 + b^2 = c^2\\n\\\\]\\n\\n여기서 \\\\(a\\\\)와 \\\\(b\\\\)는 빗변이 아닌 두 변의 길이이고, \\\\(c\\\\)는 가장 긴 변인 빗변의 길이예요. 쉽게 말해서, 두 짧은 변의 제곱을 더한 값이 가장 긴 변의 제곱과 같다는 뜻이랍니다! 예를 들어, 변의 길이가 각각 3, 4, 5인 직각삼각형이 있다면, \\\\(3^2 + 4^2 = 5^2\\\\)는 9 + 16 = 25가 되어 맞는다는 걸 확인할 수 있어요.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 256, 'prompt_tokens': 40, 'total_tokens': 296, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_90122d973c', 'id': 'chatcmpl-BQD0rMkTB93EjiSYD5mxqUWwjuULb', 'finish_reason': 'stop', 'logprobs': None}, id='run-0a07a754-0354-4011-9d49-177bf091ab4e-0', usage_metadata={'input_tokens': 40, 'output_tokens': 256, 'total_tokens': 296, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프롬프트 지시 추가\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"초등학생이 이해할 수 있는 방식으로 답을 설명하세요.: <질문>: {query}\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"query\": \"피타고라스 정리의 공식은 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'피타고라스 정리는 직각삼각형에 대한 것입니다. 직각삼각형은 한 각이 90도인 삼각형이에요. 피타고라스 정리는 이런 직각삼각형에서 가장 긴 변(빗변)과 나머지 두 변(다리들)의 길이 사이의 관계를 설명해요.\\n\\n공식은 이렇게 생겼어요:\\n\\n\\\\[ a^2 + b^2 = c^2 \\\\]\\n\\n여기서,\\n- \\\\(a\\\\)와 \\\\(b\\\\)는 직각을 이루는 두 변의 길이예요.\\n- \\\\(c\\\\)는 가장 긴 변, 즉 빗변의 길이예요.\\n\\n쉽게 말하면, 두 다리의 길이를 각각 제곱해서 더한 값이 빗변의 길이를 제곱한 값과 같다는 뜻이에요. 예를 들어, 한 변의 길이가 3이고 다른 한 변의 길이가 4인 직각삼각형이 있다면, 빗변의 길이는 5가 돼요(왜냐하면 \\\\(3^2 + 4^2 = 9 + 16 = 25\\\\)이고, \\\\(5^2 = 25\\\\)).'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ouput parser 추가\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"초등학생이 이해할 수 있는 방식으로 답을 설명하세요.: <질문>: {query}\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"query\": \"피타고라스 정리의 공식은 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티체인\n",
    "\n",
    "# 예시: 찬성 의견과 반대 의견을 각각 생성하고, 이를 결합하여 최종적인 토론 형식의 답변을 제공하는 프로세스\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableMap\n",
    "from operator import itemgetter # 파이프라인에서 데이터의 특정 부분을 추출하는 데 사용. 예를 들어, 이전 단계에서 생성된 base_response 를 다음 단계에 전달할 때 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic 에 대한 기본 논쟁을 생성하고, 그 결과를 base라는 이름으로 전달\n",
    "basictopic = (\n",
    "    ChatPromptTemplate.from_template(\"{topic}에 대한 논쟁을 한국어로 생성합니다.\")\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    "    | {\"base\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "# 긍정적인 의견 생성\n",
    "positive = (\n",
    "    ChatPromptTemplate.from_template(\"{base}의 장점 또는 긍정적인 측면을 나열하세요.\")\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 부정적인 의견 생성\n",
    "negative = (\n",
    "    ChatPromptTemplate.from_template(\"{base}의 단점 또는 부정적인 측면을 나열하세요.\")\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 최종 답변\n",
    "# 긍정적 의견(results_1)과 부정적 의견(results_2)를 통합하여 최종 비평을 생산\n",
    "final = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"ai\", \"{original_response}\"),\n",
    "            (\"human\", \"긍정:\\n{results_1}\\n\\부정:\\n{results_2}\"),\n",
    "            (\"system\", \"비평에 대한 최종 답변 생성\")\n",
    "        ]\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 병렬 처리\n",
    "# RunnableParallel을 사용하여 positive와 negative를 병렬로 처리\n",
    "# 이는 각 체인을 동시에 실행하여 전체 프로세스의 속도를 최적화하는 방법\n",
    "chain = (\n",
    "    basictopic\n",
    "    | RunnableParallel(\n",
    "        results_1 = positive,\n",
    "        results_2 = negative,\n",
    "        original_response = itemgetter(\"base\")\n",
    "    )\n",
    "    | RunnableMap(\n",
    "        {\n",
    "            \"positive_result\": itemgetter(\"results_1\"),\n",
    "            \"negative_result\": itemgetter(\"results_2\"),\n",
    "            \"final_answer\": itemgetter(\"original_response\")\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 실행\n",
    "\n",
    "result = chain.invoke({\"topic\": \"social media\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_result = result['positive_result']\n",
    "negative_result = result['negative_result']\n",
    "final_answer = result['final_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. 소통과 연결: social media를 통해 지리적 제약 없이 사람들과 소통할 수 있으며, 친구나 가족과 연락을 유지할 수 있습니다.\\n2. 정보 공유: social media를 통해 뉴스나 이벤트, 팁 등 다양한 정보를 쉽게 얻을 수 있습니다.\\n3. 마케팅과 홍보: 비즈니스나 개인 브랜드의 홍보 및 마케팅에 유용하게 사용될 수 있습니다.\\n4. 커뮤니티 형성: 공통된 관심사를 가진 사람들끼리 모여 커뮤니티를 형성하고 소통할 수 있습니다.\\n5. 창의성 촉진: 다양한 아이디어와 의견을 공유하고 피드백을 받아 창의성을 촉진할 수 있습니다.\\n\\n이러한 장점들을 최대한 활용하며, social media의 발전과 이용에 있어서는 적절한 규제와 보호 정책을 통해 사회적 문제를 예방하고 해결해야 한다는 것이 중요합니다.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. 개인정보 유출: social media를 통해 개인정보가 노출되어 사생활이 침해될 수 있습니다. 사용자의 개인정보가 무단으로 수집되거나 유출될 경우 개인정보 보호에 대한 우려가 증가하고 있습니다.\\n\\n2. 온라인 괴롭힘: social media를 통한 익명성으로 인해 온라인에서의 괴롭힘이 쉽게 일어날 수 있습니다. 특히 청소년들 사이에서는 사회적 평가나 부당한 비판이 빈번하게 발생할 수 있어 정신적인 스트레스를 유발할 수 있습니다.\\n\\n3. 정보 과부하: social media를 통해 너무 많은 정보가 동시에 제공되기 때문에 사용자들은 필요없는 정보에 시간을 낭비하거나 정보 과부하로 인해 혼란을 겪을 수 있습니다.\\n\\n4. 중독성: social media의 사용이 과도하게 이루어질 경우 중독성이 발생할 수 있습니다. 사용자들은 social media에 지나치게 의존하게 되어 현실과의 괴리를 느끼고 정서적 문제를 겪을 수 있습니다.\\n\\n5. 반사회적 행동 유발: social media를 통해 가짜 뉴스나 선정적인 콘텐츠 등이 확산될 수 있어, 사회적 방해나 윤리적 문제를 일으킬 수 있습니다. 이로 인해 사회적 갈등이나 분열이 심화될 우려가 있습니다.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'현재 사회에서는 social media에 대한 의견이 분분하게 나뉘고 있습니다. 일부 사람들은 social media가 소통과 정보 공유에 많은 도움을 준다고 주장하는 반면, 다른 일부 사람들은 social media가 개인정보 유출과 불필요한 정보 과부하를 초래한다고 우려하는 견해도 있습니다.\\n\\n특히, social media의 사용이 점점 증가함에 따라 사회적 문제도 덩달아 발생하고 있다는 의견도 제기되고 있습니다. 특히 청소년들의 social media 중독과 온라인 괴롭힘 문제 등이 큰 이슈로 부각되고 있습니다.\\n\\n이에 대해 어떤 사람들은 social media 규제가 필요하다고 주장하며, 다른 일부 사람들은 자유로운 의사소통을 방해한다는 이유로 규제에 반대하는 견해도 있습니다.\\n\\n이러한 논쟁 속에서 사회 전반적으로 어떤 방향으로 social media가 발전해야 하는지에 대한 논의가 계속되고 있습니다. 이를 통해 social media의 잠재적인 장점을 최대한 발휘하면서도 문제점을 해결하기 위한 방향을 마련하는 것이 중요한 과제로 자리잡고 있습니다.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 프롬프트\n",
    "\n",
    "프롬프트 템플릿은 개발자가 언어 모델과 상호작용하는 방식을 구조화하여 일관성 있는 응답을 얻도록 도와주는 사전 구축된 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "닭에 대한 재미있는 농담을 들려주세요.\n"
     ]
    }
   ],
   "source": [
    "# PromptTemplate 기본 사용법\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"{content}에 대한 {adjective} 농담을 들려주세요.\"\n",
    ")\n",
    "\n",
    "formatted_prompt = prompt_template.format(adjective=\"재미있는\", content=\"닭\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='너는 유능한 어시스트야 너의 이름은 김철수 이야.', additional_kwargs={}, response_metadata={}), HumanMessage(content='안녕하세요?', additional_kwargs={}, response_metadata={}), AIMessage(content='안녕하세요 무엇을 도와드릴까요?', additional_kwargs={}, response_metadata={}), HumanMessage(content='너의 이름은 뭐야?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# 채팅 프롬프트\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 대화형 템플릿 만들기\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"너는 유능한 어시스트야 너의 이름은 {name} 이야.\"),\n",
    "        (\"human\", \"안녕하세요?\"),\n",
    "        (\"ai\", \"안녕하세요 무엇을 도와드릴까요?\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 템플릿에 값 채우기\n",
    "messages = chat_template.format_messages(name=\"김철수\", user_input=\"너의 이름은 뭐야?\")\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='프로그래밍을 배우는 가장 좋은 방법은 무엇인가요?', additional_kwargs={}, response_metadata={}), AIMessage(content='변수 및 루프와 같은 기본 사항부터 시작하세요.', additional_kwargs={}, response_metadata={}), HumanMessage(content='지금까지의 대화를 10 단어로 요약합니다.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 조합 및 변수 사용\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "# 대화 요약 템플릿 만들기\n",
    "human_prompt = \"지금까지의 대화를 {word_count} 단어로 요약합니다.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "# 대화 내용과 함께 요약 요청\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [MessagesPlaceholder(variable_name=\"conversation\"), human_message_template]\n",
    ")\n",
    "\n",
    "# 대화 내용과 단어 수를 넣어서 요약 프롬프트 생성\n",
    "conversation = [\n",
    "    HumanMessage(content=\"프로그래밍을 배우는 가장 좋은 방법은 무엇인가요?\"),\n",
    "    AIMessage(content=\"변수 및 루프와 같은 기본 사항부터 시작하세요.\"),\n",
    "]\n",
    "formatted_prompt = chat_prompt.format_prompt(conversation=conversation, word_count=\"10\")\n",
    "print(formatted_prompt.to_messages())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 메모리\n",
    "\n",
    "ReAct 에이전트에 메모리 기능을 추가하여 대화의 연속성을 확보하는 방법 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain 및 필요한 라이브러리 임포트\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# LLM 모델 설정 (예: OpenAI GPT 모델)\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3503/2733803148.py:13: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_executor = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "# 에이전트\n",
    "\n",
    "# 에이전트가 사용할 툴 설정\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Echo\",\n",
    "        func=lambda x: f\"Echoing: {x}\",  # 간단한 툴로 입력한 값을 그대로 반환\n",
    "        description=\"Echo the input text\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# 에이전트 설정: Tool을 사용하여 질의를 처리할 수 있도록 설정\n",
    "agent_executor = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # 적절한 에이전트 타입 설정\n",
    "    verbose=True  # 출력되는 내용이 많아짐\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 안녕 나는 김철수야.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Echo\n",
      "Action Input: 안녕 나는 김철수야.\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mEchoing: 안녕 나는 김철수야.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer. \n",
      "\n",
      "Final Answer: 안녕 나는 김철수야.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "답변: 안녕 나는 김철수야.\n",
      "질문: 내가 누구인지 기억나니?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Echo\n",
      "Action Input: 내가 누구인지 기억나니?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mEchoing: 내가 누구인지 기억나니?\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should interpret the information provided and respond accordingly. \n",
      "\n",
      "Final Answer: 나는 인공지능 비서이기 때문에 개인적인 기억력이 없습니다. 당신이 누구인지 스스로 설명해주시면 도움이 될 것입니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "답변: 나는 인공지능 비서이기 때문에 개인적인 기억력이 없습니다. 당신이 누구인지 스스로 설명해주시면 도움이 될 것입니다.\n"
     ]
    }
   ],
   "source": [
    "# 메모리 없는 예시\n",
    "\n",
    "# 사용자 프로필 관리\n",
    "query = \"안녕 나는 김철수야.\"\n",
    "print(\"질문:\", query)\n",
    "result = agent_executor.invoke({\"input\": query})\n",
    "print(\"답변:\", result['output'])\n",
    "\n",
    "query = \"내가 누구인지 기억나니?\"\n",
    "print(\"질문:\", query)\n",
    "result = agent_executor.invoke({\"input\": query})\n",
    "print(\"답변:\", result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 기본 설정 언어를 한국어로 업데이트합니다.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo update the default language to Korean, I'll use the Echo tool to repeat the command.\n",
      "\n",
      "Action: Echo\n",
      "Action Input: 기본 설정 언어를 한국어로 업데이트합니다.\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mEchoing: 기본 설정 언어를 한국어로 업데이트합니다.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "\n",
      "Final Answer: 기본 설정 언어를 한국어로 업데이트합니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "답변: 기본 설정 언어를 한국어로 업데이트합니다.\n",
      "---\n",
      "질문: 내가 선호하는 언어는?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Echo\n",
      "Action Input: 내가 선호하는 언어는 무엇인가요?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mEchoing: 내가 선호하는 언어는 무엇인가요?\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mMy apologies for the misunderstanding. Let's go through it step-by-step. The question \"내가 선호하는 언어는?\" is asking \"What is my preferred language?\" However, as an AI, I don't know your preferences unless provided with additional context. If there's a different intention behind your question, kindly provide more information.\n",
      "\n",
      "Final Answer: Without additional information, I can't determine your preferred language.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "답변: Without additional information, I can't determine your preferred language.\n"
     ]
    }
   ],
   "source": [
    "# 사용자 설정 정보와 관련 질문\n",
    "query = \"기본 설정 언어를 한국어로 업데이트합니다.\"\n",
    "print(\"질문:\", query)\n",
    "result = agent_executor.invoke({\"input\": query})\n",
    "print(\"답변:\", result['output'])\n",
    "print('---')\n",
    "query = \"내가 선호하는 언어는?\"\n",
    "print(\"질문:\", query)\n",
    "result = agent_executor.invoke({\"input\": query})\n",
    "print(\"답변:\", result['output'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메모리 기능을 추가하기 위해 ChatMessageHistory 와 RunnableWithMessageHistory 클래스를 활용하여 대화의 기록을 저장하고, 이를 통해 에이전트가 이전 대화 내용을 기억하도록 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 클래스 정의\n",
    "\n",
    "# from pydantic import BaseModel, Field, validator\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from langchain_core.tools import StructuredTool\n",
    "from langchain_core.tools import ToolException\n",
    "\n",
    "class UserProfileInput(BaseModel):\n",
    "    name: str = Field(description=\"사용자 이름\")\n",
    "    language: str = Field(description=\"사용자 언어\")\n",
    "\n",
    "    # @validator('name')\n",
    "    @field_validator('name')\n",
    "    def validate_name(cls, v):\n",
    "        if not v or len(v) < 1:\n",
    "            raise ToolException('Name cannot be empty')\n",
    "        return v\n",
    "\n",
    "    # @validator('language')\n",
    "    @field_validator('language')\n",
    "    def validate_language(cls, v):\n",
    "        if not v or len(v) < 1:\n",
    "            raise ToolException('Language cannot be empty')\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도구 정의\n",
    "\n",
    "def update_user_profile(name: str, language: str) -> str:\n",
    "    \"\"\"이름과 선호하는 언어로 사용자 프로필을 업데이트\"\"\"\n",
    "    return f\"프로필 업데이트 완료: name: {name}, language: {language}\"\n",
    "\n",
    "def get_user_profile() -> str:\n",
    "    \"\"\"사용자의 프로필 정보를 검색\"\"\"\n",
    "    # In a real scenario, this would fetch data from a database or a file\n",
    "    return \"사용자 프로필 가져오기: name: Alex, language: 프랑스어\"\n",
    "\n",
    "update_profile_tool = StructuredTool.from_function(\n",
    "    func=update_user_profile,\n",
    "    args_schema=UserProfileInput,\n",
    "    handle_tool_error=True,\n",
    ")\n",
    "\n",
    "get_profile_tool = StructuredTool.from_function(\n",
    "    func=get_user_profile,\n",
    "    # args_schema=UserProfileInput,\n",
    "    handle_tool_error=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 기반 에이전트 설정\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're a helpful assistant\"),\n",
    "    (\"placeholder\", \"{history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "tools = [update_profile_tool, get_profile_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}  # 메시지 기록을 저장하는 더미 데이터베이스\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "agent_executor_w_memory = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메모리 사용 사례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m안녕하세요, 김철수님! 도와드릴 내용이 있으면 말씀해 주세요.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_user_profile` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m사용자 프로필 가져오기: name: Alex, language: 프랑스어\u001b[0m\u001b[32;1m\u001b[1;3m현재 프로필 정보에 따르면, 사용자의 이름은 Alex이고 선호하는 언어는 프랑스어로 되어 있습니다. 혹시 김철수로 변경하시거나 다른 정보를 업데이트하시겠어요?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '내가 누구인지 기억나?',\n",
       " 'history': [HumanMessage(content='안녕하세요, 저는 김철수입니다.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='안녕하세요, 김철수님! 도와드릴 내용이 있으면 말씀해 주세요.', additional_kwargs={}, response_metadata={})],\n",
       " 'output': '현재 프로필 정보에 따르면, 사용자의 이름은 Alex이고 선호하는 언어는 프랑스어로 되어 있습니다. 혹시 김철수로 변경하시거나 다른 정보를 업데이트하시겠어요?'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사용자 프로필 관리\n",
    "\n",
    "agent_executor_w_memory.invoke(\n",
    "    {\"input\": \"안녕하세요, 저는 김철수입니다.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user123\"}},\n",
    ")\n",
    "\n",
    "agent_executor_w_memory.invoke(\n",
    "    {\"input\": \"내가 누구인지 기억나?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 인덱스\n",
    "\n",
    "특정 작업을 수행하기 위해서는 데이터 내에 포함되지 않은 파일, 즉 외부 데이터를 처리할 수 있어야 한다.\n",
    "\n",
    "인덱스는 대규모 데이터셋에서 필요한 정보를 빠르게 찾아내는 데 매우 중요한 역할을 하며, 특히 RAG 기반 애플리케이션에서는 필수적인 요소로 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DocumentLoader\n",
    "\n",
    "DocumentLoader 는 다양한 형식의 문서를 효과적으로 로드하고 처리할 수 있는 도구  \n",
    "문서를 벡터 데이터베이스에 추가하기 전에 불필요한 정보나 중복된 내용을 걸러낼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': './ReAct README.md'}, page_content=\"# ReAct Prompting\\n\\nGPT-3 prompting code for ICLR 2023 paper [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629).\\n\\nTo use ReAct for more tasks, consider trying [LangChain's zero-shot ReAct Agent](https://python.langchain.com/docs/modules/agents/agent_types/react.html).\\n\\n## Setup\\nYou need to first have an OpenAI API key and store it in the environment variable ``OPENAI_API_KEY`` (see [here](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)).\\n\\nPackage requirement: ``openai``, and install ``alfworld`` following instructions [here](https://github.com/alfworld/alfworld).\\n\\n## Experiments\\nRun ``{hotpotqa,fever,alfworld,webshop}.ipynb``. As HotpotQA and FEVER have large validation sets, we only run 500 random examples (see notebooks). We find PaLM and GPT-3 are better at different tasks.\\n\\n\\n|                    | HotpotQA (500 random dev, EM) | FEVER (500 random dev, EM) | AlfWorld (success rate) | WebShop  (success rate) |\\n|--------------------|-------------------------------|----------------------------|-------------------------|-------------------------|\\n| PaLM-540B (paper)  | 29.4                          | 62.2                       | 70.9                    | 40                      |\\n| GPT-3 (davinci-002) | 30.4                          | 54                         | 78.4                    | 35.8                    |\\n\\n## Citation\\n\\n```bibtex\\n@inproceedings{yao2023react,\\n  title = {{ReAct}: Synergizing Reasoning and Acting in Language Models},\\n  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},\\n  booktitle = {International Conference on Learning Representations (ICLR) },\\n  year = {2023},\\n  html = {https://arxiv.org/abs/2210.03629},\\n}\\n```\\n\")]\n"
     ]
    }
   ],
   "source": [
    "# 기본 TextLoader 활용\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./ReAct README.md\")\n",
    "document = loader.load()\n",
    "\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://n.news.naver.com/mnews/article/055/0001207714', 'title': '[날씨] 아침까지 추위 계속…밤부턴 중부지방 비', 'language': 'ko'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[날씨] 아침까지 추위 계속…밤부턴 중부지방 비\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n본문 바로가기\\n\\n\\n\\n\\n\\n\\nNAVER\\n\\n뉴스\\n\\n\\n엔터\\n\\n\\n\\n\\n스포츠\\n\\n\\n\\n\\n날씨\\n\\n\\n\\n\\n프리미엄\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n검색\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n언론사별\\n\\n\\n정치\\n\\n\\n경제\\n\\n\\n사회\\n\\n\\n생활/문화\\n\\n\\nIT/과학\\n\\n\\n세계\\n\\n\\n랭킹\\n\\n\\n신문보기\\n\\n\\n오피니언\\n\\n\\nTV\\n\\n\\n팩트체크\\n\\n\\n알고리즘 안내\\n\\n\\n정정보도 모음\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSBS\\n\\nSBS\\n\\n\\n구독\\n\\nSBS 언론사 구독되었습니다. 메인 뉴스판에서  주요뉴스를  볼 수 있습니다.\\n보러가기\\n\\n\\nSBS 언론사 구독 해지되었습니다.\\n\\n\\n\\n\\n[날씨] 아침까지 추위 계속…밤부턴 중부지방 비\\n\\n\\n\\n\\n입력2024.11.20. 오전 1:00\\n\\n\\n수정2024.11.20. 오전 1:01\\n\\n기사원문\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n추천\\n\\n\\n\\n\\n쏠쏠정보\\n0\\n\\n\\n\\n\\n흥미진진\\n0\\n\\n\\n\\n\\n공감백배\\n0\\n\\n\\n\\n\\n분석탁월\\n0\\n\\n\\n\\n\\n후속강추\\n0\\n\\n\\n \\n\\n\\n\\n댓글\\n\\n\\n\\n\\n\\n텍스트 음성 변환 서비스 사용하기\\n\\n\\n\\n성별\\n남성\\n여성\\n\\n\\n말하기 속도\\n느림\\n보통\\n빠름\\n\\n이동 통신망을 이용하여 음성을 재생하면 별도의 데이터 통화료가 부과될 수 있습니다.\\n본문듣기 시작\\n\\n닫기\\n\\n\\n \\n\\n글자 크기 변경하기\\n\\n\\n\\n가1단계\\n작게\\n\\n\\n가2단계\\n보통\\n\\n\\n가3단계\\n크게\\n\\n\\n가4단계\\n아주크게\\n\\n\\n가5단계\\n최대크게\\n\\n\\n\\n\\n\\n\\nSNS 보내기\\n\\n\\n\\n인쇄하기\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t때 이른 초겨울 추위는 누그러졌지만 오늘(20일) 아침까지는 다소 춥겠습니다.\\n\\n옷차림 따뜻하게 하셔야겠습니다.\\n\\n밤부터는 중부지방에 비가 조금 내릴 텐데요.\\n\\n양은 5mm 안팎으로 많지는 않겠고 서울에도 빗방울이 떨어질 수 있겠습니다.\\n\\n오늘 하늘은 대체로 흐린 가운데 제주에는 오후까지 5~20mm가량의 비가 오겠습니다.\\n\\n중부지방은 아침 기온이 0도 안팎까지 떨어져 서리가 내리거나 물이 어는 곳이 있겠는데요.\\n\\n자세한 아침 기온 보시면 서울이 2도, 춘천 영하 1도로 중부를 중심으로 춥겠고 남부는 추위가 좀 누그러지겠습니다.\\n\\n낮 기온은 서울 11도, 광주 14도로 어제와 비슷하겠습니다.\\n\\n한동안 기온에는 큰 변화 없겠습니다.\\n\\n주 후반부터는 날도 맑아지겠고요.\\n\\n토요일에 강원 영동 지역에만 비나 눈 소식이 있습니다.\\n\\n(남유진 기상캐스터)\\n\\t\\t\\n\\n\\n\\nCopyright ⓒ SBS. All rights reserved. 무단 전재, 재배포 및 AI학습 이용 금지\\n\\n \\n이 기사는 언론사에서 생활 섹션으로 분류했습니다.\\n\\n기사 섹션 분류 안내\\n기사의 섹션 정보는 해당 언론사의 분류를 따르고 있습니다. 언론사는 개별 기사를 2개 이상 섹션으로 중복 분류할 수 있습니다.\\n닫기\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n구독\\n메인에서 바로 보는 언론사 편집 뉴스 지금 바로 구독해보세요!\\n\\n\\n구독중\\n메인에서 바로 보는 언론사 편집 뉴스 지금 바로 확인해보세요!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n네이버에서 SBS뉴스를 구독해주세요!\\n\\n\\n\\n\\n가장 확실한 SBS 제보 [클릭!]\\n\\n\\n\\n\\n\\n\\n\\n\\nSBS\\n\\nSBS\\n\\n\\t\\t\\t주요뉴스해당 언론사에서 선정하며 언론사 페이지(아웃링크)로 이동해 볼 수 있습니다.\\n\\n\\n\"살려주세요!\" 초등학교 앞 비명…\\'차량 돌진\\' 잇단 참변\\n케냐 마라톤 선수가 국내 양식장에?…대사관도 속았다\\n밀폐 공간서 차량 테스트하다…연구원 3명 사망\\n거품 목욕하다 날벼락…\\'펑\\' 욕실서 폭발해 \\'와장창\\'\\n15m 구덩이서 흙 깔려 숨진 20대…4시간 교육하고 투입?\\n\\n\\n\\n\\n\\n\\n\\n\\n이 기사를 추천합니다\\n\\n\\n기사 추천은 24시간 내 50회까지 참여할 수 있습니다.\\n닫기\\n\\n\\n\\n\\n\\n\\n쏠쏠정보\\n0\\n\\n\\n\\n\\n흥미진진\\n0\\n\\n\\n\\n\\n공감백배\\n0\\n\\n\\n\\n\\n분석탁월\\n0\\n\\n\\n\\n\\n후속강추\\n0\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n모두에게 보여주고 싶은 기사라면?beta\\n\\n이 기사를 추천합니다 버튼을 눌러주세요.  집계 기간 동안 추천을 많이 받은 기사는 네이버 자동 기사배열 영역에 추천 요소로 활용됩니다.\\n\\n레이어 닫기\\n\\n\\n \\n\\n\\nSBS 언론사홈 바로가기\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSBS 헤드라인\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n재생하기\\n재생시간02:35\\n\\n\\n[단독] \"건진법사, 사비로 \\'양재동 캠프\\' 비밀리 운영\"\\n오늘(25일)은 저희가 단독 취재한 내용으로 뉴스 시작하겠습니다. 윤석열 전 대통령이 지난 대선 때 공식 대선 캠프 말고, 비밀 선거사무소를 운영했다는 의혹이 제기돼 현재 경찰 수사가 진행되고 있습니다. 경찰은 서울\\n\\n\\n\\n\\n\\n\\n\\n재생하기\\n재생시간02:22\\n\\n\\n국회 찾은 문재인 \"부당 기소…윤 정부 3년, 퇴행의 시간\"\\n뇌물 혐의로 기소된 문재인 전 대통령이 오늘(25일) 국회를 찾아 검찰을 강하게 비판했습니다. 기소가 부당하다면서 검찰권 남용을 국민에게 알리는 데 주력해달라고 말했습니다. 손기준 기자가 보도합니다. <기자> 문재인\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n재생하기\\n재생시간02:03\\n\\n\\n시꺼먼 연기 뿜자 \"설마 폭발?\"…강남서 1,200명 \\'우르르\\'\\n평일에도 사람이 많은 서울 강남의 코엑스에서 오늘(25일) 오전 불이 나 일대가 한때 검은 연기에 뒤덮였습니다. 불은 약 2시간 만에 꺼졌고, 1천200명이 급히 대피했습니다. 김보미 기자가 취재했습니다. <기자> \\n\\n\\n\\n\\n\\n\\n\\n재생하기\\n재생시간02:17\\n\\n\\n\"허탕쳤다\" 벌써 품절 대란?…\"무조건 교체 말고\" 지침엔\\n해킹 사고가 일어난 SK텔레콤이 다음 주부터 전체 가입자를 상대로 유심을 무료로 바꿔주겠다고 밝혔습니다. 하지만 불안한 사람들이 유심을 바꾸려고 대리점에 몰리면서, 이미 재고가 다 떨어진 곳도 많습니다. 현장을 엄민\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n재생하기\\n재생시간02:17\\n\\n\\n[단독] \"따님, 타십쇼\" 앉았다 번쩍…간부가 초대한 결혼식\\n코레일 소속 직원들이 회사 간부의 딸 결혼식에 동원됐다는 제보가 왔습니다. 전통 혼례 방식으로 치러진 결혼식에서 신부의 꽃가마를 드는 역할을 했다는 겁니다. 코레일 측은 직원들이 자발적으로 참여한 거라고 해명했습니다\\n\\n\\n\\n\\n\\n\\n\\n재생하기\\n재생시간02:05\\n\\n\\n\"열리네?\" 금 75돈까지 털었다…화장실 들락날락 포착\\n한 50대 남성이 문이 잠겨 있지 않았던 차에서 금품 6000여만 원을 훔쳤다가 붙잡혔습니다. 그 남성은 범행 이후 경찰을 따돌리기 위해서 옷을 세 차례나 갈아입기도 했습니다. TJB 박범식 기자입니다. <기자> 패\\n\\n\\n\\n\\n\\n\\n\\n\\n이전 뉴스들 보기\\n\\n\\n3\\n다음 뉴스들 보기\\n\\n\\n\\n\\n\\n\\n\\nSBS가 이 기사의 댓글 정책을 결정합니다.\\n\\t\\t\\t\\t\\t\\n안내\\n댓글 정책 언론사별 선택제\\n섹션별로 기사의 댓글 제공여부와 정렬방식을 언론사가 직접 결정합니다. 기사 섹션 정보가정치/선거를 포함하는 경우 정치/선거섹션 정책이적용됩니다. 단, 운영규정에 따른삭제나 이용제한 조치는 네이버가 직접수행합니다.\\n레이어 닫기\\n\\n\\n\\n\\n\\n\\n\\nSBS 헤드라인\\n더보기\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[단독] \"건진법사, 사비로 \\'양재동 캠프\\' 비밀리 운영\"\\n2시간전\\n\\n\\n\\n\\n재생하기\\n\\n\\n\\n\\n\\n\\n\\n국회 찾은 문재인 \"부당 기소…윤 정부 3년, 퇴행의 시간\"\\n2시간전\\n\\n\\n\\n\\n재생하기\\n\\n\\n\\n\\n\\n\\n\\n시꺼먼 연기 뿜자 \"설마 폭발?\"…강남서 1,200명 \\'우르르\\'\\n2시간전\\n\\n\\n\\n\\n재생하기\\n\\n\\n\\n\\n\\n\\n\\n\"허탕쳤다\" 벌써 품절 대란?…\"무조건 교체 말고\" 지침엔\\n1시간전\\n\\n\\n\\n\\n재생하기\\n\\n\\n\\n\\n\\n\\n\\n[단독] \"따님, 타십쇼\" 앉았다 번쩍…간부가 초대한 결혼식\\n1시간전\\n\\n\\n\\n\\n재생하기\\n\\n\\n\\n\\n\\n\\n\\n\"열리네?\" 금 75돈까지 털었다…화장실 들락날락 포착\\n1시간전\\n\\n\\n\\n\\n재생하기\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n네이버 AI 뉴스 알고리즘\\n뉴스 추천 알고리즘이 궁금하다면?\\n\\n\\n\\n\\n\\n\\nSBS 랭킹 뉴스\\n오후 9시~10시까지 집계한 결과입니다.\\n오후 7시~10시까지 집계한 결과입니다.\\n더보기\\n더보기\\n\\n\\n\\n\\n\\n많이 본\\n\\n\\n댓글 많은\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1\\n\\n[단독] \"따님, 타십쇼\" 앉았다 번쩍…간부가 초대한 결혼식\\n1시간전\\n\\n\\n\\n\\n재생하기\\n\\n\\n\\n더보기\\n\\n\\n\\n\\n\\n2\\n\\n시꺼먼 연기 뿜자 \"설마 폭발?\"…강남서 1,200명 \\'우르르\\'\\n2시간전\\n\\n\\n\\n\\n재생하기\\n\\n\\n\\n더보기\\n\\n\\n\\n\\n\\n3\\n\\n[단독] \"건진법사, 사비로 \\'양재동 캠프\\' 비밀리 운영\"\\n2시간전\\n\\n\\n\\n\\n재생하기\\n\\n\\n\\n더보기\\n\\n\\n\\n\\n\\n4\\n\\n\"허탕쳤다\" 벌써 품절 대란?…\"무조건 교체 말고\" 지침엔\\n1시간전\\n\\n\\n\\n\\n재생하기\\n\\n\\n\\n더보기\\n\\n\\n\\n\\n\\n5\\n\\n\"열리네?\" 금 75돈까지 털었다…화장실 들락날락 포착\\n1시간전\\n\\n\\n\\n\\n재생하기\\n\\n\\n\\n더보기\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1\\n\\n문, 국회 찾아 \\'작심 발언\\'…\"대한민국 퇴행의 결정판\"\\n5시간전\\n\\n\\n\\n\\n재생하기\\n\\n\\n\\n더보기\\n\\n\\n\\n\\n\\n2\\n\\n\"한국 맞아?\" 비상구로 \\'우르르\\'…백화점 \\'아수라장\\'\\n5시간전\\n\\n\\n\\n\\n재생하기\\n\\n\\n\\n더보기\\n\\n\\n\\n\\n\\n3\\n\\n[단독] \"건진법사, 사비로 \\'양재동 캠프\\' 비밀리 운영\"\\n2시간전\\n\\n\\n\\n\\n재생하기\\n\\n\\n\\n더보기\\n\\n\\n\\n\\n\\n4\\n\\n류희림 방심위원장 사의 표명\\n4시간전\\n\\n\\n\\n\\n\\n\\n\\n더보기\\n\\n\\n\\n\\n\\n5\\n\\n국회 찾은 문재인 \"부당 기소…윤 정부 3년, 퇴행의 시간\"\\n2시간전\\n\\n\\n\\n\\n재생하기\\n\\n\\n\\n더보기\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n함께 볼만한 뉴스\\n안내\\n\\n이 기사를 본 이용자들이 함께 많이 본 기사, 해당 기사와 유사한 기사, 관심 기사 등을 자동 추천합니다\\n닫기\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n남편 구속 4개월 만…‘옥바라지’ 성유리 홈쇼핑으로 복귀\\n걸그룹 핑클 멤버 성유리(44)가 남편인 프로골퍼 안성현(44)씨가 법정구속된 지 4개월 여만에 홈쇼핑으로 복귀한다. GS홈쇼핑은 최근 공식 소셜미디어에 신규 프로그램을 소개하는 영상을 게재했다. GS홈쇼핑은 해당 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n“살 울룩불룩, 마흔 넘으니 야속”… 42세 김빈우, 올린 사진 봤더니 ‘아닌데’?\\n배우 김빈우(42)가 다이어트에 대한 고민을 털어놓았다. 24일 김빈우는 자신의 인스타그램에 체중 관리 중인 사진을 여러 장 공개했다. 사진과 함께 그는 “몇 달을 꾸준히 운동하고 조절하고 관리해도 3~4일 고삐 풀\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n54% “출마 반대”에도… 한덕수 ‘李 대항마’로 보수 선두권\\n국민 과반이 한덕수 대통령 권한대행의 6·3 대선 출마를 반대하는 것으로 조사됐다. 다만 국민의힘 지지층 내에서는 한 권한대행의 출마를 바라는 응답이 68%로 나타났다. ‘대선 차출론’이 이어지지만 한 권한대행은 아\\n\\n\\n\\n\\n\\n\\n\\n재생하기\\n재생시간02:15\\n\\n\\n류희림 방심위원장 돌연 사의‥\"대선 전 위원장 교체 \\'알박기\\' 의심\"\\n◀ 앵커 ▶ 가족과 지인 등을 동원해 비판적인 언론사들을 겨냥한 민원을 넣게 한 의혹으로 수사를 받는, 류희림 방송통신심의위원장이 오늘 오후 사의를 표명했습니다. 의혹이 불거진 1년 반 동안 연임까지 하며 물러설 뜻\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n“성관계 해주면 1000만원”…평창동 80대의 황당한 가사도우미 제안\\n평창동에서 가사도우미를 구한다는 공고에 지원한 40대 여성이 집주인으로부터 불건전한 제안을 받은 사연이 전해졌다. 24일 방송된 JTBC ‘사건반장’에서는 몸이 아픈 엄마를 돌보며 아이까지 키우는 40대 싱글맘 A씨\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"이재명도 품어야\" 지지자 조언에 홍준표 답변은?\\n홍준표 국민의힘 대선 경선 후보가 진보 성향 방송인 김어준 씨의 방송에 출연할 계획이라고 밝혔습니다. 어제(24일) 홍 후보의 온라인 소통 플랫폼 \\'청년의꿈\\'에는 \\'정국불안정을 해소하기 위해서는 윤석열 전 대통령도 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"진짜 없어서 못 판다던 \\'박재범 소주\\' 어쩌다\"…원소주 요즘 근황 보니\\n배우 겸 래퍼 박재범이 론칭한 프리미엄 증류식 소주 브랜드 ‘원소주’ 제조사인 원스피리츠가 전년도 감사보고서를 제출하지 않기로 하면서 그 배경에 관심이 쏠린다. 25일 관련 업계에 따르면 원스피리츠 측은 “올해는 감\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"사람 죽여 가둬놨다\"...또 살인 저지른 박찬성 신상 공개\\n교도소 출소 9개월 만에 또 살인을 저지른 박찬성(64)의 신상이 공개됐다. 대전지검 형사 제3부는 함께 살던 지인을 살해한 혐의(살인)로 박찬성을 구속기소하고 그의 신상을 공개했다고 25일 밝혔다. 박찬성은 지난 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n지하철서 중요부위 노출한 군복男, 진짜 군인이었다 \\'충격\\'\\n신분당선 지하철에서 군복을 입은 채 신체 중요 부위를 노출한 혐의(공연음란)를 받는 20대 남성의 신원이 특정됐다. 실제 현직 군인이었던 것. 25일 중앙일보에 따르면 이날 서울 서초경찰서는 폐쇄회로(CC)TV 등을\\n\\n\\n\\n\\n\\n\\n\\n재생하기\\n재생시간01:31\\n\\n\\n[단독] 처음 본 여성 무차별 폭행…수원서 출근길 난동\\n오늘 오전 경기 수원역 인근에서 일면식도 없는 여성을 무차별 폭행한 30대 남성이 경찰에 붙잡혔습니다. 남성은 폭행을 말리던 행인에게도 주먹을 휘둘렀는데, 출근길에 벌어진 이른바 \\'묻지마 폭행\\'에 시민들은 공포에 떨\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n삼성, \\'해킹사고\\' SKT 이용 임원들에 \"빨리 유심 교체해라\"(종합)\\n삼성이 해킹 사고가 발생한 SK텔레콤을 이용하는 임원들에게 서둘러 유심(USIM) 교체에 나서라고 주문한 것으로 알려졌다. 25일 재계에 따르면 삼성은 최근 주요 계열사 임원들을 상대로 \"SK텔레콤 이용자는 유심을 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n트럼프 “한국에 군사비 수십억 달러 지불… 별도로 처리할 것”\\n취임 100일 앞두고 타임지 인터뷰 도널드 트럼프 미국 대통령은 25일 공개된 시사 주간지 타임(TIME)과의 인터뷰에서 “우리는 한국에 군사 비용으로 수십억 달러를 지불하고 일본에도 수십억 달러를 지불하고 있다”며\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n트럼프 “시진핑이 전화 걸어와···관세 문제, 3~4주 내 합의할 것”\\n중국과 무역갈등을 벌이고 있는 도널드 트럼프 미국 대통령이 시진핑 국가주석이 전화를 걸어와 통화했다고 밝혔다. 도널드 트럼프 대통령은 25일 공개된 미국 시사주간지 타임(TIME)과 가진 인터뷰에서 “시 주석이 전화\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n‘앙숙’ 안철수·이준석, 판교역 광장서 ‘포옹’…빅텐트 펼치나\\n오랜 정치적 ‘앙숙’인 안철수 국민의힘 대선 경선 후보와 이준석 개혁신당 대선 후보가 25일 경기 성남시 분당구 판교역 광장에서 서로를 힘껏 끌어안았다. 두 사람의 만남은 6·3 대선을 앞두고 ‘반이재명 빅텐트’가 \\n\\n\\n\\n\\n\\n\\n\\n재생하기\\n재생시간02:18\\n\\n\\n[JTBC 여론조사] 이재명, 누구와 붙어도 50% 근접…경쟁력 1위는 한덕수\\n대선을 39일 앞두고 JTBC가 여론조사를 실시했습니다. 가상 3자 대결에서 이재명 민주당 대선 경선 후보가 어떤 후보와 만나든 오차범위 밖에서 크게 앞섰습니다. 이 후보를 상대로 가장 높은 경쟁력을 보인 건 한덕수\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n“내 남편은 게이, 임신 위해 성관계해”…이 부부의 결혼 방식, 어떻길래?\\n한 여성이 게이 남성과 결혼한 후 임신을 시도 중이라는 사실을 공개해 화제가 되고 있다. 영국 일간 미러에 따르면 사만다 그린스톤과 조쉬 호프 부부는 한눈에 보기에도 독특한 관계를 유지하며 새로운 도전을 앞두고 있다\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n전 세계 성인들 떨게 만든 대장암…\\'이것\\' 먹으면 발병률 58% 줄어든다\\n전 세계적으로 젊은 성인의 대장암 발병률이 증가하는 가운데 비타민D가 대장암 예방과 치료에 효과적이라는 연구 결과가 나왔다. 19일(현지 시간) 폭스뉴스 등 보도에 따르면 최근 헝가리 연구진은 국제 영양학 학술지 ‘\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n설마했는데 “쿠팡에 완전 밀렸다”…2위 뺏긴 티빙, “너무 볼게 없다” 아우성\\n티빙 오리지널 ‘스터디그룹’ 촬영 현장 [배우 한지은 SNS 갈무리] 넷플릭스에 대항해 ‘힘겨운’ 2위를 이어가던 티빙이 결국 온라인동영상플랫폼(OTT) 시장 2위 자리마저 내줬다. 쿠팡플레이가 과감한 콘텐츠 전략으\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n김정은이 자랑하던 ‘53층 아파트’ 10년 만에 붕괴 위기, 왜?\\n김정은 주도로 평양 한복판에 세워진 53층 고층 아파트가 심각한 균열과 부식으로 붕괴 우려를 낳고 있다. 초고층 주상복합아파트 ‘은하’는 핵·미사일 과학자들에게 배정된 상징적 건축물이다. 사진=연합뉴스, 조선중앙TV\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n보수 단일화 공감대에…한덕수, 30일 총리 사퇴후 출마 밝힐 듯\\n한덕수 대통령 권한대행 국무총리가 이르면 30일경 사퇴하고 대선 출마에 대한 입장을 밝힐 것으로 알려졌다. 국민의힘 대선 주자들이 모두 후보 단일화에 찬성하는 입장으로 돌아선 가운데 한 권한대행이 대선 출마를 선언할\\n\\n\\n\\n\\n\\n\\n\\n재생하기\\n재생시간02:35\\n\\n\\n[단독] \"네 달 맡겼다 손실 났다\"‥\\'尹 허위사실 공표\\' 수사 재개\\n◀ 앵커 ▶ 검찰이 윤석열 전 대통령의 공직선거법상 허위사실 공표 혐의에 대해, 수사를 재개한 사실이 확인됐습니다. 대선 후보 시절 김건희 여사의 도이치모터스 주가조작 의혹과 관련해 했던 발언이 허위인지를 두고, 2\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n건보료 안 냈는데 39억 돌려준 공단…장기고액 체납자에 초과금 지급\\n국민건강보험공단이 2021~2024년 건강보험료 장기고액 체납자 4089명에게 되레 약 39억원을 지급한 것으로 나타났다. 건보공단은 환자 개인이 부담하는 본인부담금(비급여, 선별급여 등을 제외한 환자 본인이 부담하\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[단독]매일 마시는 물인데···‘공업용수로도 못 쓸’ 오염된 지하수 어쩌나\\n전국 곳곳에서 식수나 생활·농업·공업용수 등으로 사용되는 지하수가 중금속, 세균 등에 오염돼 있다는 조사 결과가 나왔다. 비소 중독으로 인한 노동자 사망사고를 비롯해 다양한 환경오염을 일으키고 있다는 비판을 받고 있\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n“‘이 음식’ 꼭 쟁여두세요”… ‘28kg 감량’ 진서연, 다이어트 필수템으로 추천한 건?\\n배우 진서연(42)이 다이어트 중 먹기 좋은 식품으로 두부를 꼽았다. 지난 24일 진서연은 자신의 인스타그램 스토리에 Q&A를 진행했다. 한 팬은 “계속 먹어도 배가 고프다”며 고민을 토로했다. 이에 진서연은 “포만\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n재생하기\\n재생시간02:22\\n\\n\\n[JTBC 여론조사] \\'국민의힘 후보 적합도\\' 물었더니…1차 경선 후 \\'달라진 지형\\'\\n국민의힘은 나흘 뒤인 오는 29일, 대선 경선 후보를 2명으로 압축할 예정입니다. JTBC는 이 결과를 미리 가늠해볼 수 있는 여론조사도 진행했는데, 오차범위 안이긴 하지만 지난 조사 때에서 순위 변동이 있었습니다.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[단독]\"170만원 드릴게요. 37살 여자가 탄 제차를 박는 겁니다\"…\\'고액 알바\\' 둔갑한 \\'가피 공모\\'\\n공격수 구함. 자차 보유, 종합보험, 확실한 분만.\" 지난 15일 사회관계망서비스(SNS)에 \\'고액 알바\\'를 모집한다는 글을 보고 텔레그램을 통해 기자는 A씨에게 연락했다. A씨는 자동차 고의 충돌 사고를 일으키는\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[속보]박지원 “정통한 소식통 확인, 이재명 대법 무죄 확정”\\n뉴시스 대법원이 이재명 더불어민주당 전 대표 공직선거법 위반 상고심 재판을 전원합의체 회부해 신속 처리에 나선 가운데 박지원 더불어민주당 의원은 “정통한 소식통에 들었다”며 “어떤 경우에도 파기환송은 되지 않고 원심\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"원시인처럼 뼛국물 먹는다\"던 미인 여배우, 2년뒤 깜짝 근황\\n2년 전 야채와 뼛국물 등으로 구성된 자신의 식단을 건강식으로 소개했다가 \\'섭식 장애\\' 논란을 일으켰던 할리우드 톱스타 귀네스 팰트로(52)가 이제는 엄격한 식단에 집착하지 않는다고 밝혔다. 24일(현지시간) 미 연\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"돈 안 아낄래\" 100만원씩 턱턱…2030女 열광한 곳이\\n“한정판 티셔츠는 판매 2일 차 오전에 이미 품절됐습니다. 팝업(스토어) 첫날에만 매장에 6000명이 몰렸어요.” 지난 24일 서울 여의도 더현대 서울 지하 2층에 위치헌 콜드플레이 굿즈 팝업 매장. 한 손님이 굿즈\\n\\n\\n\\n\\n\\n\\n\\n재생하기\\n재생시간02:04\\n\\n\\n성심당 케이크에 곰팡이가…위생 논란에 “판매 중단”\\n대전 명물로 자리잡은 빵집, 성심당에서 위생논란이 불거졌습니다. 인기 케이크인 딸기 시루에서 곰팡이가 발견된 건데요. 김대욱 기자입니다. [기자] 케이크 위에 쌓인 딸기 여러 개에 검은색 곰팡이가 피어 있습니다. 케\\n\\n\\n\\n\\n\\n\\n\\n\\n이전 뉴스들 보기\\n\\n\\n5\\n다음 뉴스들 보기\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t함께 볼만한 프리미엄\\n\\t\\t\\t유료\\n\\n안내\\n\\n프리미엄콘텐츠는 네이버가 인터넷뉴스 서비스사업자로서 제공, 매개하는 기사가 아니고, 해당 콘텐츠 제공자가 프리미엄 회원을 대상으로 별도로 발행·제공하는 콘텐츠입니다.\\n닫기\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n기간한정무료\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tA2MAC1, “자동차 심장부터 뼈대까지 모든 정보 분석”\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\ne4ds news\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n기간한정무료\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGLOBAL AGENDA <250425>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\nAGENDA & ISSUE NEWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAfter Report(2025.04.25.금)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n29PER\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t타나 토라자 술라웨시 인도네시아\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n30년간의 세계일주\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t전 국민이 트럼프에 열광하는 나라\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n지식부장관 글로벌 인사이트\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t커피 가격이 계속 오르면\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n커피팟\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n기간한정무료\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t삼성-구글 \\'안드로이드 XR\\', 연내 출시 전망 나오는 이유\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n이포커스\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t[04.23 수 / 축구] FC안양 vs 울산HDFC 승부 예측 프리뷰\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n스포츠폴리오\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t세계 비물질유산 (무형문화유산) 28. 그림자 연극 [皮影戱, 피잉시]\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\nNo. 1 차이나 리포트\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t이랜드월드, 이월드 주담대 담보비율 조정 배경은\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\nthebellstock\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t관광 도시 넘어 혁신 도시로, 브로츠와프\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\nCHIEF EXECUTIVE\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t7년 전 \\'트리플 역세권\\' 선구안…한효주 27억 빌딩, 2배 올라\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n부동산 파노라마\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n이전 콘텐츠들 보기\\n\\n\\n3\\n다음 콘텐츠들 보기\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n구독\\n\\n\\n\\n\\n\\n\\n본문 듣기를 종료하였습니다.\\n\\n이 기사를 추천했습니다.\\n\\n추천을 취소했습니다.\\n\\n\\n\\n\\n\\n\\n\\n로그인\\n\\n\\n전체서비스\\n\\n\\n\\n\\n서비스안내\\n\\n\\n오류신고\\n\\n\\n고객센터\\n\\n\\n\\n기사배열 책임자 : 김수향\\n청소년 보호 책임자 : 이정규\\n\\n\\n\\t\\t\\t\\t\\t각 언론사가 직접 콘텐츠를 편집합니다.\\n\\t\\t\\t\\t\\tⓒ SBS\\n\\n\\n\\t\\t\\t\\t\\t이 콘텐츠의 저작권은 저작권자 또는 제공처에 있으며, 이를 무단 이용하는 경우 저작권법 등에 따라 법적 책임을 질 수 있습니다.\\n\\t\\t\\t\\t\\n\\nNAVER\\n\\n\\n\\n\\n\\n맨위로\\n\\n\\n\\n\\n\\n예\\n아니오\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "# 웹 페이지 URL 활용\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# 웹 페이지 로더 설정\n",
    "url = \"https://n.news.naver.com/mnews/article/055/0001207714\"\n",
    "loader = WebBaseLoader(url)\n",
    "\n",
    "# 웹 페이지 내용을 로드\n",
    "document = loader.load()\n",
    "\n",
    "# 결과 확인\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Using cached pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from pypdf) (4.13.2)\n",
      "Using cached pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 0, 'page_label': '1'}, page_content='Published as a conference paper at ICLR 2023\\nREAC T: S YNERGIZING REASONING AND ACTING IN\\nLANGUAGE MODELS\\nShunyu Yao∗*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\\n1Department of Computer Science, Princeton University\\n2Google Research, Brain team\\n1{shunyuy,karthikn}@princeton.edu\\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\\nABSTRACT\\nWhile large language models (LLMs) have demonstrated impressive performance\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\\nplan generation) have primarily been studied as separate topics. In this paper, we\\nexplore the use of LLMs to generate both reasoning traces and task-speciﬁc actions\\nin an interleaved manner, allowing for greater synergy between the two: reasoning\\ntraces help the model induce, track, and update action plans as well as handle\\nexceptions, while actions allow it to interface with and gather additional information\\nfrom external sources such as knowledge bases or environments. We apply our\\napproach, named ReAct, to a diverse set of language and decision making tasks\\nand demonstrate its effectiveness over state-of-the-art baselines in addition to\\nimproved human interpretability and trustworthiness. Concretely, on question\\nanswering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent\\nissues of hallucination and error propagation in chain-of-thought reasoning by\\ninteracting with a simple Wikipedia API, and generating human-like task-solving\\ntrajectories that are more interpretable than baselines without reasoning traces.\\nFurthermore, on two interactive decision making benchmarks (ALFWorld and\\nWebShop), ReAct outperforms imitation and reinforcement learning methods by\\nan absolute success rate of 34% and 10% respectively, while being prompted with\\nonly one or two in-context examples.\\n1 I NTRODUCTION\\nA unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\\nverbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\\nplay an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\\n1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-\\nsider the example of cooking up a dish in the kitchen. Between any two speciﬁc actions, we may\\nreason in language in order to track progress (“now that everything is cut, I should heat up the pot of\\nwater”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let\\nme use soy sauce and pepper instead”), and to realize when external information is needed (“how do\\nI prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the\\nrecipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What\\ndish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans\\nto learn new tasks quickly and perform robust decision making or reasoning, even under previously\\nunseen circumstances or facing information uncertainties.\\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision\\nmaking in autonomous systems. On one hand, properly prompted large language models (LLMs)\\nhave demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\\n∗Work during Google internship. Projet page with code: https://react-lm.github.io/.\\n1\\narXiv:2210.03629v3  [cs.CL]  10 Mar 2023'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 1, 'page_label': '2'}, page_content='Published as a conference paper at ICLR 2023\\n$FW\\x03\\x14\\x1d\\x037KLQN>)LUVW\\x03,\\x03QHHG\\x03WR\\x03ILQG\\x03D\\x03SHSSHU\\x03VKDNHU\\x11\\x11\\x11PRUH\\x03\\nOLNHO\\\\\\x03WR\\x03DSSHDU\\x03LQ\\x03FDELQHWV\\x03\\x0b\\x14\\x10\\x19\\x0c\\x0f\\x03FRXQWHUWRSV\\x03\\x0b\\x14\\x10\\x16\\x0c\\x0f\\x03\\x11\\x11\\x11\\x03@\\n$FW\\x03\\x15\\x1d\\x03*R\\x03WR\\x03FDELQHW\\x03\\x14\\x03\\n2EV\\x03\\x15\\x1d\\x032Q\\x03WKH\\x03FDELQHW\\x03\\x14\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03YDVH\\x03\\x15\\x11\\x03\\n\\x0b+HUH\\x03WKH\\x03DJHQW\\x03JR\\x03WR\\x03FDELQHW\\x03\\x14\\x0f\\x03WKHQ\\x03FDELQHW\\x03\\x15\\x0f\\x03\\x16\\x0f\\x03WKHQ\\x03FRXQWHUWRS\\x03\\x14\\x03DQG\\x03\\x15\\x0c\\n$FW\\x03\\x19\\x1d\\x03*R\\x03WR\\x03FRXQWHUWRS\\x03\\x16\\x032EV\\x03\\x19\\x1d\\x032Q\\x03WKH\\x03FRXQWHUWRS\\x03\\x16\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03DSSOH\\x03\\x14\\x0f\\x03D\\x03EUHDG\\x03\\x14\\x0f\\x03\\n\\x11\\x11\\x11\\x11\\x11\\x11\\x03D\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x0f\\x03DQG\\x03D\\x03YDVH\\x03\\x16\\x11\\n$FW\\x03\\x1a\\x1d\\x037DNH\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x03IURP\\x03FRXQWHUWRS\\x03\\x16\\x03\\n2EV\\x03\\x1a\\x1d\\x03<RX\\x03SLFN\\x03XS\\x03WKH\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x03IURP\\x03WKH\\x03FRXQWHUWRS\\x03\\x16\\x11\\x03\\n$FW\\x03\\x1b\\x1d\\x037KLQN>1RZ\\x03,\\x03ILQG\\x03D\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03WR\\x03\\nSXW\\x03LW\\x03LQ\\x12RQ\\x03GUDZHU\\x03\\x14\\x11@\\n$FW\\x03\\x1c\\x1d\\x03*R\\x03WR\\x03GUDZHU\\x03\\x14\\x03\\n2EV\\x03\\x1c\\x1d\\x03\\'UDZHU\\x03\\x14\\x03LV\\x03FORVHG\\x11\\x03\\n$FW\\x03\\x14\\x13\\x1d\\x032SHQ\\x03GUDZHU\\x03\\x14\\x03\\n2EV\\x03\\x14\\x13\\x1d\\x03<RX\\x03RSHQ\\x03\\'UDZHU\\x03\\x14\\x03ª\\n$FW\\x03\\x14\\x14\\x1d\\x033XW\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x03LQ\\x12RQ\\x03GUDZHU\\x03\\x14\\x03\\n2EV\\x03\\x14\\x14\\x1d\\x03<RX\\x03SXW\\x03SHSSHU\\x03VKDNHU\\x03\\x14\\x03LQ\\x12RQ\\x03WKH\\x03GUDZHU\\x03\\x14\\x11\\n\\x0b\\x14D\\x0c\\x036WDQGDUG\\n$QVZHU\\x1d\\x03L3RG\\n\\x0b\\x14E\\x0c\\x03&R7\\x03\\x0b5HDVRQ\\x032QO\\\\\\x0c\\n7KRXJKW\\x1d\\x03/HW\\nV\\x03WKLQN\\x03VWHS\\x03\\nE\\\\\\x03VWHS\\x11\\x03$SSOH\\x035HPRWH\\x03ZDV\\x03\\nRULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03\\nLQWHUDFW\\x03ZLWK\\x03$SSOH\\x0379\\x11\\x03\\n$SSOH\\x0379\\x03FDQ\\x03EH\\x03FRQWUROOHG\\x03\\nE\\\\\\x03L3KRQH\\x0f\\x03L3DG\\x0f\\x03DQG\\x03L3RG\\x03\\n7RXFK\\x11\\x036R\\x03WKH\\x03DQVZHU\\x03LV\\x03\\nL3KRQH\\x0f\\x03L3DG\\x0f\\x03DQG\\x03L3RG\\x03\\n7RXFK\\x11\\n$QVZHU\\x1d\\x03L3KRQH\\x0f\\x03L3DG\\x0f\\x03L3RG\\x03\\n7RXFK\\n\\x0b\\x14F\\x0c\\x03$FW\\x102QO\\\\\\x03\\n$FW\\x03\\x14\\x1d\\x036HDUFK>$SSOH\\x035HPRWH@\\x03\\n2EV\\x03\\x14\\x1d\\x037KH\\x03$SSOH\\x035HPRWH\\x03LV\\x03D\\x03\\nUHPRWH\\x03FRQWURO\\x03ª\\n$FW\\x03\\x15\\x1d\\x036HDUFK>)URQW\\x035RZ@\\x03\\n2EV\\x03\\x15\\x1d\\x03&RXOG\\x03QRW\\x03ILQG\\x03>)URQW\\x03\\n5RZ@\\x11\\x036LPLODU\\x1d\\x03ª\\n$FW\\x03\\x16\\x1d\\x036HDUFK>)URQW\\x035RZ\\x03\\n\\x0bVRIWZDUH\\x0c@\\x03\\n2EV\\x03\\x16\\x1d\\x03)URQW\\x035RZ\\x03LV\\x03D\\x03\\nGLVFRQWLQXHG\\x03PHGLD\\x03FHQWHU\\x03\\nVRIWZDUH\\x03ª\\n$FW\\x03\\x17\\x1d\\x03)LQLVK>\\\\HV@\\n\\x0b\\x14G\\x0c\\x035H$FW\\x03\\x0b5HDVRQ\\x03\\x0e\\x03$FW\\x0c\\n7KRXJKW\\x03\\x14\\x1d\\x03,\\x03QHHG\\x03WR\\x03VHDUFK\\x03$SSOH\\x035HPRWH\\x03DQG\\x03ILQG\\x03WKH\\x03\\nSURJUDP\\x03LW\\x03ZDV\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03LQWHUDFW\\x03ZLWK\\x11\\n$FW\\x03\\x14\\x1d\\x036HDUFK>$SSOH\\x035HPRWH@\\n2EV\\x03\\x14\\x1d\\x037KH\\x03$SSOH\\x035HPRWH\\x03LV\\x03D\\x03UHPRWH\\x03FRQWURO\\x03LQWURGXFHG\\x03LQ\\x03\\n2FWREHU\\x03\\x15\\x13\\x13\\x18\\x03E\\\\\\x03$SSOH\\x03ª\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03FRQWURO\\x03WKH\\x03\\n)URQW\\x035RZ\\x03PHGLD\\x03FHQWHU\\x03SURJUDP\\x03ª\\n7KRXJKW\\x03\\x15\\x1d\\x03$SSOH\\x035HPRWH\\x03ZDV\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03FRQWURO\\x03\\nWKH\\x03)URQW\\x035RZ\\x03PHGLD\\x03FHQWHU\\x03SURJUDP\\x11\\x03,\\x03QHHG\\x03WR\\x03VHDUFK\\x03)URQW\\x03\\n5RZ\\x03QH[W\\x03DQG\\x03ILQG\\x03ZKDW\\x03RWKHU\\x03GHYLFH\\x03FDQ\\x03FRQWURO\\x03LW\\x11\\n$FW\\x03\\x15\\x1d\\x036HDUFK>)URQW\\x035RZ@\\n2EV\\x03\\x15\\x1d\\x03&RXOG\\x03QRW\\x03ILQG\\x03>)URQW\\x035RZ@\\x11\\x036LPLODU\\x1d\\x03>\\n)URQW\\x035RZ\\x03\\n6HDW\\x03WR\\x03(DUWK\\n\\x0f\\x03\\n)URQW\\x035RZ\\x030RWRUVSRUWV\\n\\x0f\\n)URQW\\x035RZ\\x03\\n\\x0bVRIWZDUH\\x0c\\n\\x0f\\x03ª@\\n7KRXJKW\\x03\\x16\\x1d\\x03)URQW\\x035RZ\\x03LV\\x03QRW\\x03IRXQG\\x11\\x03,\\x03QHHG\\x03WR\\x03VHDUFK\\x03)URQW\\x03\\n5RZ\\x03\\x0bVRIWZDUH\\x0c\\x03\\x11\\n$FW\\x03\\x16\\x1d\\x036HDUFK>)URQW\\x035RZ\\x03\\x0bVRIWZDUH\\x0c@\\n2EV\\x03\\x16\\x1d\\x03)URQW\\x035RZ\\x03LV\\x03D\\x03GLVFRQWLQXHG\\x03PHGLD\\x03FHQWHU\\x03VRIWZDUH\\x03ª\\n7KRXJKW\\x03\\x17\\x1d\\x03)URQW\\x035RZ\\x03\\x0bVRIWZDUH\\x0c\\x03LV\\x03FRQWUROOHG\\x03E\\\\\\x03DQ\\x03$SSOH\\x03\\n5HPRWH\\x03RU\\x03WKH\\x03NH\\\\ERDUG\\x03IXQFWLRQ\\x03NH\\\\V\\x11\\x036R\\x03WKH\\x03DQVZHU\\x03LV\\x03\\nNH\\\\ERDUG\\x03IXQFWLRQ\\x03NH\\\\V\\x11\\n$FW\\x03\\x17\\x1d\\x03)LQLVK>NH\\\\ERDUG\\x03IXQFWLRQ\\x03NH\\\\V@\\nҼ\\n4XHVWLRQ\\x1d\\x03$VLGH\\x03IURP\\x03WKH\\x03$SSOH\\x035HPRWH\\x0f\\x03ZKDW\\x03RWKHU\\x03GHYLFH\\x03\\nFDQ\\x03FRQWURO\\x03WKH\\x03SURJUDP\\x03$SSOH\\x035HPRWH\\x03ZDV\\x03RULJLQDOO\\\\\\x03\\nGHVLJQHG\\x03WR\\x03LQWHUDFW\\x03ZLWK\"\\n\\x0b\\x15D\\x0c\\x03$FW\\x102QO\\\\\\x03$FW\\x03\\x14\\x1d\\x03*R\\x03WR\\x03GUDZHU\\x03\\x14\\x03\\n2EV\\x03\\x14\\x1d\\x037KH\\x03GUDZHU\\x03\\x14\\x03LV\\x03FORVHG\\x11\\x03\\n$FW\\x03\\x15\\x1d\\x032SHQ\\x03GUDZHU\\x03\\x14\\x03\\n2EV\\x03\\x15\\x1d\\x03<RX\\x03RSHQ\\x03WKH\\x03GUDZHU\\x03\\x14\\x11\\x037KH\\x03GUDZHU\\x03\\x14\\x03LV\\x03RSHQ\\x11\\x03,Q\\x03LW\\x0f\\x03\\n\\\\RX\\x03VHH\\x03D\\x03GLVKVSRQJH\\x03\\x15\\x0f\\x03DQG\\x03D\\x03VSRRQ\\x03\\x14\\x11\\n\\x03\\n$FW\\x03\\x16\\x1d\\x03*R\\x03WR\\x03VLQNEDVLQ\\x03\\x14\\x03\\n2EV\\x03\\x16\\x1d\\x032Q\\x03WKH\\x03VLQNEDVLQ\\x03\\x14\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03GLVKVSRQJH\\x03\\x16\\x0f\\x03D\\x03\\nVSDWXOD\\x03\\x14\\x0f\\x03DQG\\x03D\\x03VSRRQ\\x03\\x15\\x11\\x03\\n$FW\\x03\\x17\\x1d\\x037DNH\\x03SHSSHUVKDNHU\\x03\\x14\\x03IURP\\x03VLQNEDVLQ\\x03\\x14\\x03\\n2EV\\x03\\x17\\x1d\\x031RWKLQJ\\x03KDSSHQV\\x11\\x03\\n$FW\\x03\\x18\\x1d\\x037DNH\\x03SHSSHUVKDNHU\\x03\\x14\\x03IURP\\x03VLQNEDVLQ\\x03\\x14\\x03\\n2EV\\x03\\x18\\x1d\\x031RWKLQJ\\x03KDSSHQV\\x11\\x03\\n\\x0b\\x15E\\x0c\\x035H$FW\\x03\\x0b5HDVRQ\\x03\\x0e\\x03$FW\\x0c\\nҼ\\n<RX\\x03DUH\\x03LQ\\x03WKH\\x03PLGGOH\\x03RI\\x03D\\x03URRP\\x11\\x03/RRNLQJ\\x03TXLFNO\\\\\\x03DURXQG\\x03\\n\\\\RX\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03FDELQHW\\x03\\x19\\x0f\\x03D\\x03FDELQHW\\x03\\x14\\x0f\\x03D\\x03FRIIHH\\x03PDFKLQH\\x03\\x14\\x0f\\x03\\nD\\x03FRXQWHUWRS\\x03\\x16\\x0f\\x03\\x03D\\x03VWRYH\\x03EXUQHU\\x03\\x14\\x0f\\x03DQG\\x03D\\x03WRDVWHU\\x03\\x14\\x11\\x03\\n<RXU\\x03WDVN\\x03LV\\x03WR\\x1d\\x033XW\\x03VRPH\\x03SHSSHU\\x03VKDNHU\\x03RQ\\x03D\\x03GUDZHU\\x11\\n\\x0b\\x15\\x0c\\x03$OI:RUOG\\n\\x0b\\x14\\x0c\\x03+RWVSRW\\x034$\\nFigure 1: (1) Comparison of 4 prompting methods, (a) Standard, (b) Chain-of-thought ( CoT,\\nReason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)\\nquestion. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar\\net al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task\\nsolving trajectories generated by the model (Act, Thought) and the environment (Obs).\\nanswers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,\\n2022). However, this “chain-of-thought” reasoning is a static black box, in that the model uses\\nits own internal representations to generate thoughts and is not grounded in the external world,\\nwhich limits its ability to reason reactively or update its knowledge. This can lead to issues like fact\\nhallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,\\nrecent work has explored the use of pre-trained language models for planning and acting in interactive\\nenvironments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with\\na focus on predicting actions via language priors. These approaches usually convert multi-modal\\nobservations into text, use a language model to generate domain-speciﬁc actions or plans, and then\\nuse a controller to choose or execute them. However, they do not employ language models to reason\\nabstractly about high-level goals or maintain a working memory to support acting, barring Huang\\net al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the\\ncurrent state. Beyond such simple embodied tasks to interact with a few blocks, there have not been\\nstudies on how reasoning and acting can be combined in a synergistic manner for general task solving,\\nand if such a combination can bring systematic beneﬁts compared to reasoning or acting alone.\\nIn this work, we present ReAct, a general paradigm to combine reasoning and acting with language\\nmodels for solving diverse language reasoning and decision making tasks (Figure 1). ReAct\\nprompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an\\ninterleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and\\nadjust high-level plans for acting (reason to act), while also interact with the external environments\\n(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 2, 'page_label': '3'}, page_content='Published as a conference paper at ICLR 2023\\nWe conduct empirical evaluations ofReAct and state-of-the-art baselines on four diverse benchmarks:\\nquestion answering (HotPotQA, Yang et al., 2018), fact veriﬁcation (Fever, Thorne et al., 2018),\\ntext-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao\\net al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact\\nwith, ReAct outperforms vanilla action generation models while being competitive with chain-of-\\nthought reasoning (CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct\\nand CoT that allows for the use of both internal knowledge and externally obtained information\\nduring reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able\\nto outperform imitation or reinforcement learning methods trained with 103 ∼105 task instances,\\nwith an absolute improvement of 34% and 10% in success rates respectively. We also demonstrate\\nthe importance of sparse, versatile reasoning in decision making by showing consistent advantages\\nover controlled baselines with actions only. Besides general applicability and performance boost,\\nthe combination of reasoning and acting also contributes to model interpretability, trustworthiness,\\nand diagnosability across all domains, as humans can readily distinguish information from model’s\\ninternal knowledge versus external environments, as well as inspect reasoning traces to understand\\nthe decision basis of model actions.\\nTo summarize, our key contributions are the following: (1) we introduce ReAct, a novel prompt-\\nbased paradigm to synergize reasoning and acting in language models for general task solving; (2) we\\nperform extensive experiments across diverse benchmarks to showcase the advantage ofReAct in a\\nfew-shot learning setup over prior approaches that perform either reasoning or action generation in\\nisolation; (3) we present systematic ablations and analysis to understand the importance of acting in\\nreasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations ofReAct under the\\nprompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial ﬁnetuning\\nexperiments showing the potential of ReAct to improve with additional training data. Scaling up\\nReAct to train and operate on more tasks and combining it with complementary paradigms like\\nreinforcement learning could further unlock the potential of large language models.\\n2 REAC T: S YNERGIZING REASONING + AC TING\\nConsider a general setup of an agent interacting with an environment for task solving. At time\\nstep t, an agent receives an observation ot ∈O from the environment and takes an action at ∈A\\nfollowing some policy π(at|ct), where ct = (o1,a1,··· ,ot−1,at−1,ot) is the context to the agent.\\nLearning a policy is challenging when the mapping ct ↦→at is highly implicit and requires extensive\\ncomputation. For example, the agent shown in Figure 1(1c) is unable to generate the correct ﬁnal\\naction (Act 4) to ﬁnish the QA task as it requires complex reasoning over the trajectory context\\n(Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the\\ncontext that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.\\nThe idea of ReAct is simple: we augment the agent’s action space to ˆA= A∪L, where Lis the\\nspace of language. An action ˆat ∈L in the language space, which we will refer to as a thought or a\\nreasoning trace, does not affect the external environment, thus leading to no observation feedback.\\nInstead, a thought ˆat aims to compose useful information by reasoning over the current context ct,\\nand update the context ct+1 = (ct,ˆat) to support future reasoning or acting. As shown in Figure 1,\\nthere could be various types of useful thoughts, e.g. decomposing task goals and create action plans\\n(2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1),\\nextracting important parts from observations (1d, Thought2, 4), track progress and transit action plans\\n(2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on.\\nHowever, as the language space Lis unlimited, learning in this augmented action space is difﬁcult\\nand requires strong language priors. In this paper, we mainly focus on the setup where a frozen\\nlarge language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context\\nexamples to generate both domain-speciﬁc actions and free-form language thoughts for task solving\\n(Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and\\nenvironment observations to solve a task instance (see Appendix C). For the tasks where reasoning is\\nof primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the\\ntask-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision\\nmaking tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to\\n1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B.\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 3, 'page_label': '4'}, page_content='Published as a conference paper at ICLR 2023\\nappear sparsely in the most relevant positions of a trajectory, so we let the language model decide the\\nasynchronous occurrence of thoughts and actions for itself.\\nSince decision making and reasoning capabilities are integrated into a large language model, ReAct\\nenjoys several unique features: A) Intuitive and easy to design : Designing ReAct prompts is\\nstraightforward as human annotators just type down their thoughts in language on top of their actions\\ntaken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail\\nprompt design for each task in Sections 3 and 4. B) General and ﬂexible: Due to the ﬂexible thought\\nspace and thought-action occurrence format, ReAct works for diverse tasks with distinct action\\nspaces and reasoning needs, including but not limited to QA, fact veriﬁcation, text game, and web\\nnavigation. C) Performant and robust: ReAct shows strong generalization to new task instances\\nwhile learning solely from one to six in-context examples, consistently outperforming baselines with\\nonly reasoning or acting across different domains. We also show in Section 3 additional beneﬁts\\nwhen ﬁnetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.\\nD) Human aligned and controllable: ReAct promises an interpretable sequential decision making\\nand reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,\\nhumans can also control or correct the agent behavior on the go by thought editing, as shown in\\nFigure 5 in Section 4.\\n3 K NOWLEDGE -INTENSIVE REASONING TASKS\\nWe begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact\\nveriﬁcation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to\\nretrieve information to support reasoning, while also use reasoning to target what to retrieve next,\\ndemonstrating a synergy of reasoning and acting.\\n3.1 S ETUP\\nDomains We consider two datasets challenging knowledge retrieval and reasoning: (1) Hot-\\nPotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning\\nover two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact veriﬁcation\\nbenchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based\\non if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only\\nsetup for both tasks, where models only receive the question/claim as input without access to support\\nparagraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\\nan external environment to support reasoning.\\nAction Space We design a simple Wikipedia web API with three types of actions to support\\ninteractive information retrieval: (1) search[entity], which returns the ﬁrst 5 sentences from\\nthe corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the\\nWikipedia search engine, (2) lookup[string], which would return the next sentence in the page\\ncontaining string, simulating Ctrl+F functionality on the browser. (3) finish[answer], which\\nwould ﬁnish the current task with answer. We note that this action space mostly can only retrieve a\\nsmall part of a passage based on exact passage name, which is signiﬁcantly weaker than state-of-the-\\nart lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,\\nand force models to retrieve via explicit reasoning in language.\\n3.2 M ETHODS\\nReAct Prompting For HotpotQA and Fever, we randomly select 6 and 3 cases2 from the training\\nset and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts.\\nSimilar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense\\nthought), where free-form thoughts are used for various purposes. Speciﬁcally, we use a combination\\nof thoughts that decompose questions (“I need to search x, ﬁnd y, then ﬁnd z”), extract information\\nfrom Wikipedia observations (“x was started in 1844”, “The paragraph does not tell x”), perform\\ncommonsense (“x is not y, so z must instead be...”) or arithmetic reasoning (“1844 < 1989”), guide\\n2We ﬁnd more examples do not improve performance.\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 4, 'page_label': '5'}, page_content='Published as a conference paper at ICLR 2023\\nPrompt Methoda HotpotQA Fever\\n(EM) (Acc)\\nStandard 28.7 57.1\\nCoT(Wei et al., 2022) 29.4 56.3\\nCoT-SC(Wang et al., 2022a) 33.4 60.4\\nAct 25.7 58.9\\nReAct 27.4 60.9\\nCoT-SC→ReAct 34.2 64.6\\nReAct→CoT-SC 35.1 62.0\\nSupervised SoTAb 67.5 89.5\\nTable 1: PaLM-540B prompting results on\\nHotpotQA and Fever.\\naHotpotQA EM is 27.1, 28.9, 33.8 for Standard, CoT,\\nCoT-SC in Wang et al. (2022b).\\nb(Zhu et al., 2021; Lewis et al., 2020)\\n0 5 10 15 20\\n#CoT-SC trials\\n26\\n28\\n30\\n32\\n34HotpotQA EM\\n0 5 10 15 20\\n#CoT-SC trials\\n47.5\\n50.0\\n52.5\\n55.0\\n57.5\\n60.0\\n62.5\\n65.0Fever Acc\\nMethod\\nCoT-SC -> ReAct\\nReAct -> CoT-SC\\nCoT-SC\\nReAct\\nCoT\\nFigure 2: PaLM-540B prompting results with respect to\\nnumber of CoT-SC samples used.\\nsearch reformulation (“maybe I can search/look up x instead”), and synthesize the ﬁnal answer (“...so\\nthe answer is x”). See Appendix C for more details.\\nBaselines We systematically ablateReAct trajectories to build prompts for multiple baselines (with\\nformats as Figure 1(1a-1c)): (a) Standard prompting (Standard), which removes all thoughts,\\nactions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al.,\\n2022), which removes actions and observations and serve as a reasoning-only baseline. We also\\nbuild a self-consistency baseline (CoT-SC) (Wang et al., 2022a;b) by sampling 21 CoT trajectories\\nwith decoding temperature 0.7 during inference and adopting the majority answer, which is found to\\nconsistently boost performance over CoT. (c) Acting-only prompt (Act), which removes thoughts\\nin ReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the\\nInternet to answer questions, though it operates on a different task and action space, and uses imitation\\nand reinforcement learning instead of prompting.\\nCombining Internal and External Knowledge As will be detail in Section 3.3, we observe that\\nthe problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT\\nis more accurate in formulating reasoning structure but can easily suffer from hallucinated facts\\nor thoughts. We therefore propose to incorporate ReAct and CoT-SC, and let the model decide\\nwhen to switch to the other method based on the following heuristics: A) ReAct →CoT-SC: when\\nReAct fails to return an answer within given steps, back off to CoT-SC. We set 7 and 5 steps for\\nHotpotQA and FEVER respectively as we ﬁnd more steps will not improve ReAct performance3.\\nB) CoT-SC →ReAct: when the majority answer among nCoT-SC samples occurs less than n/2\\ntimes (i.e. internal knowledge might not support the task conﬁdently), back off to ReAct.\\nFinetuning Due to the challenge of manually annotating reasoning traces and actions at scale,\\nwe consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories\\nwith correct answers generated by ReAct (also for other baselines) to ﬁnetune smaller language\\nmodels (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on\\ninput questions/claims. More details are in Appendix B.1.\\n3.3 R ESULTS AND OBSERVATIONS\\nReAct outperforms Act consistently Table 1 shows HotpotQA and Fever results using PaLM-\\n540B as the base model with different prompting methods. We note that ReAct is better than Act\\non both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the\\nﬁnal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also conﬁrm the beneﬁt of reasoning\\ntraces for more informed acting.\\n3Of all trajectories with correct ﬁnal answers, those with 7 steps on HotpotQA and 5 steps on FEVER only\\ntake up 0.84% and 1.33% respectively.\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 5, 'page_label': '6'}, page_content='Published as a conference paper at ICLR 2023\\nType Deﬁnition ReAct CoT\\nSuccess True positive Correct reasoning trace and facts 94% 86%\\nFalse positive Hallucinated reasoning trace or facts 6% 14%\\nFailure\\nReasoning error Wrong reasoning trace (including failing to recover from repetitive steps) 47% 16%\\nSearch result error Search return empty or does not contain useful information 23% -\\nHallucination Hallucinated reasoning trace or facts 0% 56%\\nLabel ambiguity Right prediction but did not match the label precisely 29% 28%\\nTable 2: Types of success and failure modes of ReAct and CoT on HotpotQA, as well as their\\npercentages in randomly selected examples studied by human.\\nReAct vs. CoT On the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly\\nlags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only\\ndiffer by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge\\nis vital. To better understand the behavioral difference between ReAct and CoT on HotpotQA, we\\nrandomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct\\nand CoT respectively (thus 200 examples in total), and manually labeled their success and failure\\nmodes in Table 2. Some key observations are as follows:\\nA) Hallucination is a serious problem for CoT, resulting in much higher false positive rate than\\nReAct (14% vs. 6%) in success mode, and make up its major failure mode (56%). In contrast, the\\nproblem solving trajectory of ReActis more grounded, fact-driven, and trustworthy, thanks to the\\naccess of an external knowledge base.\\nB) While interleaving reasoning, action and observation steps improves ReAct’s grounded-\\nness and trustworthiness, such a structural constraint also reduces its ﬂexibility in formulating\\nreasoning steps, leading to more reasoning error rate than CoT. we note that there is one frequent\\nerror pattern speciﬁc to ReAct, in which the model repetitively generates the previous thoughts and\\nactions, and we categorize it as part of “reasoning error” as the model fails to reason about what the\\nproper next action to take and jump out of the loop4.\\nC) For ReAct, successfully retrieving informative knowledge via search is critical. Non-\\ninformative search, which counts for 23% of the error cases, derails the model reasoning and gives\\nit a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between\\nfactuality and ﬂexibility, which motivates our proposed strategies of combining two methods.\\nWe provide examples for each success and failure modes in Appendix E.1. We also ﬁnd some\\nHotpotQA questions may contain outdated answer labels, see Figure 4 for example.\\nReAct + CoT-SC perform best for prompting LLMs Also shown in Table 1, the best prompting\\nmethod on HotpotQA and Fever are ReAct →CoT-SC and CoT-SC →ReAct respectively.\\nFurthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC\\nsamples used. While two ReAct + CoT-SC methods are advantageous at one task each, they both\\nsigniﬁcantly and consistently outperform CoT-SC across different number of samples, reaching\\nCoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of\\nproperly combining model internal knowledge and external knowledge for reasoning tasks.\\nReAct performs best for ﬁne-tuning Figure 3 shows the scaling effect of prompting/ﬁnetuning\\nfour methods (Standard, CoT, Act, ReAct) on HotpotQA. With PaLM-8/62B, promptingReAct\\nperforms worst among four methods due to the difﬁculty to learn both reasoning and acting from\\nin-context examples. However, when ﬁnetuned with just 3,000 examples, ReAct becomes the best\\nmethod among the four, with PaLM-8B ﬁnetuned ReAct outperforming all PaLM-62B prompting\\nmethods, and PaLM-62B ﬁnetuned ReAct outperforming all 540B prompting methods. In contrast,\\nﬁnetuning Standard or CoT is signiﬁcantly worse than ﬁnetuning ReAct or Act for both PaLM-\\n8/62B, as the former essentially teaches models to memorize (potentially halluincated) knowledge\\nfacts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a\\nmore generalizable skill for knowledge reasoning. As all prompting methods are still signiﬁcantly\\nfar from domain-speciﬁc state-of-the-art approaches (Table 1), we believe ﬁnetuning with more\\nhuman-written data might be a better way to unleash the power of ReAct.\\n4We suspect that this could be due to the sub-optimal greedy decoding procedure, and future work using\\nbetter decoding (e.g. beam search) might help address this issue.\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 6, 'page_label': '7'}, page_content='Published as a conference paper at ICLR 2023\\n8b 62b 540b\\nsize\\n0\\n5\\n10\\n15\\n20\\n25\\n30HotpotQA EM\\nlearning = prompt\\n8b 62b 540b\\nsize\\nlearning = finetune\\nMethod\\nStandard\\nCoT\\nAct\\nReAct\\nFigure 3: Scaling results for prompting and ﬁnetuning on HotPotQA withReAct (ours) and baselines.\\n4 D ECISION MAKING TASKS\\nWe also test ReAct on two language-based interactive decision-making tasks, ALFWorld and\\nWebShop, both of which feature complex environments that require agents to act over long horizons\\nwith sparse rewards, warranting the need for reasoning to act and explore effectively.\\nALFWorld ALFWorld (Shridhar et al., 2020b) (Figure 1(2)) is a synthetic text-based game designed\\nto align with the embodied ALFRED benchmark (Shridhar et al., 2020a). It includes 6 types of\\ntasks in which an agent needs to achieve a high-level goal (e.g. examine paper under desklamp) by\\nnavigating and interacting with a simulated household via text actions (e.g. go to coffeetable 1, take\\npaper 2, use desklamp 1). A task instance can have more than 50 locations and take an expert policy\\nmore than 50 steps to solve, thus challenging an agent to plan and track subgoals, as well as explore\\nsystematically (e.g. check all desks one by one for desklamp). In particular, one challenge built into\\nALFWorld is the need to determine likely locations for common household items (e.g. desklamps will\\nlikely be on desks, shelfs, or dressers), making this environment a good ﬁt for LLMs to exploit their\\npretrained commonsense knowledge. To prompt ReAct, we randomly annotate three trajectories\\nfrom the training set for each task type, where each trajectory includes sparse thoughts that (1)\\ndecompose the goal, (2) track subgoal completion, (3) determine the next subgoal, and (4) reason via\\ncommonsense where to ﬁnd an object and what to do with it. We show prompts used for ALFWorld\\nin Appendix C.4. Following Shridhar et al. (2020b), we evaluate on 134 unseen evaluation games\\nin a task-speciﬁc setup. For robustness, we construct 6 prompts for each task type through each\\npermutation of 2 annotated trajectories from the 3 we annotate. Act prompts are constructed using\\nthe same trajectories, but without thoughts — since task instances are randomly chosen from the\\ntraining set, it favors neitherReAct nor Act and provides a fair and controlled comparison to test the\\nimportance of sparse thoughts. For baselines, we use BUTLER (Shridhar et al., 2020b), an imitation\\nlearning agent trained on 105 expert trajectories for each task type5.\\nWebShop Can ReAct also interact with noisy real-world language environments for practical\\napplications? We investigate WebShop (Yao et al., 2022), a recently proposed online shopping\\nwebsite environment with 1.18M real-world products and 12k human instructions. Unlike ALFWorld,\\nWebshop contains a high variety of structured and unstructured texts (e.g. product titles, descriptions,\\nand options crawled from Amazon), and requires an agent to purchase a product based on a user\\ninstruction (e.g. “I am looking for a nightstand with drawers. It should have a nickel ﬁnish, and\\npriced lower than $140”) through web interactions (e.g. search “nightstand drawers”, choose buttons\\nsuch as “color: modern-nickel-white” or “back to search”). This task is evaluated by average score\\n(percentage of desired attributes covered by the chosen product averaged across all episodes) and\\nsuccess rate (percentage of episodes where the chosen product satisﬁes all requirements) on 500 test\\ninstructions. We formulate Act prompts with actions to search, choose product, choose options,\\nand buy, with ReAct prompts additionally reasoning to determine what to explore, when to buy,\\nand what products options are relevant to the instruction. See Table 6 for an example prompt, and\\nTable 10 for model predictions in the Appendix. We compare to an imitation learning (IL) method\\n5Micheli & Fleuret (2021) ﬁnetuned a GPT-2 model on 3553 task instances and achieved a much improved\\nperformance than BUTLER, but it is trained on all task types, thus not included as a baseline.\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 7, 'page_label': '8'}, page_content='Published as a conference paper at ICLR 2023\\nMethod Pick Clean Heat Cool Look Pick 2 All\\nAct(best of 6) 88 42 74 67 72 41 45\\nReAct(avg) 65 39 83 76 55 24 57\\nReAct(best of 6) 92 58 96 86 78 41 71\\nReAct-IM(avg) 55 59 60 55 23 24 48\\nReAct-IM(best of 6) 62 68 87 57 39 33 53\\nBUTLERg (best of 8) 33 26 70 76 17 12 22\\nBUTLER(best of 8) 46 39 74 100 22 24 37\\nTable 3: AlfWorld task-speciﬁc success rates (%). BUTLER and\\nBUTLERg results are from Table 4 of Shridhar et al. (2020b). All\\nmethods use greedy decoding, except that BUTLER uses beam search.\\nMethod Score SR\\nAct 62.3 30.1\\nReAct 66.6 40.0\\nIL 59.9 29.1\\nIL+RL 62.4 28.7\\nHuman 82.1 59.6Expert\\nTable 4: Score and suc-\\ncess rate (SR) on Web-\\nshop. IL/IL+RL taken\\nfrom Yao et al. (2022).\\ntrained with 1,012 human annotated trajectories, and a imitation + reinforcement learning (IL + RL)\\nmethod additionally trained with 10,587 training instructions.\\nResults ReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On\\nALFWorld, the bestReAct trial achieves an average success rate of 71%, signiﬁcantly outperforming\\nthe best Act (45%) and BUTLER (37%) trials. In fact, even the worse ReAct trial (48%) beats\\nthe best trial of both methods. Moreover, the advantage of ReAct over Act is consistent across\\nsix controlled trials, with relative performance gain ranging from 33% to 90% and averaging 62%.\\nQualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals\\ninto smaller subgoals, or loses track of the current state of the environment. Example trajectories\\ncomparing ReAct and Act can be found in Appendix D.2.1 and Appendix D.2.2.\\nOn Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With\\nadditional sparse reasoning, ReAct achieves signiﬁcantly better performance, with an absolute 10%\\nimprovement over the previous best success rate. By checking examples, we ﬁnd that ReAct is more\\nlikely to identify instruction-relevant products and options by reasoning to bridge the gap between\\nnoisy observations and actions (e.g. “For ‘space-saving ottoman bench for living room’, the item\\nhas options ‘39x18x18inch’ and ‘blue’ and seems good to buy.”). However, existing methods are\\nstill far from the performance of expert humans (Table 4), who perform signiﬁcantly more product\\nexplorations and query re-formulations that are still challenging for prompting-based methods.\\nOn the value of internal reasoning vs. external feedback To our knowledge, ReAct is the ﬁrst\\ndemonstration of combined reasoning and action using an LLM applied to an interactive environment\\nwithin a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang\\net al. (2022b), in which actions from an embodied agent are motivated by an eponymous “inner\\nmonologue”. However, IM’s “inner monologue” is limited to observations of the environment\\nstate and what needs to be completed by the agent for the goal to be satisﬁed. In contrast, the\\nreasoning traces in ReAct for decision making is ﬂexible and sparse, allowing diverse reasoning\\ntypes (see Section 2) to be induced for different tasks.\\nTo demonstrate the differences between ReAct and IM, and to highlight the importance of internal\\nreasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought\\npattern composed of IM-like dense external feedback. As can be seen in Table 3,ReAct substantially\\noutperforms IM-style prompting ( ReAct-IM) (71 vs. 53 overall success rate), with consistent\\nadvantages on ﬁve out of six tasks. Qualitatively, we observed that ReAct-IM often made mistakes\\nin identifying when subgoals were ﬁnished, or what the next subgoal should be, due to a lack of high-\\nlevel goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where\\nan item would likely be within the ALFWorld environment, due to a lack of commonsense reasoning.\\nBoth shortcomings can be addressed in the ReAct paradigm. More details about ReAct-IM is in\\nAppendix B.2. An example prompt for ReAct-IM can be found in Appendix C.4, and an example\\ntrajectory in Appendix D.2.3.\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 8, 'page_label': '9'}, page_content='Published as a conference paper at ICLR 2023\\n5 R ELATED WORK\\nLanguage model for reasoning Perhaps the most well-known work of using LLMs for reasoning\\nis Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their\\nown “thinking procedure” for problem solving. Several follow-up works have since been performed,\\nincluding least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shot-\\nCoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently,\\n(Madaan & Yazdanbakhsh, 2022) systematically studied the formulation and structure of CoT, and\\nobserved that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other\\nwork has also been extended to more sophisticated reasoning architecture beyond simple prompting.\\nFor example Selection-Inference (Creswell et al., 2022) divides the reasoning process into two steps\\nof “selection” and “inference”. STaR (Zelikman et al., 2022) bootstraps the reasoning process by\\nﬁnetuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswell\\n& Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated\\nLM respectively. Similar approaches like Scratchpad (Nye et al., 2021), which ﬁnetunes a LM on\\nintermediate computation steps, also demonstrate improvement on multi-step computation problems.\\nIn contrast to these methods, ReAct performs more than just isolated, ﬁxed reasoning, and integrates\\nmodel actions and their corresponding observations into a coherent stream of inputs for the model to\\nreason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).\\nLanguage model for decision making The strong capability of LLMs has enabled them to perform\\ntasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a\\npolicy model for decision making, especially in interactive environments. WebGPT (Nakano et al.,\\n2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to\\ncomplicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct, WebGPT does not\\nexplicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for\\nreinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b)\\nand Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl\\net al., 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly\\nconsider the reasoning procedure either, and also relies on expensive datasets and human feedback\\ncollections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the\\ndecision making process only requires language description of the reasoning procedure.6\\nLLMS have also been increasingly employed in interactive and embodied environments for planning\\nand decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)\\nand Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision\\nmaking. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which\\nis then reranked by an affordance model grounded on the visual environments for ﬁnal prediction.\\nInner Monologue made further improvements by adding the eponymous “inner monologue\", which is\\nimplemented as injected feedback from the environment. To our knowledge, Inner Monologue is the\\nﬁrst work that demonstrates such a closed-loop system, which ReAct builds on. However, we argue\\nthat Inner Monologue does not truly comprise of inner thoughts — this is elaborated in Section 4. We\\nalso note that leveraging language as semantically-rich inputs in the process of interactive decision\\nmaking has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti\\net al., 2021; Huang et al., 2022a; Li et al., 2022). It is becoming more evident that with the help of\\nLLMs, language as a fundamental cognitive mechanism will play a critical role in interaction and\\ndecision making. What is more, progress in LLMs has also inspired the development of versatile and\\ngeneralist agents like Reed et al. (2022).\\n6 C ONCLUSION\\nWe have proposedReAct – a simple yet effective method for synergizing reasoning and acting in\\nlarge language models. Through a diverse set of experiments on multi-hop question-answering, fact\\nchecking, and interactive decision-making tasks, we show that ReAct leads to superior performance\\nwith interpretable decision traces. Despite the simplicity of our method, complex tasks with large\\naction spaces require more demonstrations to learn well, which unfortunately can easily go beyond\\nthe input length limit of in-context learning. We explore the ﬁne-tuning approach on HotpotQA\\n6Human feedback can also be incorporated in a complementary manner but we leave it for future work.\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 9, 'page_label': '10'}, page_content='Published as a conference paper at ICLR 2023\\nwith initial promising results, but learning from more high-quality human annotations will be the\\ndesiderata to further improve the performance. Scaling up ReAct with multi-task training and\\ncombining it with complementary paradigms like reinforcement learning could result in stronger\\nagents that further unlock the potential of LLMs for more applications.\\nACKNOWLEDGMENTS\\nWe thank the support and feedback of many people from Google Brain team and Princeton NLP\\nGroup. This work was supported in part by the National Science Foundation under Grant No.\\n2107048. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are\\nthose of the author(s) and do not necessarily reﬂect the views of the National Science Foundation.\\nREPRODUCIBILITY STATEMENT\\nOur main experiments are done on PaLM (Chowdhery et al., 2022), which is not an openly accessible\\nmodel yet. To increase reproducibility, we have included all used prompts in Appendix C, additional\\nexperiments using GPT-3 (Brown et al., 2020) in Appendix A.1, and associated GPT-3 ReAct\\nprompting code at https://anonymous.4open.science/r/ReAct-2268/.\\nETHICS STATEMENT\\nReAct prompts large language models to generate more human interpretable, diagnosable, and\\ncontrollable task-solving trajectories than previous methods. However, hooking up a large language\\nmodel with an action space to interact with external environments (e.g. the web, physical environ-\\nments) has potential dangers, e.g. looking up inappropriate or private information, or taking harmful\\nactions in an environment. Our experiments minimize such risks by limiting the interactions to\\nspeciﬁc websites (Wikipedia or WebShop) that are free of private information, without any dangerous\\nactions in the action space design (i.e. models cannot really buy products on WebShop the research\\nbenchmark, or edit Wikipedia). We believe researchers should be aware of such risks before designing\\nmore extensive experiments in the future.\\nREFERENCES\\nJosh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita\\nChhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim\\nHarley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathew-\\nson, Soˇna Mokrá, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne,\\nDuncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence,\\n2020. URL https://arxiv.org/abs/2012.05672.\\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine\\nHsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally\\nJesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,\\nSergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka\\nRao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander\\nToshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and\\nAndy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL\\nhttps://arxiv.org/abs/2204.01691.\\nBen Alderson-Day and Charles Fernyhough. Inner speech: development, cognitive functions,\\nphenomenology, and neurobiology. Psychological bulletin, 141(5):931, 2015.\\nAlan Baddeley. Working memory. Science, 255(5044):556–559, 1992.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 10, 'page_label': '11'}, page_content='Published as a conference paper at ICLR 2023\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\\nAntonia Creswell and Murray Shanahan. Faithful reasoning using large language models, 2022. URL\\nhttps://arxiv.org/abs/2208.14271.\\nAntonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large\\nlanguage models for interpretable logical reasoning, 2022. URL https://arxiv.org/abs/\\n2205.09712.\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, pp. 3558–3567, Florence, Italy, July 2019. Association for Com-\\nputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/\\nP19-1346.\\nCharles Fernyhough. Vygotsky, luria, and the social brain. Self and social regulation: Social\\ninteraction and the development of social understanding and executive functions, pp. 56–79, 2010.\\nAmelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Mari-\\nbeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham,\\nJonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth\\nDathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green,\\nSoˇna Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel,\\nWilliam Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and\\nGeoffrey Irving. Improving alignment of dialogue agents via targeted human judgements,\\n2022. URL https://storage.googleapis.com/deepmind-media/DeepMind.\\ncom/Authors-Notes/sparrow/sparrow-final.pdf.\\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. A simple\\nlanguage model for task-oriented dialogue. Advances in Neural Information Processing Systems,\\n33:20179–20191, 2020.\\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\\nplanners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207,\\n2022a.\\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\\nplanning with language models. arXiv preprint arXiv:2207.05608, 2022b.\\nSiddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. Lila: Language-informed\\nlatent actions. In CoRL, pp. 1379–1390, 2021. URL https://proceedings.mlr.press/\\nv164/karamcheti22a.html.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\\nlanguage models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.\\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-\\naugmented language models through few-shot prompting for open-domain question answering.\\narXiv preprint arXiv:2203.05115, 2022.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented genera-\\ntion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:\\n9459–9474, 2020.\\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An\\nHuang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba,\\nand Yuke Zhu. Pre-trained language models for interactive decision-making, 2022. URL https:\\n//arxiv.org/abs/2202.01771.\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 11, 'page_label': '12'}, page_content='Published as a conference paper at ICLR 2023\\nAleksandr Romanovich Luria. Ls vygotsky and the problem of localization of functions. Neuropsy-\\nchologia, 3(4):387–392, 1965.\\nAman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes\\ntwo to tango, 2022. URL https://arxiv.org/abs/2209.07686.\\nVincent Micheli and François Fleuret. Language models are few-shot butlers. arXiv preprint\\narXiv:2104.07972, 2021.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\\nGretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:\\nBrowser-assisted question-answering with human feedback, 2021. URL https://arxiv.\\norg/abs/2112.09332.\\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and\\nAugustus Odena. Show your work: Scratchpads for intermediate computation with language\\nmodels, 2021. URL https://arxiv.org/abs/2112.00114.\\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom\\nEccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,\\nOriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https:\\n//arxiv.org/abs/2205.06175.\\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,\\nLuke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions\\nfor everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, pp. 10740–10749, 2020a.\\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew\\nHausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv\\npreprint arXiv:2010.03768, 2020b.\\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston.\\nLanguage models that seek for knowledge: Modular search & generation for dialogue and prompt\\ncompletion. arXiv preprint arXiv:2203.13224, 2022a.\\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung,\\nMoya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman\\nGoyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. Blenderbot 3:\\na deployed conversational agent that continually learns to responsibly engage, 2022b. URL\\nhttps://arxiv.org/abs/2208.03188.\\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale\\ndataset for fact extraction and veriﬁcation. arXiv preprint arXiv:1803.05355, 2018.\\nLev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky, 1:39–285, 1987.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,\\n2022a. URL https://arxiv.org/abs/2203.11171.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented\\nensembles in language models. arXiv preprint arXiv:2207.00747, 2022b.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\\narXiv:2201.11903, 2022.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,\\nand Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\\nanswering. arXiv preprint arXiv:1809.09600, 2018.\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 12, 'page_label': '13'}, page_content='Published as a conference paper at ICLR 2023\\nShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore:\\nLanguage models for action generation in text-based games. InProceedings of the 2020 Conference\\non Empirical Methods in Natural Language Processing (EMNLP), pp. 8736–8754, Online, Novem-\\nber 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704.\\nURL https://aclanthology.org/2020.emnlp-main.704.\\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable\\nreal-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206,\\n2022.\\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with\\nreasoning, 2022. URL https://arxiv.org/abs/2203.14465.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\\nOlivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in\\nlarge language models, 2022. URL https://arxiv.org/abs/2205.10625.\\nYunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. Adaptive information\\nseeking for open-domain question answering. arXiv preprint arXiv:2109.06747, 2021.\\n13'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 13, 'page_label': '14'}, page_content='Published as a conference paper at ICLR 2023\\nA A DDITIONAL RESULTS\\nA.1 GPT-3 E XPERIMENTS\\nPaLM-540B GPT-3\\nHotpotQA (exact match) 29.4 30.8\\nALFWorld (success rate %) 70.9 78.4\\nTable 5: ReAct prompting results using PaLM-540B vs. GPT-3 (text-davinci-002, greedy decoding).\\nOn HotpotQA, we randomly sample a subset of 500 validation questions. On ALFWorld, we use all\\n134 unseen validation task instances, and use the best prompt set according to PaLM-540B.\\nWe run additional GPT-3 (Brown et al., 2020) experiments to conﬁrmReAct prompting performance\\nis general across different large language models. As shown in Table 5, GPT-3 (text-davinci-002,\\ngreedy decoding) consistently outperforms PaLM-540B on HotpotQA and ALFWorld, possibly\\nbecause it is ﬁnetuned with human instruction following. This indicates ReAct prompting is effective\\nacross different large language models on different tasks. The code for these experiments are at\\nhttps://react-lm.github.io/.\\nA.2 REAC TOBTAINS UP -TO-DATE KNOWLEDGE ON HOTPOT QA\\n\\x0b\\x14D\\x0c\\x036WDQGDUG\\n$QVZHU\\x1d\\x03\\x16\\x0f\\x13\\x13\\x13\\n\\x0b\\x14E\\x0c\\x03&R7\\x03\\x0b5HDVRQ\\x032QO\\\\\\x0c\\n7KRXJKW\\x1d\\x03/HW\\nV\\x03WKLQN\\x03VWHS\\x03\\nE\\\\\\x03VWHS\\x11\\x037KH\\x03KRWHO\\x03WKDW\\x03LV\\x03\\nKRPH\\x03WR\\x03WKH\\x03&LUTXH\\x03GX\\x03\\n6ROHLO\\x03VKRZ\\x030\\\\VWHUH\\x03LV\\x03\\n7UHDVXUH\\x03,VODQG\\x11\\x037UHDVXUH\\x03\\n,VODQG\\x03KDV\\x03\\x15\\x0f\\x1b\\x1b\\x18\\x03URRPV\\x0f\\x03VR\\x03\\nWKH\\x03DQVZHU\\x03LV\\x03\\x15\\x0f\\x1b\\x1b\\x18\\x11\\n$QVZHU\\x1d\\x03\\x15\\x0f\\x1b\\x1b\\x18\\n\\x0b\\x14F\\x0c\\x03$FW\\x102QO\\\\\\x03\\n$FW\\x03\\x14\\x1d\\x036HDUFK>&LUTXH\\x03GX\\x036ROHLO@\\x03\\n2EV\\x03\\x14\\x1d\\x03&LUTXH\\x03GX\\x036ROHLO\\x03LV\\x03D\\x03\\n&DQDGLDQ\\x03HQWHUWDLQPHQW\\x03FRPSDQ\\\\ª\\n$FW\\x03\\x15\\x1d\\x03/RRNXS>0\\\\VWHUH@\\x03\\n2EV\\x03\\x15\\x1d\\x031R\\x03PRUH\\x03UHVXOWV\\n$FW\\x03\\x16\\x1d\\x036HDUFK>0\\\\VWHUH\\x03\\x0b&LUTXH\\x03GX\\x03\\n6ROHLO\\x03VKRZ\\x0c@\\x03\\n2EV\\x03\\x16\\x1d\\x03&RXOG\\x03QRW\\x03ILQGª\\n$FW\\x03\\x17\\x1d\\x036HDUFK>0\\\\VWHUH\\x03\\x0b&LUTXH\\x03GX\\x03\\n6ROHLO\\x0c@\\x03\\n2EV\\x03\\x17\\x1d\\x030\\\\VWqUH\\x03\\x0bP\\\\\\x10VWHHU\\x0c\\x03\\x03LV\\x03\\nRQH\\x03RI\\x03VL[ª\\n$FW\\x03\\x18\\x1d\\x03/RRNXS>7UHDVXUH\\x03,VODQG\\x03\\n+RWHO\\x03DQG\\x03&DVLQR@\\x03\\nªª\\x0bHQG\\x03ZLWKRXW\\x03DQVZHU\\x0c\\n\\x0b\\x14G\\x0c\\x035H$FW\\x03\\x0b5HDVRQ\\x03\\x0e\\x03$FW\\x0c\\n7KRXJKW\\x03\\x14\\x1d\\x03,\\x03QHHG\\x03WR\\x03VHDUFK\\x03&LUTXH\\x03GX\\x036ROHLO\\x03VKRZ\\x030\\\\VWHUH\\x0f\\x03\\nILQG\\x03WKH\\x03KRWHO\\x03LW\\x03LV\\x03LQ\\x0f\\x03WKHQ\\x03ILQG\\x03WKH\\x03QXPEHU\\x03RI\\x03URRPV\\x03LQ\\x03\\nWKH\\x03KRWHO\\x11\\n$FW\\x03\\x14\\x1d\\x036HDUFK>&LUTXH\\x03GX\\x036ROHLO\\x03VKRZ\\x030\\\\VWHUH@\\n2EV\\x03\\x14\\x1d\\x03&RXOG\\x03QRW\\x03ILQG\\x03&LUTXH\\x03GX\\x036ROHLO\\x03VKRZ\\x030\\\\VWHUH\\x11\\x03\\n6LPLODU\\x1d\\x03>\\n&LUTXH\\x03GX\\x036ROHLO\\n\\x03ª\\n7KRXJKW\\x03\\x15\\x1d\\x037R\\x03ILQG\\x03WKH\\x03KRWHO\\x0f\\x03,\\x03FDQ\\x03VHDUFK\\x030\\\\VWHUH\\x03\\x0b&LUTXH\\x03\\nGX\\x036ROHLO\\x0c\\x11\\n$FW\\x03\\x15\\x1d\\x036HDUFK>0\\\\VWHUH\\x03\\x0b&LUTXH\\x03GX\\x036ROHLO\\x0c@\\n2EV\\x03\\x15\\x1d\\x030\\\\VWqUH\\x03\\x0bP\\\\\\x10VWHHU\\x0c\\x03\\x03LV\\x03ª\\x03+HOG\\x03LQ\\x03D\\x03FXVWRP\\x03WKHDWUH\\x03DW\\x03\\nWKH\\x037UHDVXUH\\x03,VODQG\\x03+RWHO\\x03DQG\\x03&DVLQR\\x03ª\\n7KRXJKW\\x03\\x16\\x1d\\x030\\\\VWHUH\\x03LV\\x03KHOG\\x03LQ\\x03D\\x03FXVWRP\\x03WKHDWUH\\x03DW\\x03WKH\\x03\\n7UHDVXUH\\x03,VODQG\\x03+RWHO\\x03DQG\\x03&DVLQR\\x11\\x036R\\x03,\\x03QHHG\\x03WR\\x03VHDUFK\\x03\\n7UHDVXUH\\x03,VODQG\\x03+RWHO\\x03DQG\\x03&DVLQR\\x03QH[W\\x03DQG\\x03ILQG\\x03WKH\\x03QXPEHU\\x03\\nRI\\x03URRPV\\x03LQ\\x03WKH\\x03KRWHO\\x11\\n$FW\\x03\\x16\\x1d\\x036HDUFK>7UHDVXUH\\x03,VODQG\\x03+RWHO\\x03DQG\\x03&DVLQR@\\n2EV\\x03\\x16\\x1d\\x037UHDVXUH\\x03,VODQG\\x03+RWHO\\x03DQG\\x03&DVLQR\\x03ª\\x03ZLWK\\x03\\x15\\x0f\\x1b\\x1b\\x17\\x03URRPV\\x03\\nDQG\\x03\\x15\\x15\\x13\\x03VXLWHV\\x03ª\\n7KRXJKW\\x03\\x17\\x1d\\x037UHDVXUH\\x03,VODQG\\x03+RWHO\\x03DQG\\x03&DVLQR\\x03KDV\\x03\\x15\\x0f\\x1b\\x1b\\x17\\x03URRPV\\x03\\nDQG\\x03\\x15\\x15\\x13\\x03VXLWHV\\x11\\x036R\\x03WKH\\x03DQVZHU\\x03LV\\x03\\x16\\x0f\\x14\\x13\\x17\\x11\\n$FW\\x03\\x17\\x1d\\x03)LQLVK>\\x16\\x0f\\x14\\x13\\x17@ Ҽ\\n4XHVWLRQ\\x1d\\x03+RZ\\x03PDQ\\\\\\x03URRPV\\x03DUH\\x03LQ\\x03WKH\\x03KRWHO\\x03WKDW\\x03LV\\x03KRPH\\x03WR\\x03\\nWKH\\x03&LUTXH\\x03GX\\x036ROHLO\\x03VKRZ\\x030\\\\VWHUH\"\\n+RWSRW4$\\x03ODEHO\\x1d\\x03\\x15\\x0f\\x19\\x19\\x17\\n\\x0b\\x14\\x0c\\x03+RWVSRW\\x034$\\n2XWGDWHG\\x03\\n8S\\x10WR\\x10GDWH\\x03\\nFigure 4: Another example HotpotQA question, where the original label is outdated. Only ReAct is\\nable to obtain the up-to-date answer thanks to real-world web interaction plus reasoning.\\nDuring trajectory inspection, we also ﬁnd that sometimesReAct does not agree with dataset labels as\\nthe labels themselves could be outdated. For example, as shown in Figure 4, the question asks about\\nthe size of a hotel, which increased from the HotpotQA construction time. WhileStandard and CoT\\ngive wrong answers due to hallucination, Act fails despite the access of real-world web interaction,\\ndue to a lack of reasoning to guide how to interact with the Internet for QA. Only ReAct is able to\\nretrieve up-to-date information from the Internet and provide a reasonable answer. Therefore, better\\nincorporation of reasoning abilities might beneﬁt recent Internet-augmented language models (Nakano\\net al., 2021; Lazaridou et al., 2022; Shuster et al., 2022a) for up-to-date task solving.\\nA.3 H UMAN -IN-THE -LOOP BEHAVIOR CORRECTION ON ALFWORLD\\nWe also explore human-in-the-loop interaction with ReAct, to allow a human to inspect and edit\\nReAct’s reasoning traces. Figure 5 shows that by simply removing a hallucinating sentence in Act\\n17 and adding some hints in Act 23, ReAct can be made to change its behavior drastically to align\\nwith these human thought edits and succeed in the task. From a human perspective, solving such a\\ntask becomes signiﬁcantly easier, from typing tens of actions to only editing a couple of thoughts,\\nwhich enables new forms of human-machine collaboration. We note that such a policy edit on-the-go\\n14'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 14, 'page_label': '15'}, page_content='Published as a conference paper at ICLR 2023\\n\\x0bD\\x0c\\x035H$FW\\x03\\n\\x11\\x11\\x11\\x11\\x11\\x11\\n2EV\\x03\\x14\\x19\\x1d\\x03<RX\\x03RSHQ\\x03WKH\\x03GUDZHU\\x03\\x17\\x11\\x037KH\\x03GUDZHU\\x03\\x17\\x03LV\\x03RSHQ\\x11\\x03,Q\\x03LW\\x0f\\x03\\\\RX\\x03\\nVHH\\x03D\\x03NH\\\\FKDLQ\\x03\\x16\\x0f\\x03DQG\\x03D\\x03ZDWFK\\x03\\x15\\x11\\x03\\n$FW\\x03\\x14\\x1a\\x1d\\x037KLQN\\x03>1RZ\\x03,\\x03ILQG\\x03WKH\\x03ILUVW\\x03NH\\\\FKDLQ\\x03\\x0b\\x16\\x0c\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03WR\\x03\\nWDNH\\x03LW\\x11\\x03,\\x03FDQ\\x03ILQG\\x03WKH\\x03VHFRQG\\x03NH\\\\FKDLQ\\x03\\x0b\\x15\\x0c\\x03ODWHU\\x03LQ\\x03GUDZHU\\x03\\x17\\x11\\x03@\\n$FW\\x03\\x14\\x1b\\x1d\\x03WDNH\\x03NH\\\\FKDLQ\\x03\\x16\\x03IURP\\x03GUDZHU\\x03\\x17\\x03\\n2EV\\x03\\x14\\x1b\\x1d\\x03<RX\\x03SLFN\\x03XS\\x03WKH\\x03NH\\\\FKDLQ\\x03\\x16\\x03IURP\\x03WKH\\x03GUDZHU\\x03\\x17\\x11\\x03\\n\\x11\\x11\\x11\\x11\\x11\\x11\\n$FW\\x03\\x15\\x15\\x1d\\x03SXW\\x03NH\\\\FKDLQ\\x03\\x16\\x03LQ\\x12RQ\\x03VDIH\\x03\\x14\\x03\\n2EV\\x03\\x15\\x15\\x1d\\x03<RX\\x03SXW\\x03WKH\\x03NH\\\\FKDLQ\\x03\\x16\\x03LQ\\x12RQ\\x03WKH\\x03VDIH\\x03\\x14\\x11\\x03\\n$FW\\x03\\x15\\x16\\x1d\\x037KLQN\\x03>1RZ\\x03,\\x03SXW\\x03WKH\\x03ILUVW\\x03NH\\\\FKDLQ\\x03LQ\\x03VDIH\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03\\nWR\\x03ILQG\\x03WKH\\x03VHFRQG\\x03NH\\\\FKDLQ\\x11\\x03,\\x03FDQ\\x03GLUHFWO\\\\\\x03JR\\x03WR\\x03GUDZHU\\x03\\x17\\x11\\x03@\\n$FW\\x03\\x15\\x17\\x1d\\x03JR\\x03WR\\x03GUDZHU\\x03\\x17\\x03\\n2EV\\x03\\x15\\x17\\x1d\\x037KH\\x03GUDZHU\\x03\\x17\\x03LV\\x03RSHQ\\x11\\x03,Q\\x03LW\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03ZDWFK\\x03\\x15\\x11\\x03\\n$FW\\x03\\x15\\x18\\x1d\\x037KLQN\\x03>1RZ\\x03,\\x03ILQG\\x03WKH\\x03VHFRQG\\x03NH\\\\FKDLQ\\x03\\x0b\\x15\\x0c\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03\\nWR\\x03WDNH\\x03LW\\x11@\\n$FW\\x03\\x15\\x19\\x1d\\x03WDNH\\x03NH\\\\FKDLQ\\x03\\x15\\x03IURP\\x03GUDZHU\\x03\\x17\\x03\\n2EV\\x03\\x15\\x19\\x1d\\x031RWKLQJ\\x03KDSSHQV\\x11\\n\\x0bE\\x0c\\x035H$FW\\x03\\x0b+XPDQ\\x037KRXJKW\\x03HGLW\\x0c\\n\\x11\\x11\\x11\\x11\\x11\\x11\\n2EV\\x03\\x14\\x19\\x1d\\x03<RX\\x03RSHQ\\x03WKH\\x03GUDZHU\\x03\\x17\\x11\\x037KH\\x03GUDZHU\\x03\\x17\\x03LV\\x03RSHQ\\x11\\x03,Q\\x03LW\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03\\nNH\\\\FKDLQ\\x03\\x16\\x0f\\x03DQG\\x03D\\x03ZDWFK\\x03\\x15\\x11\\x03\\n$FW\\x03\\x14\\x1a\\x1dѕ7KLQNѕ>1RZ\\x03,\\x03ILQG\\x03WKH\\x03ILUVW\\x03NH\\\\FKDLQ\\x03\\x0b\\x16\\x0c\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03WR\\x03WDNH\\x03LW\\x11\\x03,\\x03\\nFDQ\\x03ILQG\\x03WKH\\x03VHFRQG\\x03NH\\\\FKDLQ\\x03\\x0b\\x15\\x0c\\x03ODWHU\\x03LQ\\x03GUDZHU\\x03\\x17\\x11@\\n$FW\\x03\\x14\\x1b\\x1d\\x03WDNH\\x03NH\\\\FKDLQ\\x03\\x16\\x03IURP\\x03GUDZHU\\x03\\x17\\x03\\n2EV\\x03\\x14\\x1b\\x1d\\x03<RX\\x03SLFN\\x03XS\\x03WKH\\x03NH\\\\FKDLQ\\x03\\x16\\x03IURP\\x03WKH\\x03GUDZHU\\x03\\x17\\x11\\x03\\n\\x11\\x11\\x11\\x11\\x11\\x11\\n$FW\\x03\\x15\\x15\\x1d\\x03SXW\\x03NH\\\\FKDLQ\\x03\\x16\\x03LQ\\x12RQ\\x03VDIH\\x03\\x14\\x03\\n2EV\\x03\\x15\\x15\\x1d\\x03<RX\\x03SXW\\x03WKH\\x03NH\\\\FKDLQ\\x03\\x16\\x03LQ\\x12RQ\\x03WKH\\x03VDIH\\x03\\x14\\x11\\x03\\n$FW\\x03\\x15\\x16\\x1d\\x037KLQNѕ>1RZ\\x03,\\x03SXW\\x03WKH\\x03ILUVW\\x03NH\\\\FKDLQ\\x03LQ\\x03VDIH\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03WR\\x03ILQG\\x03\\nWKH\\x03VHFRQG\\x03NH\\\\FKDLQ\\x0f\\x03ZKLFK\\x03LV\\x03PRUH\\x03OLNHO\\\\\\x03WR\\x03DSSHDU\\x03LQ\\x03GUHVVHU\\x03\\x0b\\x14\\x0c\\x0f\\x03\\nJDUEDJHFDQ\\x03\\x0b\\x14\\x0c\\x0f\\x03VDIH\\x03\\x0b\\x14\\x0c\\x0f\\x03VLGHWDEOH\\x03\\x0b\\x14\\x0c\\x0f\\x03VRID\\x03\\x0b\\x14\\x0c\\x0f\\x03VKHOI\\x03\\x0b\\x14\\x10\\x14\\x15\\x0c\\x11@\\n$FW\\x03\\x15\\x17\\x1d\\x03JR\\x03WR\\x03GUHVVHU\\x03\\x14\\x03\\n2EV\\x03\\x15\\x17\\x1d\\x032Q\\x03WKH\\x03GUHVVHU\\x03\\x14\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03ER[\\x03\\x14\\x0f\\x03D\\x03FUHGLWFDUG\\x03\\x14\\x0f\\x03D\\x03NH\\\\FKDLQ\\x03\\x15\\x0f\\x03D\\x03\\nNH\\\\FKDLQ\\x03\\x14\\x0f\\x03D\\x03ODSWRS\\x03\\x14\\x11\\x11\\x11\\n$FW\\x03\\x15\\x18\\x1d\\x037KLQNѕ>1RZ\\x03,\\x03ILQG\\x03WKH\\x03VHFRQG\\x03NH\\\\FKDLQ\\x03\\x0b\\x15\\x0c\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03WR\\x03WDNH\\x03\\nLW\\x11@\\n$FW\\x03\\x15\\x19\\x1d\\x03WDNH\\x03NH\\\\FKDLQ\\x03\\x15\\x03IURP\\x03GUHVVHU\\x03\\x14\\x03\\n2EV\\x03\\x15\\x19\\x1d\\x03<RX\\x03SLFN\\x03XS\\x03WKH\\x03NH\\\\FKDLQ\\x03\\x15\\x03IURP\\x03WKH\\x03GUHVVHU\\x03\\x14\\x11\\x03\\n$FW\\x03\\x15\\x1a\\x1d\\x037KLQNѕ>1RZ\\x03,\\x03WDNH\\x03WKH\\x03VHFRQG\\x03NH\\\\FKDLQ\\x03\\x0b\\x15\\x0c\\x11\\x031H[W\\x0f\\x03,\\x03QHHG\\x03WR\\x03SXW\\x03LW\\x03\\nLQ\\x12RQ\\x03VDIH\\x03\\x14\\x11@\\n$FW\\x03\\x15\\x1b\\x1d\\x03JR\\x03WR\\x03VDIH\\x03\\x14\\n2EV\\x03\\x15\\x1b\\x1d\\x037KH\\x03VDIH\\x03\\x14\\x03LV\\x03RSHQ\\x11\\x03,Q\\x03LW\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03NH\\\\FKDLQ\\x03\\x16\\x0f\\x03D\\x03VWDWXH\\x03\\x15\\x0f\\x03DQG\\x03D\\x03\\nZDWFK\\x03\\x16\\x11\\n$FW\\x03\\x15\\x1c\\x1d\\x03SXW\\x03NH\\\\FKDLQ\\x03\\x15\\x03LQ\\x12RQ\\x03VDIH\\x03\\x14\\n2EV\\x03\\x15\\x1c\\x1d\\x03<RX\\x03SXW\\x03WKH\\x03NH\\\\FKDLQ\\x03\\x15\\x03LQ\\x12RQ\\x03WKH\\x03VDIH\\x03\\x14\\x11 Ҽ\\n<RX\\x03DUH\\x03LQ\\x03WKH\\x03PLGGOH\\x03RI\\x03D\\x03URRP\\x11\\x03/RRNLQJ\\x03TXLFNO\\\\\\x03DURXQG\\x03\\\\RX\\x0f\\x03\\n\\\\RX\\x03VHH\\x03D\\x03DUPFKDLU\\x03\\x14\\x0f\\x03D\\x03FDELQHW\\x03\\x17\\x0f\\x03ª\\x03D\\x03FDELQHW\\x03\\x14\\x0f\\x03D\\x03GUDZHU\\x03\\x18\\x0f\\x03\\nª\\x0f\\x03D\\x03GUDZHU\\x03\\x14\\x0f\\x03D\\x03GUHVVHU\\x03\\x14\\x0f\\x03D\\x03JDUEDJHFDQ\\x03\\x14\\x0f\\x03D\\x03VDIH\\x03\\x14\\x0f\\x03D\\x03VKHOI\\x03\\n\\x14\\x15\\x0f\\x03ª\\x0f\\x03D\\x03VKHOI\\x03\\x14\\x0f\\x03D\\x03VLGHWDEOH\\x03\\x14\\x0f\\x03DQG\\x03D\\x03VRID\\x03\\x14\\x11\\x03\\n<RXU\\x03WDVN\\x03LV\\x03WR\\x1d\\x03SXW\\x03WZR\\x03NH\\\\FKDLQ\\x03LQ\\x03VDIH\\x11\\n$OI:RUOG\\nFigure 5: A human-in-the-loop behavior correction example with ReAct in AlfWorld. (a) ReAct\\ntrajectory fails due to a hallucinating thought (Act 17). (b) By a human simply editing two thoughts\\n(Act 17, 23), the ReAct trajectory produces desirable reasoning traces and actions and succeeds.\\nis difﬁcult for Act and previous RL methods, as a human cannot change the model parameters, and\\nchanging a few actions might not edit the rest of the model behavior. This paradigm is also more than\\nhuman dialogue to update the goal or subgoal as in Huang et al. (2022b) — while editing ReAct\\nthoughts can do these, it can also modify the model’s internal belief, reasoning styles, or anything the\\nﬂexible thought space supports, for better task solving. We believe this is an exciting direction for\\nhuman alignment and leave more systematic study as future work.\\nB E XPERIMENT DETAILS\\nB.1 H OTPOT QA F INETUNING DETAILS\\nFor all ﬁnetuning we use a batch size of 64. On PaLM-8B, we ﬁnetune ReAct and Act methods\\nfor 4,000 steps and Standard and CoT methods for 2,000 steps. On PaLM-62B, we ﬁnetune\\nReAct and Act methods for 4,000 steps and Standard and CoT methods for 1,000 steps. We\\nﬁnd ReAct and Act methods generally beneﬁt from more training steps (and more training data),\\nwhile Standard and CoT methods degrade soon after ﬁnetuning.\\nB.2 A LFWORLD IM-S TYLE DETAILS\\nFor the IM-style ablation, the same expert trajectories used in ReAct are reannotated with dense\\nexternal feedback thoughts within these trajectories, that limit ReAct-IM to only think about (1)\\ndecomposing the current goal and (2) the current subgoal that needs to be completed. In particular,\\nReAct-IM lacks thoughts that (1) determine when a subgoal is completed (2) determine what the\\nnext subgoal should be (3) inducing the LLM to refer to its internal pretraining knowledge to identify\\nwhere items can be within the environment.\\n15'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 15, 'page_label': '16'}, page_content='Published as a conference paper at ICLR 2023\\nC P ROMPTS\\nC.1 H OTPOT QA\\nHotpot QA Prompts\\nOriginal\\nQuestion What is the elevation range for the area that the eastern sector of the\\nColorado orogeny extends into?\\nAnswer 1,800 to 7,000 ft\\nQuestion Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\\ncharacter Milhouse, who Matt Groening named after who?\\nAnswer Richard Nixon\\nQuestion Which documentary is about Finnish rock groups, Adam Clayton Powell or The\\nSaimaa Gesture?\\nAnswer The Saimaa Gesture\\nQuestion What profession does Nicholas Ray and Elia Kazan have in common?\\nAnswer director, screenwriter, actor\\nQuestion Which magazine was started first Arthur’s Magazine or First for Women?\\nAnswer Arthur’s Magazine\\nQuestion Were Pavel Urysohn and Leonid Levin known for the same type of work?\\nAnswer Yes\\nAct\\nQuestion What is the elevation range for the area that the eastern sector of the\\nColorado orogeny extends into?\\nAction 1 Search[Colorado orogeny]\\nObservation 1 The Colorado orogeny was an episode of mountain building (an orogeny) in\\nColorado and surrounding areas.\\nAction 2 Lookup[eastern sector]\\nObservation 2 (Result 1 / 1) The eastern sector extends into the High Plains and is called\\nthe Central Plains orogeny.\\nAction 3 Search[High Plains]\\nObservation 3 High Plains refers to one of two distinct land regions:\\nAction 4 Search[High Plains (United States)]\\nObservation 4 The High Plains are a subregion of the Great Plains. From east to west, the\\nHigh Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130\\nm).[3]\\nAction 5 Finish[1,800 to 7,000 ft]\\nQuestion Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\\ncharacter Milhouse, who Matt Groening named after who?\\nAction 1 Search[Milhouse]\\nObservation 1 Milhouse Mussolini Van Houten is a recurring character in the Fox animated\\ntelevision series The Simpsons voiced by Pamela Hayden and created by Matt\\nGroening.\\nAction 2 Lookup[named after]\\nObservation 2 (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose\\nmiddle name was Milhous.\\nAction 3 Finish[Richard Nixon]\\nQuestion Which documentary is about Finnish rock groups, Adam Clayton Powell or The\\nSaimaa Gesture?\\nAction 1 Search[Adam Clayton Powell]\\nObservation 1 Could not find [Adam Clayton Powell]. Similar: [’Adam Clayton Powell\\nIII’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr. State Office\\nBuilding’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell\\n(film)’, ’Giancarlo Esposito’].\\nAction 2 Search[Adam Clayton Powell (film)]\\nObservation 2 Adam Clayton Powell is a 1989 American documentary film directed by\\nRichard Kilberg. The film is about the rise and fall of influential\\nAfrican-American politician Adam Clayton Powell Jr.[3][4] It was later aired\\nas part of the PBS series The American Experience.\\nAction 3 Finish[The Saimaa Gesture]\\nQuestion What profession does Nicholas Ray and Elia Kazan have in common?\\nAction 1 Search[Nicholas Ray]\\nObservation 1 Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,\\n1979) was an American film director, screenwriter, and actor best known for\\nthe 1955 film Rebel Without a Cause.\\nAction 2 Search[Elia Kazan]\\nObservation 2 Elia Kazan was an American film and theatre director, producer, screenwriter\\nand actor.\\nContinued on next page\\n16'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 16, 'page_label': '17'}, page_content='Published as a conference paper at ICLR 2023\\nHotpot QA Prompts – Continued from previous page\\nAction 3 Finish[director, screenwriter, actor]\\nQuestion Which magazine was started first Arthur’s Magazine or First for Women?\\nAction 1 Search[Arthur’s Magazine]\\nObservation 1 Arthur’s Magazine (1844-1846) was an American literary periodical published\\nin Philadelphia in the 19th century.\\nAction 2 Search[First for Women]\\nObservation 2 First for Women is a woman’s magazine published by Bauer Media Group in the\\nUSA.[1] The magazine was started in 1989.\\nAction 3 Finish[Arthur’s Magazine]\\nQuestion Were Pavel Urysohn and Leonid Levin known for the same type of work?\\nAction 1 Search[Pavel Urysohn]\\nObservation 1 Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet\\nmathematician who is best known for his contributions in dimension theory.\\nAction 2 Search[Leonid Levin]\\nObservation 2 Leonid Anatolievich Levin is a Soviet-American mathematician and computer\\nscientist.\\nAction 3 Finish[yes]\\nCoT\\nQuestion What is the elevation range for the area that the eastern sector of the\\nColorado orogeny extends into?\\nThought Let’s think step by step. The eastern sector of Colorado orogeny extends\\ninto the High Plains. High Plains rise in elevation from around 1,800 to\\n7,000 ft, so the answer is 1,800 to 7,000 ft.\\nAnswer 1,800 to 7,000 ft\\nQuestion Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\\ncharacter Milhouse, who Matt Groening named after who?\\nThought Let’s think step by step. Milhouse was named after U.S. president Richard\\nNixon, so the answer is Richard Nixon.\\nAnswer Richard Nixon\\nQuestion Which documentary is about Finnish rock groups, Adam Clayton Powell or The\\nSaimaa Gesture?\\nThought Let’s think step by step. Adam Clayton Powell (film) is a documentary about\\nan African-American politician, not Finnish rock groups. So the documentary\\nabout Finnish rock groups must instead be The Saimaa Gesture.\\nAnswer The Saimaa Gesture\\nQuestion What profession does Nicholas Ray and Elia Kazan have in common?\\nThought Let’s think step by step. Professions of Nicholas Ray are director,\\nscreenwriter, and actor. Professions of Elia Kazan are director, producer,\\nscreenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in\\ncommon is director, screenwriter, and actor.\\nAnswer director, screenwriter, actor\\nQuestion Which magazine was started first Arthur’s Magazine or First for Women?\\nThought Let’s think step by step. Arthur’s Magazine was started in 1844. First\\nfor Women was started in 1989. 1844 (Arthur’s Magazine) < 1989 (First for\\nWomen), so Arthur’s Magazine was started first.\\nAnswer Arthur’s Magazine\\nQuestion Were Pavel Urysohn and Leonid Levin known for the same type of work?\\nThought Let’s think step by step. Pavel Urysohn is a mathematician. Leonid Levin\\nis a mathematician and computer scientist. So Pavel Urysohn and Leonid\\nLevin have the same type of work.\\nAnswer Yes\\nReAct\\nQuestion What is the elevation range for the area that the eastern sector of the\\nColorado orogeny extends into?\\nThought 1 I need to search Colorado orogeny, find the area that the eastern sector\\nof the Colorado orogeny extends into, then find the elevation range of the\\narea.\\nAction 1 Search[Colorado orogeny]\\nObservation 1 The Colorado orogeny was an episode of mountain building (an orogeny) in\\nColorado and surrounding areas.\\nThought 2 It does not mention the eastern sector. So I need to look up eastern\\nsector.\\nAction 2 Lookup[eastern sector]\\nObservation 2 (Result 1 / 1) The eastern sector extends into the High Plains and is called\\nthe Central Plains orogeny.\\nThought 3 The eastern sector of Colorado orogeny extends into the High Plains. So I\\nneed to search High Plains and find its elevation range.\\nAction 3 Search[High Plains]\\nObservation 3 High Plains refers to one of two distinct land regions\\nThought 4 I need to instead search High Plains (United States).\\nAction 4 Search[High Plains (United States)]\\nContinued on next page\\n17'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 17, 'page_label': '18'}, page_content='Published as a conference paper at ICLR 2023\\nHotpot QA Prompts – Continued from previous page\\nObservation 4 The High Plains are a subregion of the Great Plains. From east to west, the\\nHigh Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130\\nm).[3]\\nThought 5 High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer\\nis 1,800 to 7,000 ft.\\nAction 5 Finish[1,800 to 7,000 ft]\\nQuestion Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\\ncharacter Milhouse, who Matt Groening named after who?\\nThought 1 The question simplifies to \"The Simpsons\" character Milhouse is named after\\nwho. I only need to search Milhouse and find who it is named after.\\nAction 1 Search[Milhouse]\\nObservation 1 Milhouse Mussolini Van Houten is a recurring character in the Fox animated\\ntelevision series The Simpsons voiced by Pamela Hayden and created by Matt\\nGroening.\\nThought 2 The paragraph does not tell who Milhouse is named after, maybe I can look up\\n\"named after\".\\nAction 2 Lookup[named after]\\nObservation 2 (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose\\nmiddle name was Milhous.\\nThought 3 Milhouse was named after U.S. president Richard Nixon, so the answer is\\nRichard Nixon.\\nAction 3 Finish[Richard Nixon]\\nQuestion Which documentary is about Finnish rock groups, Adam Clayton Powell or The\\nSaimaa Gesture?\\nThought 1 I need to search Adam Clayton Powell and The Saimaa Gesture, and find which\\ndocumentary is about Finnish rock groups.\\nAction 1 Search[Adam Clayton Powell]\\nObservation 1 Could not find [Adam Clayton Powell]. Similar: [’Adam Clayton Powell\\nIII’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr. State Office\\nBuilding’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell\\n(film)’, ’Giancarlo Esposito’].\\nThought 2 To find the documentary, I can search Adam Clayton Powell (film).\\nAction 2 Search[Adam Clayton Powell (film)]\\nObservation 2 Adam Clayton Powell is a 1989 American documentary film directed by\\nRichard Kilberg. The film is about the rise and fall of influential\\nAfrican-American politician Adam Clayton Powell Jr.[3][4] It was later aired\\nas part of the PBS series The American Experience.\\nThought 3 Adam Clayton Powell (film) is a documentary about an African-American\\npolitician, not Finnish rock groups. So the documentary about Finnish rock\\ngroups must instead be The Saimaa Gesture.\\nAction 3 Finish[The Saimaa Gesture]\\nQuestion What profession does Nicholas Ray and Elia Kazan have in common?\\nThought 1 I need to search Nicholas Ray and Elia Kazan, find their professions, then\\nfind the profession they have in common.\\nAction 1 Search[Nicholas Ray]\\nObservation 1 Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,\\n1979) was an American film director, screenwriter, and actor best known for\\nthe 1955 film Rebel Without a Cause.\\nThought 2 Professions of Nicholas Ray are director, screenwriter, and actor. I need\\nto search Elia Kazan next and find his professions.\\nAction 2 Search[Elia Kazan]\\nObservation 2 Elia Kazan was an American film and theatre director, producer, screenwriter\\nand actor.\\nThought 3 Professions of Elia Kazan are director, producer, screenwriter, and actor.\\nSo profession Nicholas Ray and Elia Kazan have in common is director,\\nscreenwriter, and actor.\\nAction 3 Finish[director, screenwriter, actor]\\nQuestion Which magazine was started first Arthur’s Magazine or First for Women?\\nThought 1 I need to search Arthur’s Magazine and First for Women, and find which was\\nstarted first.\\nAction 1 Search[Arthur’s Magazine]\\nObservation 1 Arthur’s Magazine (1844-1846) was an American literary periodical published\\nin Philadelphia in the 19th century.\\nThought 2 Arthur’s Magazine was started in 1844. I need to search First for Women\\nnext.\\nAction 2 Search[First for Women]\\nObservation 2 First for Women is a woman’s magazine published by Bauer Media Group in the\\nUSA.[1] The magazine was started in 1989.\\nThought 3 First for Women was started in 1989. 1844 (Arthur’s Magazine) < 1989 (First\\nfor Women), so Arthur’s Magazine was started first.\\nAction 3 Finish[Arthur’s Magazine]\\nQuestion Were Pavel Urysohn and Leonid Levin known for the same type of work?\\nThought 1 I need to search Pavel Urysohn and Leonid Levin, find their types of work,\\nthen find if they are the same.\\nAction 1 Search[Pavel Urysohn]\\nContinued on next page\\n18'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 18, 'page_label': '19'}, page_content='Published as a conference paper at ICLR 2023\\nHotpot QA Prompts – Continued from previous page\\nObservation 1 Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet\\nmathematician who is best known for his contributions in dimension theory.\\nThought 2 Pavel Urysohn is a mathematician. I need to search Leonid Levin next and\\nfind its type of work.\\nAction 2 Search[Leonid Levin]\\nObservation 2 Leonid Anatolievich Levin is a Soviet-American mathematician and computer\\nscientist.\\nThought 3 Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn\\nand Leonid Levin have the same type of work.\\nAction 3 Finish[yes]\\n19'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 19, 'page_label': '20'}, page_content='Published as a conference paper at ICLR 2023\\nC.2 F EVER\\nFEVER Prompts\\nOriginal Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if\\nthere is NOT ENOUGH INFORMATION.\\nClaim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\\nAnswer SUPPORTS\\nClaim Stranger Things is set in Bloomington, Indiana.\\nAnswer REFUTES\\nClaim Beautiful reached number two on the Billboard Hot 100 in 2003.\\nAnswer NOT ENOUGH INFO\\nAct Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if\\nthere is NOT ENOUGH INFORMATION.\\nClaim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\\nAction2 Search[Nikolaj Coster-Waldau]\\nObservation 1 Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and\\nproducer. He graduated from the Danish National School of Performing Arts\\nin Copenhagen in 1993,[1] and had his breakthrough role in Denmark with\\nthe film Nightwatch (1994). He played Jaime Lannister in the HBO fantasy\\ndrama series Game of Thrones, for which he received two Primetime Emmy\\nAward nominations for Outstanding Supporting Actor in a Drama Series..\\nCoster-Waldau has appeared in numerous films in his native Denmark and\\nScandinavia, including Headhunters (2011) and A Thousand Times Good Night\\n(2013). In the U.S, his debut film role was in the war film Black Hawk Down\\n(2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a\\ndetective in the short-lived Fox television series New Amsterdam (2008), and\\nappeared in the 2009 Fox television film Virtuality, originally intended as\\na pilot.\\nAction 2 Finish[SUPPORTS]\\nClaim Stranger Things is set in Bloomington, Indiana.\\nAction 1 Search[Stranger Things]\\nObservation 1 Stranger Things is an American science fiction horror drama television\\nseries created by the Duffer Brothers. Set in the 1980s, primarily in\\nthe fictional town of Hawkins, Indiana, the series centers on a number of\\nmysteries and supernatural events occurring around the town and their impact\\non an ensemble of child and adult characters.\\nAction 2 Finish[REFUTES]\\nClaim Beautiful reached number two on the Billboard Hot 100 in 2003.\\nAction 1 Search[Beautiful]\\nObservation 1 Could not find [Beautiful]. Similar: [’Beautiful’, ’Beautiful, Beautiful’,\\n’A Beautiful Mind (film)’, ’Beautiful (Christina Aguilera song)’, ’Life Is\\nBeautiful’].\\nAction 2 Search[Beautiful (Christina Aguilera song)\\nObservation 2 \"Beautiful\" is a song recorded by American singer Christina Aguilera for her\\nfourth studio album, Stripped (2002).\\nAction 3 Lookup[Billboard Hot 100]\\nObservation 3 (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the\\nUnited States, where it was certified Gold for 500,000 units shipped.\\nAction 4 Finish[NOT ENOUGH INFO]\\nCoT Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if\\nthere is NOT ENOUGH INFORMATION.\\nClaim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\\nThought Nikolaj William Coster-Waldau appeared in the 2009 Fox television film\\nVirtuality, so he has worked with the Fox Broadcasting Company.\\nAnswer SUPPORTS\\nClaim Stranger Things is set in Bloomington, Indiana.\\nThought Stranger Things is in the fictional town of Hawkins, Indiana, not in\\nBloomington, Indiana.\\nAnswer REFUTES\\nContinued on next page\\n20'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 20, 'page_label': '21'}, page_content='Published as a conference paper at ICLR 2023\\nFever Prompts – Continued from previous page\\nClaim Beautiful reached number two on the Billboard Hot 100 in 2003.\\nThought The song peaked at number two on the Billboard Hot 100 in the United States,\\nbut not sure if it was in 2003.\\nAnswer NOT ENOUGH INFO\\nReAct Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if\\nthere is NOT ENOUGH INFORMATION.\\nClaim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\\nThought 1 I need to search Nikolaj Coster-Waldau and find if he has worked with the\\nFox Broadcasting Company.\\nAction 1 Search[Nikolaj Coster-Waldau]\\nObservation 1 Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and\\nproducer. He graduated from the Danish National School of Performing Arts\\nin Copenhagen in 1993,[1] and had his breakthrough role in Denmark with\\nthe film Nightwatch (1994). He played Jaime Lannister in the HBO fantasy\\ndrama series Game of Thrones, for which he received two Primetime Emmy\\nAward nominations for Outstanding Supporting Actor in a Drama Series..\\nCoster-Waldau has appeared in numerous films in his native Denmark and\\nScandinavia, including Headhunters (2011) and A Thousand Times Good Night\\n(2013). In the U.S, his debut film role was in the war film Black Hawk Down\\n(2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a\\ndetective in the short-lived Fox television series New Amsterdam (2008), and\\nappeared in the 2009 Fox television film Virtuality, originally intended as\\na pilot.\\nThought 2 Because he \"appeared in the 2009 Fox television film Virtuality\", he should\\nhave worked with the Fox Broadcasting Company.\\nAction 2 Finish[SUPPORTS]\\nClaim Stranger Things is set in Bloomington, Indiana.\\nThought 1 I should search for Stranger Things, and see if it is set in Bloomington,\\nIndiana.\\nAction 1 Search[Stranger Things]\\nObservation 1 Stranger Things is an American science fiction horror drama television\\nseries created by the Duffer Brothers. Set in the 1980s, primarily in\\nthe fictional town of Hawkins, Indiana, the series centers on a number of\\nmysteries and supernatural events occurring around the town and their impact\\non an ensemble of child and adult characters.\\nThought 2 The observation says that it is set in a \"fictional town of Hawkins,\\nIndiana\", so it is not set in Bloomington.\\nAction 2 Finish[REFUTES]\\nClaim Beautiful reached number two on the Billboard Hot 100 in 2003.\\nThought 1 I need to search the song Beautiful and find if it reached number two on the\\nBillboard Hot 100 in 2003.\\nAction 1 Search[Beautiful]\\nObservation 1 Could not find [Beautiful]. Similar: [’Beautiful’, ’Beautiful, Beautiful’,\\n’A Beautiful Mind (film)’, ’Beautiful (Christina Aguilera song)’, ’Life Is\\nBeautiful’].\\nThought 2 From suggestions, I should search \"Beautiful (Christina Aguilera song)\" to\\nfind the song.\\nAction 2 Search[Beautiful (Christina Aguilera song)]\\nObservation 2 \"Beautiful\" is a song recorded by American singer Christina Aguilera for her\\nfourth studio album, Stripped (2002).\\nThought 3 It does not mention Billboard, so I need to look up \"Billboard Hot 100\" to\\nfind if it reached number two on it in 2003.\\nAction 3 Lookup[Billboard Hot 100]\\nObservation 3 (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the\\nUnited States, where it was certified Gold for 500,000 units shipped.\\nThought 4 It only says the song peaked at number two on the Billboard Hot 100, but not\\nif it was in 2003. I am not sure if this claim is true or not.\\nAction 4 Finish[NOT ENOUGH INFO]\\n21'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 21, 'page_label': '22'}, page_content='Published as a conference paper at ICLR 2023\\nC.3 W EBSHOP\\nTable 6: Prompts used for the Webshop experiments in Section 4. We use the prompt on the left for\\nAct, and use the prompt on the right (add reasoning) for ReAct.\\nInstruction: i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars\\nAct ReAct\\nAction: search\\n[\\n3 ounce bright citrus deodorant sensitive skin\\n]\\nObservation:[\\nBack to Search\\n]\\nPage 1 (Total results: 50)[\\nNext\\n]\\n[\\nB078GWRC1J\\n]\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\n$10.99[\\nB078GTKVXY\\n]\\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\n$10.99[\\nB08KBVJ4XN\\n]\\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men\\n, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar\\n& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage,\\n2.7 oz, 2-Pack)\\n$15.95\\nAction: click\\n[\\nB078GWRC1J\\n]\\nObservation:[\\nBack to Search\\n]\\n[\\nPrev\\n]\\nscent [assorted scents][bright citrus][calming lavender][ginger\\n[fresh] simply non-scents]\\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2\\n-pack)]\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\nPrice: $10.99\\nRating: N.A.\\n[Description]\\n[Features]\\n[Reviews]\\n[Buy Now]\\nAction: click[bright citrus]\\nObservation: You have clicked bright citrus.\\nAction: click[3 ounce (pack of 1)]\\nObservation: You have clicked 3 ounce (pack of 1).\\nAction: click[Buy Now]\\nAction: search\\n[\\n3 ounce bright citrus deodorant sensitive skin\\nObservation:[\\nBack to Search\\n]\\nPage 1 (Total results: 50)[\\nNext\\n]\\n[\\nB078GWRC1J\\n]\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\n$10.99[\\nB078GTKVXY\\n]\\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\n$10.99[\\nB08KBVJ4XN\\n]\\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men\\n, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar\\n& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage,\\n2.7 oz, 2-Pack)\\n$15.95\\nAction: think[B078GWRC1J and B078GTKVXY are bright citrus\\ndeodorant less then 50 dollars. I can check B078GWRC1J ﬁrst.\\nObservation: OK.\\nAction: click\\n[\\nB078GWRC1J\\n]\\nObservation:[\\nBack to Search\\n]\\n[\\nPrev\\n]\\nscent [assorted scents][bright citrus][calming lavender][ginger\\n[fresh] simply non-scents]\\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2\\n-pack)]\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\\nCalendula 3-Ounce\\nPrice: $10.99\\nRating: N.A.\\n[Description]\\n[Features]\\n[Reviews]\\n[Buy Now]\\nAction: think[For 3 ounce bottle of bright citrus deodorant\\nfor sensitive skin, the item has options ’bright citrus’\\nand ’3 ounce (pack of 1)’ and seems good to buy.]\\nObservation: OK.\\nAction: click[bright citrus]\\nObservation: You have clicked bright citrus.\\nAction: click[3 ounce (pack of 1)]\\nObservation: You have clicked 3 ounce (pack of 1).\\nAction: click[Buy Now]\\n22'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 22, 'page_label': '23'}, page_content='Published as a conference paper at ICLR 2023\\nC.4 ALFW ORLD\\nTable 7: An Act prompt on the ALFWorld clean task. No thoughts are provided.\\nYou are in the middle of a room. Looking quickly around you, you see a cabinet\\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\\nYour task is to: put a clean lettuce in diningtable.\\n> go to fridge 1\\nThe fridge 1 is closed.\\n> open fridge 1\\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a\\npotato 3, and a potato 2.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\\n> take lettuce 1 from diningtable 1\\nYou pick up the lettuce 1 from the diningtable 1.\\n> go to sinkbasin 1\\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\\n> clean lettuce 1 with sinkbasin 1\\nYou clean the lettuce 1 using the sinkbasin 1.\\n> think: Now I clean a lettuce (1). Next, I need to put it in/on diningtable 1.\\nOK.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\\nspatula 3, a tomato 2, and a tomato 1.\\n> put lettuce 1 in/on diningtable 1\\nYou put the lettuce 1 in/on the diningtable 1.\\n23'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 23, 'page_label': '24'}, page_content='Published as a conference paper at ICLR 2023\\nTable 8: An ReAct prompt on the ALFWorld clean task.\\nYou are in the middle of a room. Looking quickly around you, you see a cabinet\\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\\nYour task is to: put a clean lettuce in diningtable.\\n> think: To solve the task, I need to find and take a lettuce, then clean it with\\nsinkbasin, then put it in diningtable.\\nOK.\\n> think: First I need to find a lettuce. A lettuce is more likely to appear in\\nfridge (1), diningtable (1), sinkbasin (1), stoveburner (1-3), cabinet (1-13). I\\ncan check one by one, starting with fridge 1.\\nOK.\\n> go to fridge 1\\nThe fridge 1 is closed.\\n> open fridge 1\\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a\\npotato 3, and a potato 2.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\\n> think: Now I find a lettuce (1). Next, I need to take it.\\nOK.\\n> take lettuce 1 from diningtable 1\\nYou pick up the lettuce 1 from the diningtable 1.\\n> think: Now I take a lettuce (1). Next, I need to go to sinkbasin (1) and clean\\nit.\\nOK.\\n> go to sinkbasin 1\\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\\n> clean lettuce 1 with sinkbasin 1\\nYou clean the lettuce 1 using the sinkbasin 1.\\n> think: Now I clean a lettuce (1). Next, I need to put it in/on diningtable 1.\\nOK.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\\nspatula 3, a tomato 2, and a tomato 1.\\n> put lettuce 1 in/on diningtable 1\\nYou put the lettuce 1 in/on the diningtable 1.\\n24'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 24, 'page_label': '25'}, page_content='Published as a conference paper at ICLR 2023\\nTable 9: An ReAct-IM prompt on the ALFWorld clean task.\\nYou are in the middle of a room. Looking quickly around you, you see a cabinet\\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\\nYour task is to: put a clean lettuce in diningtable.\\n> think: To solve the task, I need to find and take a lettuce, then clean it with\\nsinkbasin, then put it in diningtable.\\nOK.\\n> think: First I need to find a lettuce.\\nOK.\\n> go to fridge 1\\nThe fridge 1 is closed.\\n> think: First I need to find a lettuce.\\nOK.\\n> open fridge 1\\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a\\npotato 3, and a potato 2.\\n> think: First I need to find a lettuce.\\nOK.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\\n> think: First I need to find a lettuce.\\nOK.\\n> take lettuce 1 from diningtable 1\\nYou pick up the lettuce 1 from the diningtable 1.\\n> think: I need to clean this lettuce (1) using sinkbasin 1.\\nOK.\\n> go to sinkbasin 1\\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\\n> think: I need to clean this lettuce (1) using sinkbasin 1.\\nOK.\\n> clean lettuce 1 with sinkbasin 1\\nYou clean the lettuce 1 using the sinkbasin 1.\\n> think: I need to put this cleaned lettuce (1) in/on diningtable 1.\\nOK.\\n> go to diningtable 1\\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\\nspatula 3, a tomato 2, and a tomato 1.\\n> think: I need to put this cleaned lettuce (1) in/on diningtable 1.\\nOK.\\n> put lettuce 1 in/on diningtable 1\\nYou put the lettuce 1 in/on the diningtable 1.\\nD T RAJECTORIES\\nD.1 FEVER T RAJECTORIES\\nWe provide some trajectories produced by ReAct, CoTand Acton several randomly selected exam-\\nples from the FEVER dev set below. Search results are omitted to space.\\n25'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 25, 'page_label': '26'}, page_content='Published as a conference paper at ICLR 2023\\nExample 2491 (gt: REFUTES)\\nReAct\\nClaim: Bermuda Triangle is a loosely-defined region in the Pacific Ocean.\\nThought 1: I need to search Bermuda Triangle and find if it is in the Pacific Ocean.\\nAction 1: Search[Bermuda Triangle]\\nObservation 1: Reportedly haunted locations...\\nThought 2: The observation says that it is in the western part of the North Atlantic\\nOcean, so it is not in the Pacific Ocean.\\nAction 2: Finish[REFUTES]\\nObservation 2: Episode finished\\nAct\\nClaim: Bermuda Triangle is a loosely-defined region in the Pacific Ocean.\\nAction 1 : Search[Bermuda Triangle]\\nObservation 1 : Reportedly haunted locations:...\\nAction 2: Finish[REFUTES]\\nObservation 2: Episode finished\\nCoT\\nClaim: Bermuda Triangle is a loosely-defined region in the Pacific Ocean.\\nThought: Let’s think step by step. The Bermuda Triangle is a loosely-defined region\\nin the Atlantic Ocean. The Pacific Ocean is not the Atlantic Ocean.\\nAnswer: REFUTES\\nExample 5908 (gt: SUPPORTS)\\nReAct\\nClaim: Princess Mononoke is a film.\\nThought 1: I need to search Princess Mononoke and find if it is a film.\\nAction 1: Search[Princess Mononoke]\\nObservation 1: Princess Mononoke ...\\nThought 2: From the observation, it says that Princess Mononoke is a film.\\nAction 2: Finish[SUPPORTS]\\nObservation 2: Episode finished\\nAct\\nClaim: Princess Mononoke is a film.\\nAction 1: Search[Princess Mononoke]\\nObservation 1: Princess Mononoke ...\\nAction 2: Finish[SUPPORTS]\\nObservation 2: Episode finished\\nCoT\\nClaim: Princess Mononoke is a film.\\nThought: Let’s think step by step. Princess Mononoke is a movie, and movies are\\nfilms.\\nAnswer: SUPPORTS\\nExample 1951 (gt: REFUTES)\\nReAct\\nClaim: Soyuz was part of the American space program.\\nThought 1: I need to search Soyuz and find if it was part of the American space\\nprogram.\\nAction 1: Search[Soyuz]\\nObservation 1: Soyuz is a transliteration...\\nThought 2: It does not mention the American space program, so I need to search\\n\"American space program\" to find if it was part of it.\\nAction 2: Search[American space program]\\nObservation 2: The space policy...\\nThought 3: It does not mention Soyuz, so I am not sure if this claim is true or not.\\nAction 3: Finish[NOT ENOUGH INFO]\\nObservation 3: Episode finished\\nAct\\nClaim: Soyuz was part of the American space program.\\nAction 1: Search[Soyuz]\\nObservation 1: Soyuz is a transliteration...\\nAction 2: Finish[NOT ENOUGH INFO]\\nObservation 2: Episode finished\\nCoT\\nClaim: Soyuz was part of the American space program.\\nThought: Let’s think step by step. Soyuz is a Russian spacecraft. The American\\nspace program is NASA. NASA and Russia have worked together on the\\nInternational Space Station.\\nAnswer: SUPPORTS\\n26'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 26, 'page_label': '27'}, page_content='Published as a conference paper at ICLR 2023\\nExample 3208 (gt: REFUTES)\\nReAct\\nClaim: Reign Over Me is an American film made in 2010.\\nThought 1: I need to search for Reign Over Me and find if it is an American film made\\nin 2010.\\nAction 1: Search[Reign Over Me]\\nObservation 1: Reign Over Me ...\\nThought 2: The observation says that it is an American film made in 2007, so it is not\\nmade in 2010.\\nAction 2: Finish[REFUTES]\\nObservation 2: Episode finished\\nAct\\nClaim: Reign Over Me is an American film made in 2010.\\nAction 1: Search[Reign Over Me]\\nAction 2: Finish[SUPPORTS]\\nObservation 2: Episode finished\\nCoT\\nClaim: Reign Over Me is an American film made in 2010.\\nThought: Let’s think step by step. First, is Reign Over Me an American film? Yes,\\nit is. Second, was it made in 2010? Yes, it was.\\nAnswer: SUPPORTS\\nD.2 ALFW ORLD TRAJECTORIES\\nWe provide an example trajectory ofReAct, Act, and ReAct-IM on the same ALFWorld game,\\npick_clean_then_place_in_recep-Knife-None-CounterTop-10/\\ntrial_T20190909_110445_675754.\\nD.2.1 REAC TTRAJECTORY\\nReAct is able to correctly ﬁnd the knife, clean it, and then place it on a countertop.\\n27'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 27, 'page_label': '28'}, page_content='Published as a conference paper at ICLR 2023\\nReAct ALFWorld trajectory\\nYou are in the middle of a room. Looking quickly around you, you see a cabinet 6, a\\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\\n1.\\nYour task is to: put a clean knife in countertop.\\n> think: To solve the task, I need to find and take a knife, then clean it with\\nsinkbasin, then put it in countertop.\\nOK.\\n> think: First I need to find a knife. A knife is more likely to appear in cabinet\\n(1-6), drawer (1-3), countertop (1-3), fridge (1), garbagecan (1), shelf (1-3),\\nsinkbasin (1), stoveburner (1-4), toaster (1). I can check one by one, starting with\\ncabinet 1.\\nOK.\\n> go to cabinet 1\\nOn the cabinet 1, you see a bowl 1.\\n> go to cabinet 2\\nThe cabinet 2 is closed.\\n> go to cabinet 3\\nOn the cabinet 3, you see a glassbottle 1.\\n> go to cabinet 4\\nOn the cabinet 4, you see a mug 1.\\n> go to cabinet 5\\nThe cabinet 5 is closed.\\n> go to cabinet 6\\nThe cabinet 6 is closed.\\n> go to drawer 1\\nThe drawer 1 is closed.\\n> go to drawer 2\\nThe drawer 2 is closed.\\n> go to drawer 3\\nThe drawer 3 is closed.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a potato 3, and a statue 1.\\n> think: Now I find a knife (1). Next, I need to take it.\\nOK.\\n> take knife 1 from countertop 2\\nYou pick up the knife 1 from the countertop 2.\\n> think: Now I take a knife (1). Next, I need to go to sinkbasin (1) and clean it.\\nOK.\\n> go to sinkbasin 1\\nOn the sinkbasin 1, you see a fork 3, a lettuce 3, and a spatula 2.\\n> clean knife 1 with sinkbasin 1\\nYou clean the knife 1 using the sinkbasin 1.\\n> think: Now I clean a knife (1). Next, I need to put it in/on countertop 1.\\nOK.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> put knife 1 in/on countertop 1\\nYou put the knife 1 in/on the countertop 1.\\nD.2.2 AC TTRAJECTORY\\nAct is able to correctly ﬁnd the knife, but tries to clean it using a sinkbasin before actually going\\nto the sinkbasin. From there on, the model is unable to recover, and is stuck repeating the same\\nsequence of commands. With ReAct, an existing thought would have concretely identiﬁed that the\\nknife had been taken, and that the next subgoal was to go to a sinkbasin to clean it.\\n28'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 28, 'page_label': '29'}, page_content='Published as a conference paper at ICLR 2023\\nAct ALFWorld trajectory\\nYou are in the middle of a room. Looking quickly around you, you see a cabinet 6, a\\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\\n1.\\nYour task is to: put a clean knife in countertop.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a potato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nYou pick up the knife 1 from the countertop 2.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\n> go to countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife 2 from countertop 3\\nNothing happens.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nNothing happens.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\n> go to countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife 2 from countertop 3\\nNothing happens.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nNothing happens.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\n> go to countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife 2 from countertop 3\\nNothing happens.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nNothing happens.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\nD.2.3 REAC T-IM TRAJECTORY\\nReAct-IM is able to correctly ﬁnd the knife, but is unable to clean it as well. The LLM then\\ngets stuck trying to place this knife on a countertop. One possible issue is the incorrect thought\\nthink: I need to find a clean knife. , which seems to trick the LLM in believing\\nthat the knife is already cleaned. In ReAct, this sequence of subgoals would be concretely captured\\nby each thought.\\n29'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 29, 'page_label': '30'}, page_content='Published as a conference paper at ICLR 2023\\nReAct-IM ALFWorld trajectory\\nYou are in the middle of a room. Looking quickly around you, you see a cabinet 6, a\\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\\n1.\\nYour task is to: put a clean knife in countertop.\\n> think: To solve the task, I need to find and take a clean knife, then put it in\\ncountertop.\\nOK.\\n> think: I need to find a clean knife.\\nOK.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> think: I need to find a clean knife.\\nOK.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a potato 3, and a statue 1.\\n> think: I need to find a clean knife.\\nOK.\\n> take knife 1 from countertop 2\\nYou pick up the knife 1 from the countertop 2.\\n> think: I need to put this knife (1) in/on countertop 1.\\nOK.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> think: I need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nYou put the knife 1 in/on the countertop 1.\\n> think: I need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think: I need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think: I need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think: I need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think: I need to put this knife (1) in/on countertop 1.\\nOK.\\n> put knife 1 in/on countertop 1\\nNothing happens.\\n> think: I need to put this knife (1) in/on countertop 1.\\nOK.\\n30'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 30, 'page_label': '31'}, page_content='Published as a conference paper at ICLR 2023\\nD.3 W EBSHOP TRAJECTORIES\\nTable 10: Example trajectories for Webshop predicted by Act and ReAct. Compared to Act,\\nReAct uses reasoning to ﬁnd products that satisfy all target attributes.\\nInstruction: get me a sixteen pack of apple cinnamon freeze dried banana chips, and price lower than 50.00 dollars\\nAct ReAct\\nAction: search\\n[\\nsixteen pack apple cinnamon freeze dried banana\\nchips\\n]\\nObservation:[\\nBack to Search\\n]\\nPage 1 (Total results: 50)[\\nNext\\n]\\n[\\nB0061IVFZE\\n]\\nBrothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -\\nOunce (Pack of 100)\\n$85.0[\\nB096H2P6G2\\n]\\nMoon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -\\n(Variety Pack)\\n$18.99[\\nB092JLLYK6\\n]\\nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect\\nFor School Lunches or an On-The-Go Snack - No Sugar Added,\\nNon GMO, Gluten Free, Nothing Artiﬁcial (0.53oz) 6-Pack\\n$12.99\\nAction: click\\n[\\nB0061IVFZE\\n]\\nObservation:[\\nBack to Search\\n]\\n[\\nPrev\\n]\\nﬂavor name [asian pear][banana][fuji apple & cinnamon][strawberry\\nbanana] Brothers-ALL-Natural Fruit Crisps, Strawberry Banana,\\n0.42 Ounce (Pack of 100)\\nPrice: $85.0\\nRating: N.A.\\n[Description]\\n[Features]\\n[Reviews]\\n[Buy Now]\\nAction: click[Buy Now]\\nAction: search\\n[\\nsixteen pack apple cinnamon freeze dried banana\\nchips\\n]\\nObservation:[\\nBack to Search\\n]\\nPage 1 (Total results: 50)[\\nNext\\n]\\n[\\nB0061IVFZE\\n]\\nBrothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -\\nOunce (Pack of 100)\\n$85.0[\\nB096H2P6G2\\n]\\nMoon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -\\n(Variety Pack)\\n$18.99[\\nB092JLLYK6\\n]\\nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect\\nFor School Lunches or an On-The-Go Snack - No Sugar Added,\\nNon GMO, Gluten Free, Nothing Artiﬁcial (0.53oz) 6-Pack\\n$12.99\\nAction: think[B0061IVFZE is strawberry banana, not apple cinnamon.\\nB096H2P6G2 is fruit snacks, not freeze dried banana chips.\\nB092JLLYK6 is banana crisps, not apple cinnamon. I can check\\nB092JLLYK6 ﬁrst.\\nObservation: OK.\\nAction: click\\n[\\nB092JLLYK6\\n]\\nObservation:[\\nBack to Search\\n]\\n[\\nPrev\\n]\\nﬂavor name [apple][apple cinnamon][banana][cantaloupe][peach][pear]\\n[strawberry][strawberry banana][sampler variety pack][mega variety\\npack][orchard variety pack]\\nsize [0.53 ounce (pack of 6)][0.53 ounce (pack of 8)][0.53 ounce (pack\\nof 16)]\\nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect\\nFor School Lunches or an On-The-Go Snack - No Sugar Added, Non\\nGMO, Gluten Free, Nothing Artiﬁcial (0.53oz) 6-Pack\\nPrice: $12.99\\nRating: N.A.\\n[Description]\\n[Features]\\n[Reviews]\\n[Buy Now]\\nAction: think[For sixteen pack of apple cinnamon freeze dried banana\\nchips, the item has options ’apple cinnamon’ and ’0.53 ounce (pack of\\n16)’ and seems good to buy.]\\nObservation: OK.\\nAction: click[apple cinnamon]\\nObservation: You have clicked apple cinnamon.\\nAction: click[0.53 ounce (pack of 16)]\\nObservation: You have clicked 0.53 ounce (pack of 16).\\nAction: click[Buy Now]\\nScore: 0.125 Score: 1.0\\n31'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 31, 'page_label': '32'}, page_content='Published as a conference paper at ICLR 2023\\nE M ORE ANALYSIS\\nE.1 S UCCESS AND FAILURE MODES ANALYSIS\\nWe provide some examples corresponding to the success and error mode analysis given in Sec. 3.3.\\nSearch results and non-representative steps are omitted to reduce space.\\nSuccess: True positive\\nReAct\\nQuestion: Author David Chanoff has collaborated with a U.S. Navy admiral who served as\\nthe ambassador to the United Kingdom under which President?\\nThought 1: I need to search David Chanoff and find the U.S. Navy admiral he\\ncollaborated with.\\nAction 1: Search[David Chanoff]\\nObservation 1: David Chanoff is a noted author...\\nAction 2: Search[William J. Crowe]\\nObservation 2: William J. Crowe is...\\nThought 3: William J. Crowe served as the ambassador to the United Kingdom under\\nPresident Bill Clinton. So the answer is Bill Clinton.\\nAction 3: Finish[Bill Clinton]\\nObservation 3: Episode finished\\nCoT\\nQuestion: What was the name of the team that represented an Arizona university in\\nthe conference that covers a broad expanse of the western United States,\\nincluding Arizona, California, and New Mexico?\\nThought: Let’s think step by step. The name of the team that represented an Arizona\\nuniversity in the conference that covers a broad expanse of the western\\nUnited States, including Arizona, California, and New Mexico is Arizona\\nState Sun Devils.\\nAnswer: Arizona State Sun Devils\\nSuccess: False positive\\nReAct\\nQuestion: Did the board game San Marco or About Time come out first?\\nThought 1: I need to search San Marco and About Time, and find which came out first.\\nAction 1: Search[San Marco]\\nObservation 1: San Marco is one of the ... The was built in venice.\\nThought 2: San Marco is a sestiere of Venice. I need to search About Time next.\\nAction 2: Search[About Time]\\nObservation 2: Could not find [About Time]. Similar: [...].\\nThought 3: About Time is a 2013 film. So San Marco came out first.\\nAction 3: Finish[San Marco]\\nObservation 3: Episode finished\\nCoT\\nQuestion: Was Vanderbilt University or Emory University founded first?\\nThought: Let’s think step by step. Vanderbilt University was founded in 1873. Emory\\nUniversity was founded in 1836. 1873 (Vanderbilt University) > 1836 (Emory\\nUniversity), so Vanderbilt University was founded first.\\nAnswer: Emory University\\nFailure: Reasoning error\\nReAct\\nQuestion: What was the middle name of the actress who was cast alongside Harvey\\nFierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laura\\nBell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur in\\nHairspray?\\nThought 1: I need to search Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell,\\nMary Bond Davis, Laura Bell Bundy, Matthew Morrison, Corey Reynolds, and\\nMarissa Jaret Winokur, find the actress that was cast alongside them in\\nHairspray, then find its middle name.\\nAction 1: Search[Harvey Fierstein]\\nObservation 1: Harvey Forbes Fierstein ...\\nThought 2: Harvey Fierstein is an actor, playwright, and screenwriter. I need to\\nsearch Linda Hart next...\\nAction 2: Search[Linda Hart]...\\nCoT\\nQuestion: Who is older Danny Green or James Worthy?\\nThought: Let’s think step by step. Danny Green was born in 1987. James Worthy was\\nborn in 1961. 1987 (Danny Green) > 1961 (James Worthy), so Danny Green is\\nolder.\\nAnswer: Danny Green\\n32'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './2210.03629v3-2.pdf', 'total_pages': 33, 'page': 32, 'page_label': '33'}, page_content='Published as a conference paper at ICLR 2023\\nFailure: Search error\\nReAct\\nQuestion: What does the goddess associated with the goddess frigg consists of what\\ntales?\\nThought 1: I need to search goddess frigg and find its associated goddess.\\nAction 1: Search[goddess frigg]\\nObservation 1: Could not find goddess frigg. Similar: [’Frigg’, ’Frigg and Freyja common\\norigin hypothesis’, ’Freyja (section Relation to Frigg and other goddesses\\nand figures)’, ’Eir (redirect from Eir (goddess))’, ’Baldr’].\\nThought 2: I need to search Frigg instead...\\nFailure: Hallucination\\nCoT\\nQuestion: What was the first year a scientific journal published by an organization\\nlocated in the Bronx was published?\\nThought: Let’s think step by step. The first year a scientific journal published by\\nan organization located in the Bronx was published was 1916.\\nAnswer: 1916 (Label: 1909)\\nFailure: Label ambiguity\\nReAct\\nQuestion: What nationality is the man who developed the dictator game?\\nThought 1: I need to search dictator game, find the man who developed it, then find his\\nnationality.\\nAction 1: Search[dictator game]\\nObservation 1: The dictator game is a popular experimental instrument in social psychology\\nand economics...\\nAction 3: Search[Amos Tversky]\\nObservation 3: Amos Nathan Tversky...\\nThought 4: Amos Tversky is an Israeli cognitive and mathematical psychologist. So the\\nman who developed the dictator game is Israeli.\\nAction 4: Finish[Israeli] (label: Israel-American)\\nCoT\\nQuestion: Kurt Fuller from Stockton, CA appeared in which American Detective Comedy\\nDrama television series created by Steve Franks?\\nThought: Let’s think step by step. Kurt Fuller from Stockton, CA appeared in Psych.\\nPsych is an American Detective Comedy Drama television series created by\\nSteve Franks.\\nAnswer: Psych (label: Psych is an American detective comedy-drama)\\n33')]\n"
     ]
    }
   ],
   "source": [
    "# PDF 문서 활용\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 경로 설정\n",
    "pdf_path = \"./2210.03629v3-2.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "# PDF 파일 내용을 로드\n",
    "document = loader.load()\n",
    "\n",
    "# 결과 확인\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 벡터 데이터베이스\n",
    "\n",
    "벡터 데이터베이스는 고차원 벡터로 변환된 텍스트 데이터를 저장하고, 이를 기반으로 유사성 검색을 가능하게 하는 데이터베이스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from faiss-cpu) (2.2.4)\n",
      "Requirement already satisfied: packaging in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from faiss-cpu) (23.2)\n",
      "Using cached faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (30.7 MB)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"# ReAct Prompting\\n\\nGPT-3 prompting code for ICLR 2023 paper [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629).\\n\\nTo use ReAct for more tasks, consider trying [LangChain's zero-shot ReAct Agent](https://python.langchain.com/docs/modules/agents/agent_types/react.html).\\n\\n## Setup\\nYou need to first have an OpenAI API key and store it in the environment variable ``OPENAI_API_KEY`` (see [here](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)).\\n\\nPackage requirement: ``openai``, and install ``alfworld`` following instructions [here](https://github.com/alfworld/alfworld).\\n\\n## Experiments\\nRun ``{hotpotqa,fever,alfworld,webshop}.ipynb``. As HotpotQA and FEVER have large validation sets, we only run 500 random examples (see notebooks). We find PaLM and GPT-3 are better at different tasks.\\n\\n\\n|                    | HotpotQA (500 random dev, EM) | FEVER (500 random dev, EM) | AlfWorld (success rate) | WebShop  (success rate) |\\n|--------------------|-------------------------------|----------------------------|-------------------------|-------------------------|\\n| PaLM-540B (paper)  | 29.4                          | 62.2                       | 70.9                    | 40                      |\\n| GPT-3 (davinci-002) | 30.4                          | 54                         | 78.4                    | 35.8                    |\\n\\n## Citation\\n\\n```bibtex\\n@inproceedings{yao2023react,\\n  title = {{ReAct}: Synergizing Reasoning and Acting in Language Models},\\n  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},\\n  booktitle = {International Conference on Learning Representations (ICLR) },\\n  year = {2023},\\n  html = {https://arxiv.org/abs/2210.03629},\\n}\\n```\\n\"]\n",
      "총 1 개의 문서가 벡터 데이터베이스에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# VectorStore 의 기본 예제를 통해 벡터 데이터베이스를 생성하고, 문서를 임베딩하여 저장한 후, 유사성 검색을 수행하는 방법을 소개한다.\n",
    "\n",
    "# FAISS 활용\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# 1. 텍스트 문서 로드\n",
    "# pdf_path = \"./2210.03629v3-2.pdf\"\n",
    "# loader = PyPDFLoader(pdf_path)\n",
    "loader = TextLoader(\"./ReAct README.md\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. 문서 내용을 리스트로 변환\n",
    "texts = [doc.page_content for doc in documents]\n",
    "print(texts)\n",
    "\n",
    "# 3. OpenAI 임베딩 모델로 텍스트 임베딩 생성\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "document_embeddings = embedding_model.embed_documents(texts)\n",
    "\n",
    "# 4. FAISS 벡터 데이터베이스 생성 및 임베딩 저장\n",
    "vector_db = FAISS.from_texts(texts, embedding_model)\n",
    "\n",
    "# 5. 데이터베이스 상태 확인\n",
    "print(f\"총 {len(texts)} 개의 문서가 벡터 데이터베이스에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사 문서 1:\n",
      "# ReAct Prompting\n",
      "\n",
      "GPT-3 prompting code for ICLR 2023 paper [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629).\n",
      "\n",
      "To use ReAct for more tasks, consider trying [LangChain's zero-shot ReAct Agent](https://python.langchain.com/docs/modules/agents/agent_types/react.html).\n",
      "\n",
      "## Setup\n",
      "You need to first have an OpenAI API key and store it in the environment variable ``OPENAI_API_KEY`` (see [here](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)).\n",
      "\n",
      "Package requirement: ``openai``, and install ``alfworld`` following instructions [here](https://github.com/alfworld/alfworld).\n",
      "\n",
      "## Experiments\n",
      "Run ``{hotpotqa,fever,alfworld,webshop}.ipynb``. As HotpotQA and FEVER have large validation sets, we only run 500 random examples (see notebooks). We find PaLM and GPT-3 are better at different tasks.\n",
      "\n",
      "\n",
      "|                    | HotpotQA (500 random dev, EM) | FEVER (500 random dev, EM) | AlfWorld (success rate) | WebShop  (success rate) |\n",
      "|--------------------|-------------------------------|----------------------------|-------------------------|-------------------------|\n",
      "| PaLM-540B (paper)  | 29.4                          | 62.2                       | 70.9                    | 40                      |\n",
      "| GPT-3 (davinci-002) | 30.4                          | 54                         | 78.4                    | 35.8                    |\n",
      "\n",
      "## Citation\n",
      "\n",
      "```bibtex\n",
      "@inproceedings{yao2023react,\n",
      "  title = {{ReAct}: Synergizing Reasoning and Acting in Language Models},\n",
      "  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},\n",
      "  booktitle = {International Conference on Learning Representations (ICLR) },\n",
      "  year = {2023},\n",
      "  html = {https://arxiv.org/abs/2210.03629},\n",
      "}\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 벡터 데이터베이스에서 유사한 문서를 검색해보자\n",
    "\n",
    "# 6. 쿼리 문장을 임베딩하여 벡터로 변환\n",
    "query = \"What is a document loader?\"\n",
    "\n",
    "# 7. 벡터 데이터베이스에서 쿼리와 유사한 문서 검색\n",
    "search_results = vector_db.similarity_search(query, k=3)  # 상위 3개의 유사 문서 반환\n",
    "\n",
    "# 검색 결과 출력\n",
    "for i, result in enumerate(search_results, 1):\n",
    "    print(f\"유사 문서 {i}:\")\n",
    "    print(result.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Splitter\n",
    "\n",
    "Text Splitter는 긴 문서나 문단을 작은 청크 단위로 분할하여 검색 엔진이 효율적으로 인덱싱하고 검색할 수 있도록 돕는 도구\n",
    "\n",
    "Text Splitter는 문맥을 유지하며 문서를 분할하는 데 중점을 둔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='# ReAct Prompting\n",
      "\n",
      "GPT-3 prompting code for ICLR 2023 paper [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629).\n",
      "\n",
      "To use ReAct for more tasks, consider trying [LangChain's zero-shot ReAct Agent](https://python.langchain.com/docs/modules/agents/agent_types/react.html).' metadata={'source': './ReAct README.md'}\n",
      "page_content='To use ReAct for more tasks, consider trying [LangChain's zero-shot ReAct Agent](https://python.langchain.com/docs/modules/agents/agent_types/react.html).\n",
      "\n",
      "## Setup\n",
      "You need to first have an OpenAI API key and store it in the environment variable ``OPENAI_API_KEY`` (see [here](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)).\n",
      "\n",
      "Package requirement: ``openai``, and install ``alfworld`` following instructions [here](https://github.com/alfworld/alfworld).' metadata={'source': './ReAct README.md'}\n",
      "page_content='Package requirement: ``openai``, and install ``alfworld`` following instructions [here](https://github.com/alfworld/alfworld).\n",
      "\n",
      "## Experiments\n",
      "Run ``{hotpotqa,fever,alfworld,webshop}.ipynb``. As HotpotQA and FEVER have large validation sets, we only run 500 random examples (see notebooks). We find PaLM and GPT-3 are better at different tasks.' metadata={'source': './ReAct README.md'}\n",
      "page_content='|                    | HotpotQA (500 random dev, EM) | FEVER (500 random dev, EM) | AlfWorld (success rate) | WebShop  (success rate) |\n",
      "|--------------------|-------------------------------|----------------------------|-------------------------|-------------------------|\n",
      "| PaLM-540B (paper)  | 29.4                          | 62.2                       | 70.9                    | 40                      |' metadata={'source': './ReAct README.md'}\n",
      "page_content='| PaLM-540B (paper)  | 29.4                          | 62.2                       | 70.9                    | 40                      |\n",
      "| GPT-3 (davinci-002) | 30.4                          | 54                         | 78.4                    | 35.8                    |' metadata={'source': './ReAct README.md'}\n",
      "page_content='## Citation\n",
      "\n",
      "```bibtex\n",
      "@inproceedings{yao2023react,\n",
      "  title = {{ReAct}: Synergizing Reasoning and Acting in Language Models},\n",
      "  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},\n",
      "  booktitle = {International Conference on Learning Representations (ICLR) },\n",
      "  year = {2023},\n",
      "  html = {https://arxiv.org/abs/2210.03629},\n",
      "}\n",
      "```' metadata={'source': './ReAct README.md'}\n"
     ]
    }
   ],
   "source": [
    "# 문서 로드 후 분할\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 텍스트 로더 설정\n",
    "loader = TextLoader(\"./ReAct README.md\")\n",
    "\n",
    "# 텍스트 스플리터 설정 (500자 단위로 분할)\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "\n",
    "# 문서를 로드하면서 동시에 분할\n",
    "documents = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# 각 청크 확인\n",
    "for doc in documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 콜백 및 평가\n",
    "\n",
    "콜백은 랭체인 작업의 각 단계에서 발생하는 이벤트를 추적하고 모델의 성능을 모니터링하는 데 사용되는 중요한 기능\n",
    "\n",
    "콜백과 평가 기능의 핵심은 작업의 흐름을 상세히 추적하고 분석\n",
    "- 체인의 각 단계에서 발생하는 이벤트를 기록하여 작업이 어떻게 진행되고 있는지 실시간으로 파악\n",
    "- 응답 생성 과정에서 발생한 에러나 특정 단계에서의 성능 저하 같은 문제를 확인 가능\n",
    "- 실시간 스트리밍 기능을 통해 작업 상태를 스트리밍하며 관리자에게 알림을 제공\n",
    "- 각 작업에 대한 평가 지표를 기록하여 모델의 성능을 모니터링하는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='프랑스의 수도는 파리, 영국의 수도는 런던, 스페인의 수도는 마드리드입니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 20, 'total_tokens': 50, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_0011324330', 'id': 'chatcmpl-BQDuN14cqHr7i2yOrcpE1z1JvVoHf', 'finish_reason': 'stop', 'logprobs': None} id='run-b3fd7f1b-5e5a-429f-b05f-a464d2ef02ea-0' usage_metadata={'input_tokens': 20, 'output_tokens': 30, 'total_tokens': 50, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Total Tokens: 50\n",
      "Total Cost: 0.00035\n"
     ]
    }
   ],
   "source": [
    "# 콜백 함수\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    response = llm.invoke(\"프랑스, 영국, 스페인의 수도는?\")\n",
    "    print(response)\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Total Cost: {cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
