{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG\n",
    "\n",
    "정보 검색의 품질을 더욱 향상시키기 위해 사용되는 다양한 Advanced RAG 기술 중 4가지 기술에 대해서 설명한다.\n",
    "\n",
    "- 검색된 문서의 순서를 변경하는 => 리랭커(Re-ranker)\n",
    "- 가상의 문서를 생성하는 => HyDE(Hypothentical Document Expansion)\n",
    "- 더 많은 정보를 포함하도록 쿼리를 변경하는 => 쿼리 확장\n",
    "- 다양한 관점을 반영하도록 다중으로 쿼리를 생성하는 => 멀티 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 리랭커(Re-ranker)\n",
    "\n",
    "문서의 순위를 조절하는 전통적인 정보 검색(Information Retrieval, IR) 방법으로는\n",
    "- TF-IDF (Term Frequency-Invert Document Frequency)\n",
    "- BM25 (Best Matching 25)\n",
    "\n",
    "두 방법 모두 특정 단어가 얼마나 중요한지를 평가하는 방식으로, 문서 검색의 초기 단계에서 많이 사용하는 대표적인 통계 기반 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF\n",
    "\n",
    "TF 는 특정 단어가 한 문서 내에서 얼마나 자주 등장하는지를 나타냄 -> 자주 등장할수록 중요한 단어로 간주됨  \n",
    "IDF 는 특정 단어가 전체 문서에서 얼마나 드물게 등장하는지를 나타냄 -> 이는 흔히 등장하는 단어의 중요도를 줄여주는 역할\n",
    "\n",
    "예시)  \n",
    "전체 단어의 수: 100  \n",
    "해당 문서에서 등장한 횟수: 3  \n",
    "=> TF = 3/100\n",
    "\n",
    "전체 문서의 개수: 4  \n",
    "해당 단어가 등장한 문서의 개수: 2  \n",
    "=> IDF = log(4/2)\n",
    "\n",
    "TF-IDF = TF * IDF = (3/100) * log(4/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BM25\n",
    "\n",
    "TF-IDF를 개선한 정보 검색 모델\n",
    "\n",
    "문서와 쿼리 간의 관련성을 평가하는데 있어 보다 세밀한 조정을 가능하게 함\n",
    "\n",
    "장점)  \n",
    "문서의 길이에 따라 가중치를 조정하는 기능이 있어 짧은 문서와 긴 문서 간의 공정한 비교가 가능하다는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "리랭커 모델은 크게 Cross-encoder 방식과 Bi-encoder 방식\n",
    "\n",
    "- Cross-encoder 방식\n",
    "  - 쿼리와 문서를 한 쌍으로 결합하여 입력으로 넣고, 두 텍스트가 얼마나 관련이 있는지에 대한 점수를 직접 계산하여 각 문서의 최종 순위를 재조정하는 방식\n",
    "- Bi-encoder 방식\n",
    "  - 쿼리와 문서를 독립적으로 처리하여 임베딩을 계산하는 방식\n",
    "  - 문서에 대한 임베딩을 오프라인에서 미리 계산할 수 있다는 장점이 있음 -> 이는 대규모 검색 시스템에서 매우 중요한 효율성을 제공함\n",
    "  - 쿼리와 문서를 독립적으로 임베딩하므로, 두 입력 간의 복잡한 상호작용을 잘 반영하지 못한다는 한계가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: pymupdf in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (1.25.5)\n",
      "Requirement already satisfied: sentence-transformers in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (4.1.0)\n",
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.0.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
      "Requirement already satisfied: filelock in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from triton==3.3.0->torch) (78.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: Pillow in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/haiqv/conda/envs/agent/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading hf_xet-1.0.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (54.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers pymupdf sentence-transformers hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haiqv/conda/envs/agent/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# FAISS 를 통해서 최근접 이웃을 먼저 찾고, 이를 Cross-encoder 기반의 리랭커 모델을 사용하여 직접적인 스코어를 산출하여 문서를 재정렬 해보자.\n",
    "\n",
    "# PDF 파일에서 -> 텍스트 추출 -> HuggingFaceEmbeddings 를 사용하여 임베딩 -> FAISS 벡터 스토어에 저장\n",
    "\n",
    "import torch\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 로드\n",
    "loader = PyMuPDFLoader(\"BBS_202402151054353090.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Document 객체에서 텍스트 내용 추출\n",
    "text_contents = [doc.page_content for doc in texts]\n",
    "\n",
    "# 임베딩 모델 설정\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# 벡터 스토어 설정\n",
    "vectorstore = FAISS.from_texts(text_contents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BGE Reranker 모델 설정\n",
    "model_name = \"BAAI/bge-reranker-v2-m3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리랭킹을 수행하고 기본 검색 결과와 비교해 보자\n",
    "\n",
    "def rerank_documents(query, docs, top_k=5):\n",
    "    pairs = [[query, doc.page_content] for doc in docs]\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        scores = model(**inputs).logits.squeeze(-1)\n",
    "    ranked_indices = scores.argsort(descending=True)\n",
    "    reranked_docs = [(docs[i], scores[i].item()) for i in ranked_indices[:top_k]]\n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 기본 검색 결과\n",
      "문서 1:\n",
      "공공기관의 돌봄서비스 제공에 활용할 수 있는 기술개발이 필요\n",
      " 신체 및 정서 변화를 겪는 고령층을 위한 다양한 기술과 서비스는 글로벌 사회가 직면한 \n",
      "초고령 사회의 문제점을 해결...\n",
      "\n",
      "문서 2:\n",
      "고령화시대 해결을 위한 기술개발\n",
      "- 19 -\n",
      "◎ 돌봄 분야\n",
      " (한국, 스마트요양원) 2020년부터 부산에서 요양원을 디지털 전환하는 ‘스마트 요양원 사업’을 \n",
      "진행하여 현장에 적...\n",
      "\n",
      "문서 3:\n",
      "TIPA 이슈 리포트\n",
      "Vol. 6\n",
      "- 14 -\n",
      "Ⅲ. 고령화 기술·제품 동향\n",
      "◎ 신체활동/이동 보조 분야\n",
      " (한국, 스마트 지팡기) 말하는 스마트 지팡기 ‘톡톡스틱’은 기존 지팡이...\n",
      "\n",
      "문서 4:\n",
      "고령화시대 해결을 위한 기술개발\n",
      "- 17 -\n",
      "◎ 정서/인지 케어 분야\n",
      " (한국, 인지치료 로봇) ICT 인지중재 프로그램이 탑재된 앵무새 로봇 ‘피오’\n",
      "▪ 경도인지장애 및 초기치...\n",
      "\n",
      "문서 5:\n",
      "정기적으로 전달하고 응답내용을 분석하여 위험에 처하거나 특별한 주의가 필요하다고 인식되면 응급 \n",
      "알람 메시지를 가족이나 의사에게 전달\n",
      "▪ 시니어의 건강상태가 좋지않아 즉각적인 도움...\n",
      "\n",
      "\n",
      "## 리랭킹 결과\n",
      "문서 1 (점수: 3.0630):\n",
      "고령화시대 해결을 위한 기술개발\n",
      "- 3 -\n",
      "Ⅰ. 고령화시대의 도래\n",
      "1\n",
      "국내외 고령화 현황 및 전망\n",
      " 고령화(高齡化)란 평균 수명의 증가에 따라 총 인구 중 차지하는 노인의 인구비...\n",
      "\n",
      "문서 2 (점수: -1.5022):\n",
      "고령화시대 해결을 위한 기술개발\n",
      "- 7 -\n",
      " 정부는 고령친화 우수사업자·고령친화 우수제품 지정제도 도입으로 안심하고 고령친화사업의 \n",
      "용품을 이용할 수 있도록 하고 있으나 고령친화...\n",
      "\n",
      "문서 3 (점수: -2.1617):\n",
      "고령화시대 해결을 위한 기술개발\n",
      "- 21 -\n",
      "Ⅴ. 결론 및 시사점\n",
      " 현재 고령친화산업은 사물인터넷(IoT), 모바일과 인공지능의 발달과 비대면 서비스의 확대로 \n",
      "국내외 관심이 더...\n",
      "\n",
      "문서 4 (점수: -2.7052):\n",
      "TIPA 이슈 리포트\n",
      "Vol. 6\n",
      "- 20 -\n",
      "Ⅳ. 고령친화산업의 해결과제\n",
      " 국내 고령친화산업은 중소기업 형태의 제조업체들이 주로 활동하고 있어 기술 경쟁력 확보를 \n",
      "통해 시장에...\n",
      "\n",
      "문서 5 (점수: -2.8106):\n",
      "공공기관의 돌봄서비스 제공에 활용할 수 있는 기술개발이 필요\n",
      " 신체 및 정서 변화를 겪는 고령층을 위한 다양한 기술과 서비스는 글로벌 사회가 직면한 \n",
      "초고령 사회의 문제점을 해결...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 리트리버 설정\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "\n",
    "# 쿼리 실행\n",
    "query = \"국내 고령화 전망에 대해 알려주세요\"\n",
    "\n",
    "# 기본 검색 결과 출력\n",
    "print(\"## 기본 검색 결과\")\n",
    "base_docs = base_retriever.invoke(query)\n",
    "for i, doc in enumerate(base_docs[:5]):\n",
    "    print(f\"문서 {i+1}:\")\n",
    "    print(doc.page_content[:100] + \"...\\n\")\n",
    "\n",
    "# 리랭킹 결과 출력\n",
    "print(\"\\n## 리랭킹 결과\")\n",
    "reranked_docs = rerank_documents(query, base_docs)\n",
    "for i, (doc, score) in enumerate(reranked_docs):\n",
    "    print(f\"문서 {i+1} (점수: {score:.4f}):\")\n",
    "    print(doc.page_content[:100] + \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) HyDE\n",
    "\n",
    "Hypothetical Document Embeddings\n",
    "\n",
    "쿼리에 대한 가상 문서(Hypothetical Document)를 생성하고, 이를 기반으로 검색을 수행하는 방식  \n",
    "벡터 스토어로의 검색은 하지 않는다.\n",
    "\n",
    "기존의 BM25 나 TF-IDF 같은 전통적인 정보 검색 기법과 함께 사용되며, 특히 신경망 기반 리랭커와의 조합을 통해 높은 성능을 발휘한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haiqv/conda/envs/agent/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# HyDE로 가상의 문서를 생성하고, 이를 바탕으로 검색하는 실습\n",
    "\n",
    "import torch\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import HypotheticalDocumentEmbedder\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 로드\n",
    "loader = PyMuPDFLoader(\"BBS_202402151054353090.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "text_contents = [doc.page_content for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_553/2212684356.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 모델 설정\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# 벡터 스토어 설정\n",
    "vectorstore = FAISS.from_texts(text_contents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_553/3568116609.py:5: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(\n"
     ]
    }
   ],
   "source": [
    "# Qwen/Qwen2.5-1.5B-Instruct 모델 설정\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,  # 생성할 최대 새 토큰 수\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a deprecated class. Please use `from langchain.chains import HypotheticalDocumentEmbedder` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 가상 문서:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_553/485704254.py:23: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm(custom_prompt.format(question=query)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 질문에 대한 가상의 문서를 생성해주세요: 국내 고령화 전망에 대해 알려주세요\n",
      "\n",
      "문서: 국내 고령화 전망\n",
      "\n",
      "고령화는 세계적으로 인기 있는 이슈 중 하나로, 한국也不例外입니다. 한국은 2018년부터 고령화 사회로 진입하였으며, 이에 따른 변화와 위협이 대두되고 있습니다.\n",
      "\n",
      "한국의 고령화 현상을 이해하기 위해서는 주요 특징과 미래 전망을 살펴보는 것이 중요합니다. \n",
      "\n",
      "첫째, 고령화 시대의 가장 큰 문제점은 노인 심身 건강 문제입니다. 노인이 점점 늙어지면서 다양한 건강 문제가 발생하고 있어 이를 해결하기 위한 노력이 필요합니다. \n",
      "\n",
      "둘째, 고령화는 경제에도 영향을 미칩니다. 고령화 사회에서는 더 많은 노인들이 일자리를 찾는데 어려움을 겪게 됩니다. 따라서 정책의 한계로, 고령화가 경제적 안정성을 약화시키는 요인으로 작용할 수 있습니다.\n",
      "\n",
      "셋째, 고령화는 교육 분야에서도 중요한 변화를 가져올 것입니다. 교육의 형태가 변화하면서 더 많은 학생들이 고령의 부모님을 돌볼 수 있도록\n",
      "생성된 임베딩: [np.float64(0.04337908327579498), np.float64(0.25073957443237305), np.float64(0.07979312539100647), np.float64(-0.10748119652271271), np.float64(0.17996497452259064), np.float64(0.07843017578125), np.float64(-0.14852800965309143), np.float64(0.11208377033472061), np.float64(0.02158929780125618), np.float64(0.23853909969329834)]\n",
      "관련 문서 1:\n",
      "고령화시대 해결을 위한 기술개발\n",
      "- 3 -\n",
      "Ⅰ. 고령화시대의 도래\n",
      "1\n",
      "국내외 고령화 현황 및 전망\n",
      " 고령화(高齡化)란 평균 수명의 증가에 따라 총 인구 중 차지하는 노인의 인구비...\n",
      "\n",
      "관련 문서 2:\n",
      "고령화시대 해결을 위한 기술개발\n",
      "- 5 -\n",
      "2\n",
      "고령친화 기술의 필요성\n",
      " 국내 생산가능인구 감소 및 노년부양비 증가 등 고령화 시대 도래에 따른 문제점을 극복하기 \n",
      "위한 해결책이 ...\n",
      "\n",
      "관련 문서 3:\n",
      "TIPA 이슈 리포트\n",
      "Vol. 6\n",
      "- 10 -\n",
      "2\n",
      "고령친화산업 동향\n",
      "◎ 시장규모 및 전망\n",
      " (미국) 실버시장 규모 1위의 미국 시장은 2025년 약 3조 5천억 달러가 될 것으로...\n",
      "\n",
      "관련 문서 4:\n",
      "시장에 변화를 가져옴\n",
      " (국내) 2020년 기준 고령친화산업 시장 규모는 72조 8,305억 원으로 추계되었으며 여가, \n",
      "식품, 의약품, 요양서비스 분야가 높은 시장점유율을 나타...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사용자 정의 프롬프트 템플릿 생성\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"다음 질문에 대한 가상의 문서를 생성해주세요: {question}\\n\\n문서:\"\n",
    ")\n",
    "\n",
    "# HyDE 설정\n",
    "hyde = HypotheticalDocumentEmbedder.from_llm(\n",
    "    llm=llm,\n",
    "    base_embeddings=embeddings,\n",
    "    custom_prompt=custom_prompt\n",
    ")\n",
    "\n",
    "# 쿼리 실행\n",
    "query = \"국내 고령화 전망에 대해 알려주세요\"\n",
    "hyde_embedding = hyde.embed_query(query)\n",
    "\n",
    "# vectorstore를 사용하여 검색\n",
    "results = vectorstore.similarity_search_by_vector(hyde_embedding)\n",
    "\n",
    "# hyde.embed_query() 호출 전후에 중간 결과 출력\n",
    "print(\"생성된 가상 문서:\")\n",
    "print(llm(custom_prompt.format(question=query)))\n",
    "print(\"생성된 임베딩:\", hyde_embedding[:10])  # 임베딩의 처음 10개 값만 출력\n",
    "\n",
    "# 결과 출력\n",
    "for i, doc in enumerate(results[:5]):  # 상위 5개 결과만 출력\n",
    "    print(f\"관련 문서 {i+1}:\")\n",
    "    print(doc.page_content[:100] + \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 쿼리 확장 (Query Expansion)\n",
    "\n",
    "쿼리를 보다 구체적이고 풍부하게 만드는 기술\n",
    "\n",
    "쿼리 확장은 크게 2가지 방식\n",
    "- 통계적 접근: 자주 함께 등장하거나 맥락적으로 유사한 단어를 추가하여 검색 범위를 확장\n",
    "- 신경망 접근: 딥러닝을 통해 맥락적으로 관련된 단어를 추가하여 의미적 이해를 향상시킨다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 로드 및 텍스트 분할\n",
    "loader = PyMuPDFLoader(\"BBS_202402151054353090.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "text_contents = [doc.page_content for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 모델 설정\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# 벡터 스토어 설정\n",
    "vectorstore = FAISS.from_texts(text_contents, embeddings)\n",
    "\n",
    "# Qwen/Qwen2.5-1.5B-Instruct 모델 설정\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "        top_p=0.85,\n",
    "        repetition_penalty=1.2,\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "확장된 쿼리: ' \n",
      "\n",
      "1) 국내 고령화는 어떤 특징이 있는가?\n",
      "2) 이에 따른 사회적 문제들은 무엇인가요? \n",
      "3) 어떻게 대응해야 할까요?\n",
      "\n",
      "각 항목마다 추가적인 정보나 분석, 예시 등을 포함한 문장을 만들어보세요.\n",
      "4) 국내 고령화 현상과 그에 따른 변화를 이해하기 위해 필요한 데이터 및 통계 자료는 어디에서 찾을 수 있을까? 5) 고령사회에 적합한 교육 시스템 개선 방안들을 제안할 수 있나요? 6) 경제적으로 안정적이면서도 건강하고 활기찬 노년기를 위한 정책들이 필요하다고 생각합니다.\n",
      "\n",
      "7) 이러한 주제에 대한 연구 또는 논문 작성에 있어 중요한 참고자료들 중 하나는 무엇일까요? 8) 고령화에 따라 생활 패턴이 바뀌며, 이를 반영하는 새로운 서비스나 제품들의 발전 가능성은 어느 정도라고 볼 수 있을까요? 9) 고령화 사회에서는 인공 지능(AI), 빅데이터 등 기술 도입으로 인해 일자리 창출 효과가 크다고 주장하는데, 실제로 이런 영향이 미치는지는 어떠한 방법으로 파악할 수 있을까요? 10) 현재까지 이루어진 고령화 대응策 실험 결과에 대해서 조사하면 좋을 것 같습니다. 특히, 어떤 지역이나 단체에서 실증 프로그램을 진행했는지를 찾아볼 수 있습니다. 그리고 그 과정에서 발견되는 성공 사례들과 실패 사례들도 함께 살펴보고 싶습니다. 11) 고령화에 따른 소비 트렌드 변화에 대해 알아봅니다. 예를 들어, 젊은 세대보다 더 많은 돈을 쓰는 것이 일반적이라고 한다면, 이것이 어떤 의미인지 해설하시거나, 혹은 다른 사람들이 보기에 가장 큰 변화였던 부분은 무엇인지도 말해주시겠어요? 12) 고령화에 따른 교통수단 사용 변화에 대해 이야기하겠습니다. 저에게서 주변 사람들의 행동 변화를 알릴 수 있도록 하는 것은 중요하지만, 또한 이 변화가 사회 전체에 미치는 영향력도 언급해주실 수 있으신가요? 13) 고령화 사회에서 공감 능력을 갖춘 리더십 역량이 요구되는데, 이는 개인뿐만 아니라 조직 내에서도 적용될 수 있다고 합니다. 따라서, 이에 대한 강조점은 무엇이며, 실제 삶 속에서 어떻게 활용될 수 있을까요? 14) 고령화 사회에서 발생하는 법적 문제가 무엇이고, 이를 해결하려는 노력들은 어떤 방식으로 진행되고 있는가? 15) 고령화 사회에서 윤리를 다루는 입장에서 고려해야 할 요소들有哪些？ 16) 고령화 사회에서의 활력 넘버원(건강한 노年人물)을 유지시키는 방법에는 어떤 것을 추천받았습니까? 17) 고령화 사회에서의 혁신성과 창업의 가능성을 검토하세요. 이것은 개인의 차별화된 아이디어를 통해 시작되어야 하고, 동시에 사회적 가치를 제공하며, 재미있는 사업 모델을 만들 수 있다는 것입니다. 18) 고령화 사회에서의 학습 능력 증진에 관한 몇 가지 방법을 소개해 주시길 바랍니다. 여기에는 온라인 학습 사이트 이용, 면접 준비, 취업준비 등의 내용이 포함됩니다. 19) 고령화 사회에서의 의료 접근성이 어떻게 관리되고 있는가? 이는 특정 장애물을 극복하거나, 효율적인 의사소통을 지원하는 것으로 나타낼 수 있습니다. 20) 고령화 사회에서의 종교생활 변화에 대해 말씀드리겠습니다. 이는 신앙심이 더욱 강화되었으며, 가족 중심의 공동체 참여가 증가함에도 불구하고, 일부 사람들에게는 불편 함이 있다라는 사실입니다. 21) 고령화 사회에서의 청중 감수성 변화에 대해 이야기하겠습니다. 이는 음악, 영화, 책 같은 여러 가지 매개체를 통해 표현될 수 있으며, 개인의 경험과 견해를 존중하면서도 서로 다른 의견을 받아들이는 능력이 요구된다. 22) 고령화 사회에서의 비즈니스 운영 변화에 대해 구체적으로 설명해 주십시오. 이는 고객 탐색, 판매 전략, 직원 구성 등 다양한 영역에서 확인 될 수 있습니다. 23) 고령화 사회에서의 급\n"
     ]
    }
   ],
   "source": [
    "# Query expansion 함수\n",
    "def expand_query(original_query):\n",
    "    expansion_prompt = f\"다음 질문을 확장하여 관련된 다양한 키워드와 문구를 간결하게 한국어로 생성해주세요. 설명이나 문장 형태의 답변은 하지 마세요. 원래 질문: '{original_query}'\"\n",
    "    expanded_query = llm(expansion_prompt).split(original_query)[-1]\n",
    "    return expanded_query\n",
    "\n",
    "# 원래 쿼리\n",
    "original_query = \"국내 고령화 전망에 대해 알려주세요\"\n",
    "\n",
    "# 쿼리 확장\n",
    "expanded_query = expand_query(original_query)\n",
    "print(f\"확장된 쿼리: {expanded_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관련 문서 1:\n",
      "고령화시대 해결을 위한 기술개발\n",
      "- 9 -\n",
      "▪ 2017년 건강노화 제품 및 서비스에 대한 R&D, 파일럿 테스트 프로젝트, 예비 상업화 프로젝트 외 \n",
      "50건의 프로젝트를 진행 중임...\n",
      "\n",
      "관련 문서 2:\n",
      "고령화시대 해결을 위한 기술개발\n",
      "- 7 -\n",
      " 정부는 고령친화 우수사업자·고령친화 우수제품 지정제도 도입으로 안심하고 고령친화사업의 \n",
      "용품을 이용할 수 있도록 하고 있으나 고령친화...\n",
      "\n",
      "관련 문서 3:\n",
      "증가 대응방안으로는 고령층 新수요에 대응하는 고령친화 산업 육성, 고령자 맞춤형 주거지원, \n",
      "고령친화도시 조성 등 포함\n",
      "▪ 고령자가 시설이 아닌 가정과 지역사회에서 계속 거주하면서...\n",
      "\n",
      "관련 문서 4:\n",
      "TIPA 이슈 리포트\n",
      "Vol. 6\n",
      "- 6 -\n",
      "Ⅱ. 고령화 정책 및 산업 동향\n",
      "1\n",
      "고령화 관련 정책동향\n",
      "◎ 국내 정책동향\n",
      "  2020년 보건복지부에서 제4차 저출산･고령사회기본계획...\n",
      "\n",
      "관련 문서 5:\n",
      "고령화시대 해결을 위한 기술개발\n",
      "- 5 -\n",
      "2\n",
      "고령친화 기술의 필요성\n",
      " 국내 생산가능인구 감소 및 노년부양비 증가 등 고령화 시대 도래에 따른 문제점을 극복하기 \n",
      "위한 해결책이 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 확장된 쿼리로 검색 수행\n",
    "search_results = vectorstore.similarity_search(expanded_query, k=5)\n",
    "\n",
    "# 결과 출력\n",
    "for i, doc in enumerate(search_results):\n",
    "    print(f\"관련 문서 {i+1}:\")\n",
    "    print(doc.page_content[:100] + \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del model\n",
    "del llm\n",
    "\n",
    "# 2. 가비지 컬렉션 수동 실행\n",
    "gc.collect()\n",
    "\n",
    "# 3. GPU 메모리 비우기\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 멀티 쿼리\n",
    "\n",
    "사용자가 입력한 단일 쿼리를 여러 개의 하위 쿼리로 나누거나, 여러 가지 관점에서의 쿼리를 생성하여 검색을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import logging\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import List\n",
    "from langchain_core.output_parsers import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('langchain.retrievers.multi_query')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda:0\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# PDF 로드 및 텍스트 분할\n",
    "loader = PyMuPDFLoader(\"BBS_202402151054353090.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "text_contents = [doc.page_content for doc in texts]\n",
    "\n",
    "# 임베딩 모델 설정\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# 벡터 스토어 설정\n",
    "vectorstore = FAISS.from_texts(text_contents, embeddings)\n",
    "\n",
    "# LLM 설정 (Qwen/Qwen2.5-1.5B-Instruct 사용)\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_553/2754249418.py:29: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever_from_llm.get_relevant_documents(query)\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['당신은 AI 언어 모델 어시스턴트입니다. 사용자가 제공한 질문에 대해 벡터 데이터베이스에서 관련 문서를 검색할 수 있도록 질문을 3가지 다른 버전으로 생성하는 것이 당신의 임무입니다.', '사용자의 질문을 다양한 관점에서 재구성하여 거리 기반 유사도 검색의 한계를 극복할 수 있도록 돕는 것이 목표입니다.', '각 버전의 질문은 줄바꿈으로 구분하여 작성하세요.', '한국어로 작성하세요. 원본 질문: 국내 고령화 전망에 대해 알려주세요. 버전1: 국내 고령화 현황과 예상 변화를 설명해주세요. 버전2: 국내에서 인구 고령화 현상을 어떻게 볼 수 있나요? 버전3: 한국 고령화 상황의 현재 상태와 미래发展趋势에 대한 정보를 제공해주세요.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관련 문서 1:\n",
      "TIPA 이슈 리포트\n",
      "Vol. 6\n",
      "- 14 -\n",
      "Ⅲ. 고령화 기술·제품 동향\n",
      "◎ 신체활동/이동 보조 분야\n",
      " (한국, 스마트 지팡기) 말하는 스마트 지팡기 ‘톡톡스틱’은 기존 지팡이...\n",
      "\n",
      "관련 문서 2:\n",
      " (일본, DMM PALMI) 노인용 대화로봇으로써 분위기 등을 감지하여 상대의 말 예측 \n",
      "가능하고, 데이터가 쌓이면 말투를 비롯한 말솜씨가 발전, 상대방의 과거 대화내용·취미 ...\n",
      "\n",
      "관련 문서 3:\n",
      "고령화시대 해결을 위한 기술개발\n",
      "- 15 -\n",
      "[ 보행 재활기구, 트리 ]\n",
      "[ 배설지원 로봇 ]\n",
      "* 출처: : Shinkachi-portal\n",
      "* 출처: 일본 사이버다인, 배설지원 로...\n",
      "\n",
      "관련 문서 4:\n",
      "정기적으로 전달하고 응답내용을 분석하여 위험에 처하거나 특별한 주의가 필요하다고 인식되면 응급 \n",
      "알람 메시지를 가족이나 의사에게 전달\n",
      "▪ 시니어의 건강상태가 좋지않아 즉각적인 도움...\n",
      "\n",
      "관련 문서 5:\n",
      "▪ 돌봄 분야 그동안의 개발은 공급자 중심으로 이루어졌지만, 수많은 기술개발의 성공에도 사업화로 \n",
      "이어진 사례는 미미하였으며, 주요 원인으로 수요자의 요구와 피드백을 직접적이고 상...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사용자 정의 프롬프트 템플릿 생성\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"당신은 AI 언어 모델 어시스턴트입니다. 사용자가 제공한 질문에 대해 벡터 데이터베이스에서 관련 문서를 검색할 수 있도록 질문을 3가지 다른 버전으로 생성하는 것이 당신의 임무입니다.\n",
    "사용자의 질문을 다양한 관점에서 재구성하여 거리 기반 유사도 검색의 한계를 극복할 수 있도록 돕는 것이 목표입니다.\n",
    "각 버전의 질문은 줄바꿈으로 구분하여 작성하세요.\n",
    "한국어로 작성하세요. 원본 질문: {question}\"\"\"\n",
    ")\n",
    "\n",
    "# 출력 파서 정의\n",
    "class LineListOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        return text.strip().split(\"\\n\")\n",
    "\n",
    "# LLM 체인 생성\n",
    "output_parser = LineListOutputParser()\n",
    "llm_chain = custom_prompt | llm | output_parser\n",
    "\n",
    "# 다중 쿼리 리트리버 설정\n",
    "retriever_from_llm = MultiQueryRetriever(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    llm_chain=llm_chain,\n",
    "    parser_key=\"lines\"\n",
    ")\n",
    "retriever_from_llm.verbose = True\n",
    "\n",
    "# 쿼리 실행\n",
    "query = \"국내 고령화 전망에 대해 알려주세요\"\n",
    "results = retriever_from_llm.get_relevant_documents(query)\n",
    "\n",
    "# 결과 출력\n",
    "for i, doc in enumerate(results[:5]):  # 상위 5개 결과만 출력\n",
    "    print(f\"관련 문서 {i+1}:\")\n",
    "    print(doc.page_content[:100] + \"...\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
